<!doctype html><html lang dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="🚲 转到 https://nlp.letout.cn 🔔 RNN ➡️ 这一节我们来学习循环神经网络Recurrent Neural Networks。本节的内容是 Simple RNN，以及用 Pytorch 编程实现 Simple RNN。 🌱 简介 现在"><title>RNN</title><link rel=canonical href=https://emerywan.github.io/blog/p/nlp-in-action/simple-rnn/><link rel=stylesheet href=/blog/scss/style.min.9fdfe1d69c4486d1a84c3c7b288ce74b0111a02a60a6c12f21c6b7cc02b131b4.css><meta property="og:title" content="RNN"><meta property="og:description" content="🚲 转到 https://nlp.letout.cn 🔔 RNN ➡️ 这一节我们来学习循环神经网络Recurrent Neural Networks。本节的内容是 Simple RNN，以及用 Pytorch 编程实现 Simple RNN。 🌱 简介 现在"><meta property="og:url" content="https://emerywan.github.io/blog/p/nlp-in-action/simple-rnn/"><meta property="og:site_name" content="一层"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="自然语言处理"><meta property="article:published_time" content="2022-03-08T20:36:14+08:00"><meta property="article:modified_time" content="2022-03-08T20:36:14+08:00"><meta property="og:image" content="https://nlp.letout.cn/img/banner.png"><meta name=twitter:title content="RNN"><meta name=twitter:description content="🚲 转到 https://nlp.letout.cn 🔔 RNN ➡️ 这一节我们来学习循环神经网络Recurrent Neural Networks。本节的内容是 Simple RNN，以及用 Pytorch 编程实现 Simple RNN。 🌱 简介 现在"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nlp.letout.cn/img/banner.png"><link rel="shortcut icon" href=https://emerywan.github.io/blog/imgs/favicon.ico><link rel=preconnect href=https://fonts.proxy.ustclug.org><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.proxy.ustclug.org/css2?family=Noto+Sans+SC&family=Roboto&family=Roboto+Mono&display=swap" rel=stylesheet><style>:root{--sys-font-family:-apple-system, BlinkMacSystemFont, "Roboto", "Noto Sans SC", "Segoe UI", "Droid Sans", "Helvetica Neue", Ubuntu;--zh-font-family:-apple-system, BlinkMacSystemFont, "Roboto", "Noto Sans SC",  "PingFang SC", "Hiragino Sans GB", "Droid Sans Fallback", Ubuntu, "Microsoft YaHei";--code-font-family:"SF Mono", SFMono-Regular, "Roboto Mono", "Ubuntu Mono", Menlo, Monaco, Consolas, "Courier New", monospace;--base-font-family:-apple-system, BlinkMacSystemFont, "Roboto", "Noto Sans SC", var(--sys-font-family), var(--zh-font-family), sans-serif;--article-font-family:-apple-system, BlinkMacSystemFont, "Roboto", "Noto Sans SC", var(--base-font-family);--article-line-height:1.6;--article-font-size:1.6rem}</style></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog><img src=/blog/img/avatar_hud10ed4afecf1b63b1391b35c2778f738_36574_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/blog>一层</a></h1><h2 class=site-description>迎着这岁月的风。</h2></div></header><ol class=social-menu><li><a href=https://github.com/emerywan target=_blank title=GitHub><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.letout.cn target=_blank title=Home><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-smart-home" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#6f32be" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M19 8.71l-5.333-4.148a2.666 2.666.0 00-3.274.0L5.059 8.71A2.665 2.665.0 004.03 10.815v7.2a2 2 0 002 2h12a2 2 0 002-2v-7.2c0-.823-.38-1.6-1.03-2.105"/><path d="M16 15c-2.21 1.333-5.792 1.333-8 0"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>首页</span></a></li><li><a href=/blog/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>归档</span></a></li><li><a href=/blog/categories/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg><span>分类</span></a></li><li><a href=/blog/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>关于</span></a></li><li><a href=/blog/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>搜索</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>🌗 Dark</span></li></div></ol></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/blog/p/nlp-in-action/simple-rnn/><img src=https://nlp.letout.cn/img/banner.png loading=lazy alt="Featured image of post RNN"></a></div><div class=article-details><header class=article-category><a href=/blog/categories/nlp-in-action/ style=background-color:#d3adf7;color:#fff>NLP in action</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/nlp-in-action/simple-rnn/>RNN</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>03 - 08, 2022</time></div></footer></div></header><section class=article-content><hr><p><a class=link href=https://nlp.letout.cn/nlp/simple-rnn target=_blank rel=noopener>🚲 转到 https://nlp.letout.cn 🔔 RNN ➡️</a></p><hr><p>这一节我们来学习<strong>循环神经网络</strong><code>Recurrent Neural Networks</code>。本节的内容是 <code>Simple RNN</code>，以及用 <code>Pytorch</code> 编程实现 <code>Simple RNN</code>。</p><h2 id=-简介>🌱 简介</h2><p>现在 <code>RNN</code> 没有以前流行，尤其是在自然语言处理上，<code>RNN</code> 已经有一些过时了，如果训练的数据足够多，<code>RNN</code> 的效果不如 <code>Transformer</code> 模型，但是在小规模的问题上，<code>RNN</code> 还是很有用的。</p><h2 id=-如何建模时序数据>🔖 如何建模时序数据？</h2><p>机器学习中经常用到文本、语音等 <strong>时序数据</strong><code>sequential data</code>（按时间顺序记录的数据列，有长度不固定的特点）。</p><p>首先思考一个问题，怎么对这样的时序数据进行建模？
在上一小节中，我们将一段文字整体输入到一个逻辑回归 <code>Logistic Regression</code> 模型中，让模型来做二分类，这属于一个 <code>one-to-one</code> 模型，一个输入对应一个输出。</p><p><img src=https://nlp.letout.cn/img/nlp/03/1.png loading=lazy alt=1></p><p>全连接神经网络和卷积神经网络都属于 <code>one-to-one</code> 模型。</p><p>人脑并不会使用 <code>one-to-one</code> 模型处理时序数据，不会把一整段文字全部输入到大脑，我们在阅读的时候，会从左到右阅读一段文字，不断地在大脑里积累信息，阅读一段话之后，你脑子里就积累了一段文字的大意。</p><p><code>one-to-one</code> 模型要求一个输入对应一个输出，比如：输入一张图片，输出每一类的概率值，<code>one-to-one</code> 的模型比较适合这类图片问题，但是不太适合文本问题。</p><p>对于文本问题，输入和输出的长度并不固定，一段话可长可短，所以输入的长度并不固定；输出的长度也不固定，比如将英语翻译成汉语，一句英语有十个单词，翻译成汉语可能有十个字，可能有八个字，也可能是四个字的成语，输出汉语的字数并不固定，由于输入和输出的长度不固定，<code>one-to-one</code> 模型就不太适合了。</p><p><img src=https://nlp.letout.cn/img/nlp/03/translation.png loading=lazy alt=translation></p><p>对于时序数据，更好的是 <code>many-to-one</code> 或者是 <code>many-to-many</code> 模型，<code>RNN</code> 就是这样的模型，输入和输出的长度都不固定。所以 <code>RNN</code> 很适合语音，文本等时序序列数据。</p><p><img src=https://nlp.letout.cn/img/nlp/03/2.png loading=lazy alt=2></p><h2 id=-rnn>🍀️ RNN</h2><p><code>RNN</code> 和跟人的阅读习惯很类似：人每次看一个词，会逐渐在大脑里积累信息；<code>RNN</code> 每看一个词，会用状态向量 $h$ 来积累阅读过的信息。</p><p>首先，我们将输入的每个词用 词嵌入<code>word embedding</code> 变成一个词向量 $x$。</p><p><img src=https://nlp.letout.cn/img/nlp/03/3.png loading=lazy alt=3></p><p>每次把一个词向量输入 <code>RNN</code>，就会更新状态 $h$ ，把新的输入积累到状态 $h$ 里面。</p><p><img src=https://nlp.letout.cn/img/nlp/03/4.png loading=lazy alt=4></p><p>在 $h_0$中，包含了第一个词 <code>the</code> 的信息，在 $h_1$ 里面，包含了前两个 <code>the cat</code> 的信息；以此类推，状态 $h_2$ 包含 了前三个词 <code>the cat sat</code> 的信息，最后一个状态 $h_t$ 包含了整句话的信息，可以把 $h_t$ 看做 <code>RNN</code> 从整句话 <code>the cat sat on the mat</code> 抽取的特征向量，在更新状态 $h$ 的时候，需要用到参数矩阵 $A$。</p><p><img src=https://nlp.letout.cn/img/nlp/03/5.png loading=lazy alt=5></p><p><strong>注意：整个 RNN 只有一个参数矩阵</strong> $A$。无论这条链有多长，参数 $A$ 只有一个，$A$ 随机初始化，然后利用训练数据来学习 $A$。下面首先讲解 <code>Simple RNN Model</code>。</p><h2 id=-simple-rnn>🚀 Simple RNN</h2><p>我们具体看看，<code>Simple RNN</code> 简单循环神经网络是怎么把输入的词向量 $x$，结合到状态 $h$ 中的。</p><p><img src=https://nlp.letout.cn/img/nlp/03/6.png loading=lazy alt=6></p><p>我们将上一个状态记做 $h_t-1$，新输入词向量记做 $x_t$，将这两个向量做拼接 <code>concatenation</code>，得到一个更高维的向量。</p><p><img src=https://nlp.letout.cn/img/nlp/03/8.png loading=lazy alt=8></p><p>图中这个矩阵 $A$ 是 <code>RNN</code> 的模型参数，这里计算矩阵 $A$ 和这个向量的乘积（拼接后的向量），矩阵和向量的乘积是一个向量，然后使用激活函数 <code>tanh</code> 作用在向量的每一个元素上，最后把激活函数的输出记做新的状态 $h_t$。</p><p>这个激活函数式 双曲正切函数 <code>hyperbolic tangent function</code>，输入是任意实数，输出在 $(-1, +1)$ 之间。由于用了双曲正切激活函数，向量 $h_t$ 的每一个元素都在 $(-1, +1)$ 之间。</p><p><img src=https://nlp.letout.cn/img/nlp/03/7.png loading=lazy alt="hyperbolic tangent function"></p><p>这个神经网络的结构图可以这样理解：新的状态 $h_t$，是旧状态 $h_{t-1}$ 和新的输入 $x_t$ 的函数，神经网络模型的参数是 $A$：新的状态 $h_t$，依赖于向量 $h_{t-1}$, 向量 $x_t$ 以及矩阵 $A$。</p><h3 id=-为什么需要使用-tanh-作为激活函数>🎨 为什么需要使用 <code>tanh</code> 作为激活函数？</h3><p>我们思考这样一个问题：为什么需要使用 <code>tanh</code> 作为激活函数？能否将这个激活函数去掉，去掉之后会发生什么呢？</p><p><img src=https://nlp.letout.cn/img/nlp/03/9.png loading=lazy alt=9></p><p>首先我们做个简化，假设输入的词向量的元素都是 $0$。如图，这等同于输入的词向量 $x_t$ 都去掉，把矩阵 $A$ 右边一半也去掉。</p><p>$x_0 = x_1 = &mldr; = x_{100} = 0$</p><p><img src=https://nlp.letout.cn/img/nlp/03/10.png loading=lazy alt=10></p><p>这么一来，第 <code>100</code> 维的特征向量 $h_{100} = Ah_{99} = A^2h_{98} = &mldr; = A^{100}h_0$。</p><ul><li>🌰 假设矩阵 $A$ 最大的特征值略小于 <code>1</code></li></ul><p>比如，最大的特征值等于 <code>0.9</code>。那么会发生什么呢？</p><p>$0.9^{100}$ 非常接近于 <code>0</code> 了，所以矩阵 $A^{100}$ 非常接近于 <code>0</code>，那么新的特征向量 $h_{100}$ 也几乎也是一个全零的向量。</p><ul><li>🌰 假设矩阵 $A$ 最大的特征值略大于 <code>1</code></li></ul><p>比如，最大的特征值等于 <code>1.2</code>。</p><p>$1.2^{100}=82817974.522$，所以矩阵 $A^{100}$ 的元素都超级大，$A^{100}$的每个元素都很大，假如循环的次数更多一些，或者 $A$ 的特征值再大一些，状态向量的值就会爆炸。</p><p>假如没有这个激活函数 <code>tanh</code>，数值计算的时候很有可能会出问题，要么计算出的结果全部等于 <code>0</code>，要么爆炸了全部是 <code>NaN: Not a Number</code>。通过使用这个激活函数，每次更新状态 $h$ 后，都会做一个标准化操作 <code>normalization</code>，让 $h$ 恢复到 $(-1, +1)$ 这个合适的区间里。</p><h3 id=-simple-rnn-模型参数数量>🏝️ Simple RNN 模型参数数量</h3><p>我们来数一下 <code>Simple RNN</code> 有多少个模型参数。</p><p>如图，先看一下这个拼接后向量，这个向量的维度是 $h_{t-1}$ 的维度加上 $x_t$ 的维度：</p><p>所以 $A$ 一定要有 $shape(h)+shape(x)$ 维度这么多列：</p><p><img src=https://nlp.letout.cn/img/nlp/03/11.png loading=lazy alt=11></p><p>$A$ 的行数等于 $h$ 的维度：</p><p><img src=https://nlp.letout.cn/img/nlp/03/12.png loading=lazy alt=12></p><p>所以，最终矩阵 $A$ 的大小等于：</p><p>$parameter(A) = shape(h) * [shape(h) + shape(x)]$</p><p>这个乘积 $parameter(A)$ 就是 <code>simple RNN</code> 的最终的参数数量。</p><p>我们来搭一个简单的网络。最底层是一个词嵌入层 <code>Word Embedding Layer</code>，它可以把词映射为词向量。</p><p><img src=https://nlp.letout.cn/img/nlp/03/13.png loading=lazy alt=13></p><p>词向量的维度由自己设置（这是一个超参数，我们应该使用交叉验证 <code>cross validation</code> 选择最佳的维度），这里设置 $x$ 的维度是 $32$。</p><p>然后下一层是 <code>Simple RNN Layer</code>，输入的是词向量 $x_i$，输出的是状态 $h_i$。</p><p><img src=https://nlp.letout.cn/img/nlp/03/14.png loading=lazy alt=14></p><p>$h$ 的维度也是由自己设置，我们设置 $h$ 维度为 $32$。这里 $x$ 和 $h$ 的维度都是 $32$，这只是一个巧合而已，$h$ 和 $x$ 的维度通常不一样。</p><p>前面说过，状态向量 $h$ 会积累输入的信息，比如：$h_0$ 包含第一个单词 <code>I</code> 的信息，$h_1$ 包含前两个词 <code>I love</code> 的信息，最后一个状态 $h_t$ 包含整句话 <code>I love the movice so much</code> 的信息。</p><p>我们可以从 <code>PyTorch</code> 中获取所有的状态 $h={h_1, h_2, &mldr;, h_t}$，也可以只获得最后一个状态向量 $h_t$ 的信息。</p><p>$h_t$ 积累了整句话的信息，所有通常使用 $h_t$ 这一个向量就够了，这里我们只使用 $h_t$，把前面的所有状态 ${h_1, h_2, &mldr;, h_{t-1}}$ 全部都丢掉。</p><p><img src=https://nlp.letout.cn/img/nlp/03/15.png loading=lazy alt=15></p><p>$h_t$ 相当于从文本中提取的特征向量，把 $h_t$ 输入这个分类器 $sigmoid(v^T h_t)$，分类器就会输出一个 <code>0</code> 或 <code>1</code> 之间的数值，<code>0</code> 代表了负面电影评价，<code>1</code> 代表正面电影评价。</p><p>然后我们设置超参数：</p><ul><li>设置 <code>vocabulary = 1000</code>，意思是词典里有 <code>10000</code> 个词汇；</li><li><code>embedding_dim = 32</code>，意思是词向量 $x$ 的维度是 $32$；</li><li><code>word_num = 500</code>，意思是每条电影评价有 <code>500</code> 个单词，如果超过 <code>500</code> 个单词，就会被截掉，只保留 <code>500</code> 个，如果不够 <code>500</code>，就用 <code>zero_padding</code> 将句子补成长度为 <code>500</code>；</li><li><code>state_dim = 32</code>，意思是状态 $h$ 的维度等于 $32$。</li></ul><h3 id=-pytorch-实现>👩‍🚒 PyTorch 实现</h3><p>接下来开始搭网络，首先我们定义一个类 <code>Model</code>，我们让它继承 <code>nn.Model</code> 父类。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Model</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocabulary_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>,</span> <span class=n>state_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>super</span><span class=p>(</span><span class=n>Model</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>vocabulary_size</span> <span class=o>=</span> <span class=n>vocabulary_size</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>embedding_dim</span> <span class=o>=</span> <span class=n>embedding_dim</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>state_dim</span> <span class=o>=</span> <span class=n>state_dim</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>Embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>vocabulary_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>RNN</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>RNN</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embedding_dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>state_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>state_dim</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>sigmoid</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sigmoid</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>embedded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>output</span><span class=p>,</span> <span class=n>h_n</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>RNN</span><span class=p>(</span><span class=n>embedded</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>h_n</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>h_n</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>h_n</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>接下来，我们在构造函数 <code>__init__</code> 中定义我们的模型结构，并重载 <code>forword</code> 方法。</p><p>首先是词嵌入层 <code>nn.Embedding()</code>，它是把词映射成向量。</p><p>然后是 simple RNN 层 <code>nn.RNN()</code>，需要指定词向量的维度 <code>embedding_dim</code> 和状态向量 $h$ 的维度 <code>state_dim</code>；</p><p>最后是一个全连接层 <code>nn.Linear()</code>，并且会使用 <code>nn.Sigmoid()</code> 作用于它的结果，输入最后一个状态向量 $h$，输出一个 <code>0、1</code> 之间的数。</p><p><code>PyTorch</code> 中的 <code>RNN</code> 会有两个返回值：<code>output</code>，<code>h_n</code>。</p><ul><li><code>output</code> 是 <code>RNN</code> 所有时刻的状态向量集合（矩阵）；</li><li><code>h_n</code> 是 <code>RNN</code> 中最后一个状态向量。</li></ul><p>这是模型的一个概要， 词嵌入层 <code>Embedding</code> 的输出是一个 $500*32$ 的矩阵，<code>500</code> 的意思是每个句子有 $500$ 个词，<code>32</code> 的意思是每个词用 $32$ 维的词向量表示。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>==========================================================================================
</span></span><span class=line><span class=cl>Layer (type:depth-idx)                   Output Shape              Param #
</span></span><span class=line><span class=cl>==========================================================================================
</span></span><span class=line><span class=cl>Model                                    --                        --
</span></span><span class=line><span class=cl>├─Embedding: 1-1                         [500, 1, 32]              320,000
</span></span><span class=line><span class=cl>├─RNN: 1-2                               [500, 1, 32]              2,112
</span></span><span class=line><span class=cl>├─Linear: 1-3                            [1, 1]                    33
</span></span><span class=line><span class=cl>├─Sigmoid: 1-4                           [1, 1]                    --
</span></span><span class=line><span class=cl>==========================================================================================
</span></span><span class=line><span class=cl>Total params: 322,145
</span></span><span class=line><span class=cl>Trainable params: 322,145
</span></span></code></pre></td></tr></table></div></div><p><code>Simple RNN</code> 每个状态的输出 $h_t$ 都是一个 <code>32</code> 维的向量。我们看一下 RNN 的参数，他有 <code>2080</code> 个参数，它是这样算出来的：</p><p>$shape(h) * (shape(h)+shape(x)) = 32*(32+32)+32 = 2080$</p><p>这是矩阵 $A$ 的大小，后面的 $32$ 的维度来自 <code>intercept</code>，也叫 <code>bias</code>，偏移量。RNN 会默认使用 <code>intercept</code>，但这个不重要，这里暂时不管它。</p><p>剩下的 <code>32</code> 个参数来自于最后一个状态，因为 <code>PyTorch</code> 中的 <code>RNN</code> 会同时输出所有时刻的状态向量集合和最后一个状态向量。</p><h3 id=-运行模型>🧑‍🔧 运行模型</h3><p>搭好模型之后，初始化模型，然后用训练数据拟合模型。</p><p>我们指定算法是 <code>optim.Adam</code>，损失函数是 <code>nn.BCELoss()</code>，评价标准是准确率 <code>accuracy</code>。</p><p>然后用训练数据拟合模型，我让算法训练 <code>3</code> 个 <code>epochs</code>，只让算法运行 <code>3</code> 个 epochs，是出现了过拟合，<code>3</code> 个 <code>epochs</code> 之后，<code>validate accuracy</code> 会变差。提前让算法停止运行称为 <code>early stoping</code> 提前终止训练，使 <code>validate accuracy</code> 变差之前就停止。</p><p>最后，用测试数据评价模型的表现，把测试数据作为输入，调用 <code>model.evaluate()</code>，返回 loss 和 accuracy，测试的 accuracy，测试的 accuracy 是 84.36%，比上一节中的 逻辑回归好很多（75%）。</p><p>刚才搭模型的时候，只使用了RNN最后一个状态 $h_t$，把之前的状态都丢掉了，想用 $h_0$ 到 $h_t$ 所有状态也可以，但并没有太大区别。</p><p>我们让 <code>RNN</code> 的第一个输出 <code>output</code>，它是一个矩阵，矩阵每一行就是一个状态向量 $h$。</p><p>如果用所有状态，就要加一个 <code>flatten</code> 层，将状态矩阵变成一个向量，然后把这个向量作为分类器的输入，来判断电影是正面的，还是负面的。只要把网络稍作改动就可以了。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>start_dim</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>end_dim</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=-simple-rnn-的缺陷>🧰 simple RNN 的缺陷</h2><p>下面看一下 <code>simple RNN</code> 这种简单的模型有什么缺陷。</p><p>举个栗子 🌰 ，现在有这样一个问题，给定半句话，要求预测下一个单词。</p><p><img src=https://nlp.letout.cn/img/nlp/03/16.png loading=lazy alt=16></p><p>输入是 <code>clouds are in the</code>，正确的输出应该是 <code>sky</code>，如果在大量文本中预测 RNN，应该是有能力做出这样的预测的。在这个例子里，<code>RNN</code> 只需要看最近的几个词，尤其是 <code>clouds are</code>，并不需要更多的上下文看的更远。</p><p>这个例子是对 <code>simple RNN</code> 十分有利，<code>simple RNN</code> 特别擅长这种 <code>short-term dependence</code>，<code>simple RNN</code> 不擅长的是 <code>long-term dependence</code>。</p><p><code>RNN</code> 的状态 $h$，和之前所有的输入 $x$ 都有函数依赖关系，照理来说，如果改变输入的单词 $x_1$，所有的状态 $h$ 都会发生变化，但实际上，<code>simple RNN</code> 并没有这种性质，所以很不合理。如果把第 <code>100</code> 个状态向量 $h_{100}$，关于输入 $x_1$ 求导，你会发现导数几乎等于 <code>0</code>。</p><p>$\frac{\partial h_{100}}{\partial x_1} \approx 0$</p><p><img src=https://nlp.letout.cn/img/nlp/03/17.png loading=lazy alt=17></p><p>导数几乎等于 <code>0</code> 说明什么呢？说明当我们改变 $x_1$时，$h_{100}$ 几乎不会发生任何变化，也就是说状态 $h_{100}$ 和 <code>100</code> 步之前的输入 $x_1$ 几乎没有关系，这显然不合理，说明状态 $h_{100}$ 几乎把很多步之前的输入都给忘记了，<code>simple RNN</code> 的这种遗忘会给后续操作造成很多问题。</p><p>再举个栗子 🌰 ，这是很长的一段话，一开始是 <code>I grow up in China when I was a child, ... ...</code> 到了很多句话之后，有这样一句，<code>I speak fluent ...</code>。</p><p><img src=https://nlp.letout.cn/img/nlp/03/18.png loading=lazy alt=18></p><p>下一个词应该是 <code>Chinese</code>，我小时候在中国，所有会说流利的中文，然而 <code>simple RNN</code> 不太可能会做出 <code>Chinese</code> 这个正确的预测，因为 RNN 已经把前文给忘记了。<code>simple RNN</code> 擅长的是 <code>short-term dependence</code>，RNN 看到最近的单词是 <code>speak fluent</code>，所以 RNN 知道下一个单词可能是某种语言，可能是 <code>Chinese、English、French、Japanese</code> 等等，但正确答案是 <code>Chinese</code>，因为上文有 <code>I grow up in china when i was child</code>，simple RNN 就像金鱼一样记忆力只有 <code>7</code> 秒，<code>RNN</code> 根本就不记得上文有这句话，所以 <code>I speak fluent ...</code> 预测单词可能是 <code>English , French</code> 等任何一种语言，未必是 <code>Chinese</code>。</p><h2 id=-总结>🎐 总结</h2><p>最后总结一下这一节的内容：</p><p><code>RNN</code> 是一种神经网络，但是他的结构不同于全连接网络和卷积网络，<code>RNN</code> 适用于文本，语音等时序序列数据，<code>RNN</code> 按照顺序读取每一个词向量，并且在状态向量 $h$ 中积累看到过得信息，$h_0$ 中包含了 $x_0$ 的信息，$h_1$ 中包含了 $x_0$ 和 $x_1$ 的信息，$h_t$ 中积累了之前所有 $x={x_0, x_1, &mldr;, x_t}$ 的信息。</p><p><img src=https://nlp.letout.cn/img/nlp/03/19.png loading=lazy alt=19></p><p>有一种错误的看法是 $h_t$ 中只包含了 $x_t$ 的信息，这是不对的，$h_t$ 中包含了之前所有输入的信息，可以认为 $h_t$ 代表了 <code>RNN</code> 从整个序列中抽取的特征向量，所有我们只需要 $h_t$ 就可以判断电影评价是正面的还是负面的。</p><p><code>simple RNN</code> 有一个参数矩阵 $A$，它可能还会一个 <code>intercept</code> 参数向量 $b$，上面的介绍中忽略了这个参数向量 $b$，这个参数矩阵 $A$ 的维度是：</p><p>$shape(h) * [shape(h) + shape(x)]$</p><p>参数矩阵 $A$ 一开始随机初始化，然后从训练数据上学习。注意：<code>simple RNN</code> 只有一个参数矩阵，不管这个序列有多长，参数矩阵只有一个，所有模块里的参数都是一样的。</p><p><code>RNN</code> 有一个缺点，<code>RNN</code> 的记忆比较短，会遗忘很久之前的输入 $x$，如果这个时间序列很长，有好几十步，最终 <code>RNN</code> 就会忘记了之前的输入。下一节将介绍 <code>LSTM</code>，<code>LSTM</code> 的记忆会比 <code>simple RNN</code> 长很多，但是 <code>RNN</code> 也还是会有遗忘的问题。</p></section><footer class=article-footer><section class=article-tags><a href=/blog/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/>自然语言处理</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css integrity=sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js integrity=sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script></article><aside class=related-contents--wrapper><h2 class=section-title>相关文章</h2><div class=related-contents><div class="flex article-list--tile"><article class=has-image><a href=/blog/p/nlp-in-action/long-short-term-memory/><div class=article-image><img src=https://nlp.letout.cn/img/banner.png loading=lazy data-key=/nlp-in-action/long-short-term-memory data-hash=https://nlp.letout.cn/img/banner.png></div><div class=article-details><h2 class=article-title>Long Shorter Term Memory</h2></div></a></article><article class=has-image><a href=/blog/p/nlp-in-action/text-processing-and-word-embedding/><div class=article-image><img src=https://nlp.letout.cn/img/banner.png loading=lazy data-key=/nlp-in-action/text-processing-and-word-embedding data-hash=https://nlp.letout.cn/img/banner.png></div><div class=article-details><h2 class=article-title>文本处理与词嵌入</h2></div></a></article><article class=has-image><a href=/blog/p/nlp-in-action/data-processing/><div class=article-image><img src=https://nlp.letout.cn/img/banner.png loading=lazy data-key=/nlp-in-action/data-processing data-hash=https://nlp.letout.cn/img/banner.png></div><div class=article-details><h2 class=article-title>数据处理基础</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2023 一层</section><section class=powerby><b><a href=http://beian.miit.gov.cn/ target=_blank rel="nofollow noopener">赣ICP备19004365号-3</a></b><br>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=%s>Stack</a></b> by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#-简介>🌱 简介</a></li><li><a href=#-如何建模时序数据>🔖 如何建模时序数据？</a></li><li><a href=#-rnn>🍀️ RNN</a></li><li><a href=#-simple-rnn>🚀 Simple RNN</a><ol><li><a href=#-为什么需要使用-tanh-作为激活函数>🎨 为什么需要使用 <code>tanh</code> 作为激活函数？</a></li><li><a href=#-simple-rnn-模型参数数量>🏝️ Simple RNN 模型参数数量</a></li><li><a href=#-pytorch-实现>👩‍🚒 PyTorch 实现</a></li><li><a href=#-运行模型>🧑‍🔧 运行模型</a></li></ol></li><li><a href=#-simple-rnn-的缺陷>🧰 simple RNN 的缺陷</a></li><li><a href=#-总结>🎐 总结</a></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>