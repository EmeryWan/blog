<!doctype html><html lang dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="这一节，将介绍 LSTM (Long Shorter Term Memory)，以及用 pytorch 实现 LSTM 。 LSTM 是一种 RNN 模型，是对 simple RNN 的改进，LSTM 可以避免梯度消失的问题，可以有更长的记忆。LS"><title>Long Shorter Term Memory</title><link rel=canonical href=https://emerywan.github.io/blog/p/nlp-in-action/long-short-term-memory/><link rel=stylesheet href=/blog/scss/style.min.bfed2465c829134957c445783af0e826d7477e4532d5a709f556db8c69272479.css><meta property="og:title" content="Long Shorter Term Memory"><meta property="og:description" content="这一节，将介绍 LSTM (Long Shorter Term Memory)，以及用 pytorch 实现 LSTM 。 LSTM 是一种 RNN 模型，是对 simple RNN 的改进，LSTM 可以避免梯度消失的问题，可以有更长的记忆。LS"><meta property="og:url" content="https://emerywan.github.io/blog/p/nlp-in-action/long-short-term-memory/"><meta property="og:site_name" content="一层"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="自然语言处理"><meta property="article:published_time" content="2022-03-19T20:36:14+08:00"><meta property="article:modified_time" content="2022-03-19T20:36:14+08:00"><meta property="og:image" content="https://nlp.letout.cn/img/nlp/banner.png"><meta name=twitter:title content="Long Shorter Term Memory"><meta name=twitter:description content="这一节，将介绍 LSTM (Long Shorter Term Memory)，以及用 pytorch 实现 LSTM 。 LSTM 是一种 RNN 模型，是对 simple RNN 的改进，LSTM 可以避免梯度消失的问题，可以有更长的记忆。LS"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nlp.letout.cn/img/nlp/banner.png"><link rel="shortcut icon" href=https://emerywan.github.io/blog/imgs/favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog><img src=/blog/img/avatar_hud10ed4afecf1b63b1391b35c2778f738_36574_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/blog>一层</a></h1><h2 class=site-description>迎着这岁月的风。</h2></div></header><ol class=social-menu><li><a href=https://github.com/emerywan target=_blank title=GitHub><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>首页</span></a></li><li><a href=/blog/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>归档</span></a></li><li><a href=/blog/categories/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg><span>分类</span></a></li><li><a href=/blog/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>关于</span></a></li><li><a href=/blog/link/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>链接</span></a></li><li><a href=/blog/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>搜索</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/blog/p/nlp-in-action/long-short-term-memory/><img src=https://nlp.letout.cn/img/nlp/banner.png loading=lazy alt="Featured image of post Long Shorter Term Memory"></a></div><div class=article-details><header class=article-category><a href=/blog/categories/nlp-in-action/ style=background-color:#d3adf7;color:#fff>NLP in action</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/nlp-in-action/long-short-term-memory/>Long Shorter Term Memory</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>03 - 19, 2022</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>阅读时长: 5 分钟</time></div></footer></div></header><section class=article-content><p>这一节，将介绍 <code>LSTM (Long Shorter Term Memory)</code>，以及用 <code>pytorch</code> 实现 <code>LSTM </code>。</p><p><code>LSTM</code> 是一种 <code>RNN</code> 模型，是对 <code>simple RNN</code> 的改进，<code>LSTM</code> 可以避免梯度消失的问题，可以有更长的记忆。<code>LSTM</code> 的论文在 1997 年发表。</p><blockquote><p>Hochreiter and Schmidhuber. Long short-term memory. Neural computation, 1997.</p></blockquote><h2 id=-lstm>🔖 LSTM</h2><p><code>LSTM</code> 也是一种循环神经网络，原理跟 <code>simple RNN</code> 差不多，每当读取一个新的输入 $x$，就会更新状态 $h$。</p><p><img src=https://nlp.letout.cn/img/nlp/04/1.png loading=lazy alt=1></p><p><code>LSTM</code> 的结构比 <code>simple RNN</code> 要复杂很多，<code>simple RNN</code> 只有一个参数矩阵， <code>LSTM</code> 有四个参数矩阵。接下来我们具体来看看 <code>LSTM</code> 的内部结构。</p><h3 id=-传送带>🚠 传送带</h3><p><code>LSTM</code> 最重要的设计是这个传送带 <code>Conveyor belt</code>，即为向量 $C$。过去的信息通过传送带，直接送到下一个时刻，不会发生太大的变化。<code>LSTM</code> 就是靠传送带来避免梯度消失的问题。</p><p><img src=https://nlp.letout.cn/img/nlp/04/2.png loading=lazy alt=2></p><p><code>LSTM</code> 中有很多个门 <code>gate</code>，可以有选择的让信息通过。</p><h3 id=-forgate-gate>🚪 Forgate Gate</h3><p>首先介绍 <code>forget gate</code> 遗忘门。遗忘门由 ☘️ <code>sigmoid</code> 函数，和 🍀 元素积 <code>element wise multiplication</code> 两部分组成。</p><p><img src=https://nlp.letout.cn/img/nlp/04/3.png loading=lazy alt=3></p><p>🌼 输入 <code>sigmoid</code> 的是一个向量 $a$，<code>sigmoid</code> 作用到向量 $a$ 的每一个元素上，把每一个元素都压到 <code>0</code> 和 <code>1</code> 之间。</p><p>举个例子，假如向量 $a$ 是：<code>[1, 3, 0, -2]</code>，那么，<code>sigmoid</code> 函数将分别作用在这四个元素上。然后分别输出：<code>[0.73, 0.95, 0.5, 0.12]</code> 。</p><p><img src=https://nlp.letout.cn/img/nlp/04/4.png loading=lazy alt=4></p><p>输入的向量 $a$，与输出的向量 $f$ 应该有相同的维度，这个例子里，向量 $a$ 是四维的，向量 $f$ 也会是四维的。</p><p>🌸 算出 $f$ 向量之后，计算传送带向量 $c$ 与遗忘门向量 $f$ 的元素积。元素积 <code>element wise multiplication</code> 是这样算的：</p><p>$c$ 和 $f$ 都是四维的向量，将它们的每一个元素分别相乘。所以元素积的结果也是个四维的向量。</p><p><img src=https://nlp.letout.cn/img/nlp/04/5.png loading=lazy alt=5></p><p>这个遗忘门 $f$，有选择的让传送带 $c$ 的值通过：</p><ul><li><p>🌰 假如 $f$ 的一个元素是 $0$，那么 $c$ 对应的元素不能通过，对应的输出是 $0$；</p></li><li><p>🌰 假如 $f$ 的一个元素是 $1$，那么 $c$ 对应的元素就全部通过，对应的输出是 $c$ 本身。</p></li></ul><p>遗忘门 $f$ 具体是这么算出来的：首先看这张结构图，$f_t$ 是上一个状态 $h_{t-1}$，与当前输入 $x$ 的函数。</p><p><img src=https://nlp.letout.cn/img/nlp/04/6.png loading=lazy alt=6></p><p>把状态 $h_{t-1}$ 与输入 $x_t$ 做拼接 <code>concatnation</code>，得到更高维度的向量。然后计算矩阵 $w_f$ 与这个向量的乘积，得到一个向量，再用 <code>sigmoid</code> 函数，得到向量 $f_t$，$f_t$ 的每一个元素都介于 <code>0</code> 和 <code>1</code> 之间，遗忘门有一个参数矩阵 $w_f$，需要通过 <strong>反向传播</strong> 从训练数据里学习。</p><p><img src=https://nlp.letout.cn/img/nlp/04/7.png loading=lazy alt=7></p><h3 id=-input-gate>🚪 Input Gate</h3><p>刚才讲了遗忘门，现在来看一看 <code>input gate</code> 输入门。在这张结构图里，输入门 $i_t$，依赖于旧的状态向量 $h_{t-1}$，和新的输入 $x_t$。</p><p><img src=https://nlp.letout.cn/img/nlp/04/8.png loading=lazy alt=8></p><p>输入门 $i_t$ 的计算类似于遗忘门，把旧的状态 $h_{t-1}$，与新的输入 $x_t$ 做拼接 <code>concatnation</code>，得到更高维的向量。</p><p>然后计算矩阵 $w_i$ 与这个向量的乘积得到一个向量，最后使用激活函数 <code>sigmod</code>，得到向量 $i_t$（$i_t$ 的每一个元素都介于 $0$ 和 $1$ 之间）。</p><p><img src=https://nlp.letout.cn/img/nlp/04/9.png loading=lazy alt=8></p><p>输入门也有自己的参数矩阵，计作 $W_i$，$W_i$ 也需要从训练数据中学习。</p><h3 id=-new-value>🆕 New Value</h3><p>还需要计算新的输入值 <code>new value</code> $\widetilde{c}_t$，$\widetilde{c}_t$ 是个向量，计算方法跟遗忘门和输入门都很像。也是把旧状态 $h_{t-1}$，与新输入 $x_t$ 做拼接，再乘到参数矩阵上。</p><p>区别在于激活函数不是 <code>sigmoid</code>，而是双曲正切函数 <code>tanh</code>，所以算出的向量 $\widetilde{c}_t$ 的元素都介于 <code>(-1, +1)</code>。</p><p><img src=https://nlp.letout.cn/img/nlp/04/10.png loading=lazy alt=8></p><p>计算 <code>new value</code> $\widetilde{c}_t$，也需要一个单独的参数矩阵矩阵 $w_c$。</p><h3 id=-更新-传输带>🚂 更新 传输带</h3><p>我们已经算出了遗忘门 $f_t$，输入门 $i_t$，以及新的输入值 $\widetilde{c}_t$，我们还知道传送带旧的值 $c_{t-1}$，现在可以更新传送带 $c$ 了。</p><p><img src=https://nlp.letout.cn/img/nlp/04/11.png loading=lazy alt=8></p><p>1️⃣ 计算遗忘门 $f_t$ 和传送带旧的值 $c_{t-1}$ 的元素积。</p><p>遗忘门 $f_t$，和传送带 $c_{t-1}$ 是维度相同的向量，算出的乘积也是个向量。遗忘门 $f_t$，可以选择性的遗忘 $c_{t-1}$ 中的一些元素，如果 $f_t$ 中的一个元素是 $0$，那么 $c_{t-1}$ 相应的元素就会被遗忘。</p><p>上一步通过 🚪 遗忘门 选择性删除掉了传送带 $c_{t-1}$ 的一些元素，现在要往传送带上添加新的信息。</p><p>2️⃣ 计算输入门 $i_t$，和新的输入值 $\widetilde{c}_t$ 的元素积。</p><p>输入门 $i_t$ 和新的值 $\widetilde{c}_t$ 都是维度相同的向量，他们的乘积也是维度相同的向量，把乘积加到传送带上，这样就完成了对传送带的一轮更新。</p><p>用遗忘门删除了传送带上的一些信息，然后用遗忘门输入加入新的信息，得到了传送带新的值 $c_t$，到现在，已经更新完传送带 $c$ 。</p><h3 id=-output-gate>🚪 Output Gate</h3><p>最后一步是计算 <code>LSTM</code> 的输出，也就是状态向量 $h_t$。</p><p>$h_t$ 是这么计算的：首先计算输出门 $o_t$，输出门 $o_t$ 跟前面的遗忘门，输入门的计算基本一样。</p><p>把旧的状态 $h_{t-1}$，与新的输入 $x_t$ 做拼接，得到更高维的向量，然后算矩阵 $W_o$ 与这个向量的乘积，得到一个向量，最后使用激活函数 <code>sigmod</code> 得到向量 $o_t$。$o_t$ 的每一个元素都介于 <code>(0, 1)</code>，输出门也有自己的参数向量 $W_o$，$W_o$ 也需要从训练数据中学习。</p><p><img src=https://nlp.letout.cn/img/nlp/04/12.png loading=lazy alt=12></p><p>现在计算状态向量 $h_t$，对传送带 $c_t$ 的每一个元素求双曲正切<code>tanh</code>，把元素全都压到 <code>(-1, +1)</code> 区间。</p><p>然后，求这两个向量的元素积，这个红色向量是刚刚求出的输出门 $o_t$，这样就得到了状态向量 $h_t$。</p><p><img src=https://nlp.letout.cn/img/nlp/04/13.png loading=lazy alt=13></p><p>看一下结构图，$h_t$ 他有两份 <code>copys</code>，$h_t$ 的一份 <code>copy</code> 传输到了下一步，另一份 <code>copy</code> 成了 <code>LSTM</code> 的输出。</p><p>到第 <code>t</code> 步为止，一共有 <code>t</code> 个向量 $x$ 被输入了 <code>LSTM</code>，我们可以认为所有这些 $x$ 向量的信息，都积累在了状态 $h_t$ 里面。</p><h2 id=-lstm-的参数数量>🧮 LSTM 的参数数量</h2><p>我们来算一下 <code>LSTM</code> 的参数数量，<code>LSTM</code> 有 ❶ 遗忘门；❷ 输入门；❸ 新的输入；❹ 输出门。</p><p><img src=https://nlp.letout.cn/img/nlp/04/14.png loading=lazy alt=14></p><p>这四个模块都有各自的参数矩阵 $w$，所以一共有 <code>4</code> 个参数矩阵，矩阵的行数是：$shape(h)$，列数是： $shape(h)+shape(x)$</p><p>所以，<code>LSTM</code> 参数的数量是：</p><p>$4 * shape(h) * [ shape(h) + shape(x)]$</p><h2 id=-实现-lstm>🛠 实现 LSTM</h2><p><strong>Doing</strong></p><h2 id=-总结>🎐 总结</h2><p>总结一下这一节的内容，这节介绍了 <code>LSTM</code> 模型和用 <code>PyTorch</code> 的实现。</p><p><code>LSTM</code> 和 <code>simple RNN</code> 主要的区别，是用了一条传送带，让过去的信息可以很容易传输到下一时刻，这样就有了更长的记忆。</p><p><code>LSTM</code> 的表现总是比 <code>simple RNN</code> 要好，所以当我们想使用 <code>RNN</code> 的时候就用 🙋‍♂️ <code>LSTM</code> 模型，而不要用 🙅‍♂️ <code>simple RNN</code> 模型。</p><p><code>LSTM</code> 有四个组件，分别是：</p><ul><li>🚪 <code>Forget Gate</code> 遗忘门</li><li>🚪 <code>Input Gate</code> 输入门</li><li>🆕 <code>New Value</code> 新的输入</li><li>🚪 <code>Output Gate</code> 输出门</li></ul><p>这四个组件各自有一个参数矩阵，所以一共有四个参数矩阵，<code>LSTM</code> 参数的数量是：</p><p>$4 * shape(h) * [ shape(h) + shape(x)]$</p><p>下一节将介绍：</p><ul><li><code>stacked RNN</code></li><li><code>bi-directional RNN</code></li><li>预训练</li></ul><h2 id=-参考>⛓ 参考</h2><ul><li>🔗 <a class=link href=https://github.com/wangshusen/DeepLearning/blob/master/Slides/9_RNN_3.pdf target=_blank rel=noopener>https://github.com/wangshusen/DeepLearning/blob/master/Slides/9_RNN_3.pdf</a></li><li>🔗 <a class=link href=https://colah.github.io/posts/2015-08-Understanding-LSTMs/ target=_blank rel=noopener>https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li><li>🔗 <a class=link href="https://www.youtube.com/watch?v=vTouAvxlphc" target=_blank rel=noopener>https://www.youtube.com/watch?v=vTouAvxlphc</a></li><li>🔗 <a class=link href=https://www.bilibili.com/video/BV1UK4y1d7xa target=_blank rel=noopener>https://www.bilibili.com/video/BV1UK4y1d7xa</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/blog/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/>自然语言处理</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css integrity=sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js integrity=sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script></article><aside class=related-contents--wrapper><h2 class=section-title>相关文章</h2><div class=related-contents><div class="flex article-list--tile"><article class=has-image><a href=/blog/p/nlp-in-action/text-processing-and-word-embedding/><div class=article-image><img src=https://nlp.letout.cn/img/nlp/banner.png loading=lazy data-key=/nlp-in-action/text-processing-and-word-embedding data-hash=https://nlp.letout.cn/img/nlp/banner.png></div><div class=article-details><h2 class=article-title>文本处理与词嵌入</h2></div></a></article><article class=has-image><a href=/blog/p/nlp-in-action/simple-rnn/><div class=article-image><img src=https://nlp.letout.cn/img/nlp/banner.png loading=lazy data-key=/nlp-in-action/simple-rnn data-hash=https://nlp.letout.cn/img/nlp/banner.png></div><div class=article-details><h2 class=article-title>RNN</h2></div></a></article><article class=has-image><a href=/blog/p/nlp-in-action/data-processing/><div class=article-image><img src=https://nlp.letout.cn/img/nlp/banner.png loading=lazy data-key=/nlp-in-action/data-processing data-hash=https://nlp.letout.cn/img/nlp/banner.png></div><div class=article-details><h2 class=article-title>数据处理基础</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2022 一层</section><section class=powerby><b><a href=http://beian.miit.gov.cn/ target=_blank rel="nofollow noopener">赣ICP备19004365号-3</a></b><br>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=%s>Stack</a></b> by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#-lstm>🔖 LSTM</a><ol><li><a href=#-传送带>🚠 传送带</a></li><li><a href=#-forgate-gate>🚪 Forgate Gate</a></li><li><a href=#-input-gate>🚪 Input Gate</a></li><li><a href=#-new-value>🆕 New Value</a></li><li><a href=#-更新-传输带>🚂 更新 传输带</a></li><li><a href=#-output-gate>🚪 Output Gate</a></li></ol></li><li><a href=#-lstm-的参数数量>🧮 LSTM 的参数数量</a></li><li><a href=#-实现-lstm>🛠 实现 LSTM</a></li><li><a href=#-总结>🎐 总结</a></li><li><a href=#-参考>⛓ 参考</a></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>