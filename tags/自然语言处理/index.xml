<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>自然语言处理 on 一层</title><link>https://blog.letout.cn/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</link><description>Recent content in 自然语言处理 on 一层</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sat, 30 Apr 2022 21:14:45 +0800</lastBuildDate><atom:link href="https://blog.letout.cn/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/index.xml" rel="self" type="application/rss+xml"/><item><title>文本处理与词嵌入</title><link>https://blog.letout.cn/p/nlp-in-action/text-processing-and-word-embedding/</link><pubDate>Sat, 30 Apr 2022 21:14:45 +0800</pubDate><guid>https://blog.letout.cn/p/nlp-in-action/text-processing-and-word-embedding/</guid><description>&lt;img src="https://nlp.letout.cn/img/nlp/banner.png" alt="Featured image of post 文本处理与词嵌入" />&lt;p>本节主要内容为 &lt;strong>文本处理&lt;/strong> &lt;code>Text Processing&lt;/code> 和 &lt;strong>词嵌入&lt;/strong> &lt;code>Word Embedding&lt;/code>。本节和下面两节内容都会使用 IMDb 电影评论的数据，用来搭建机器学习模型分析电影评论。&lt;/p>
&lt;h2 id="-imdb">🌱 IMDb&lt;/h2>
&lt;p>&lt;a class="link" href="https://zh.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91%E7%94%B5%E5%BD%B1%E8%B5%84%E6%96%99%E5%BA%93" target="_blank" rel="noopener"
>IMDb&lt;/a> 是最有名的电影评论网站，用户可以在 IMDb 上给电影打分，1 分是非常讨厌，10 分是非常喜欢，如果乐意，还可以写一段电影评论。&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/02/imdb.png"
loading="lazy"
alt="IMDb"
>&lt;/p>
&lt;p>如果不给你看分数，只给你看评论，你大概能猜到用户打的分数，但你的猜测可能不太准确。如果换种方式，让你判断电影评论是 &lt;strong>正面&lt;/strong> &lt;code>positive&lt;/code> 的还是 &lt;strong>负面&lt;/strong> &lt;code>negative&lt;/code> 的，你应该会有很高的准确率。有人从 IMDb 上爬了 5 万条电影评论，这些评论都是很极端的，都是强烈的喜欢，或者强烈反感。这个二分类问题对于人来说很简单，人读一下电影评论就能轻易知道这是正面评价还是负面评价，人应该能有 100% 的准确率，这个数据集被分成两半，2 万 5 千条作为训练数据 &lt;code>Train&lt;/code>，另外 2 万 5 千条作为测试数据 &lt;code>Test&lt;/code>。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>你可以在下面的链接中获取到数据集：&lt;a class="link" href="https://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="noopener"
>https://ai.stanford.edu/~amaas/data/sentiment/&lt;/a>&lt;/p>
&lt;h2 id="-文本处理文本处理">🔖 文本处理文本处理&lt;/h2>
&lt;p>在词嵌入 &lt;code>Word Embedding&lt;/code> 和搭建机器学习模型之前，首先要进行文本处理，将文本变成序列 &lt;code>Sequence&lt;/code>，文本处理很无聊，但我们应该重视它，文本处理的好坏，会直接影响机器学习模型的准确率。&lt;/p>
&lt;h3 id="-tokenization">🚀 Tokenization&lt;/h3>
&lt;p>文本处理的第一步是 &lt;code>Tokenization&lt;/code>，把文本分割成很多 &lt;code>tokens&lt;/code>，这里我们把文本分割成很多单词，一个 &lt;code>token&lt;/code> 就是一个单词（假如你把文本分割成字符，那么一个 &lt;code>token&lt;/code> 就是一个字符），做完 &lt;code>Tokenization&lt;/code>，一个很长的字符串就被分割成一个很多单词的列表。&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/02/1.png"
loading="lazy"
alt="1"
>&lt;/p>
&lt;p>&lt;code>Tokenization&lt;/code> 看起来很简单，但是讲究很多。比如：&lt;/p>
&lt;ul>
&lt;li>🌰 是否应该把大写变成小写？&lt;/li>
&lt;/ul>
&lt;p>通常情况下应该把大写变成小写，大小写单词通常是一个意思；但有时候会混淆单词（Apple -&amp;gt; apple），比如 Apple 是苹果公司，apple 是水果，大小写的 apple 并不是相同的单词。&lt;/p>
&lt;ul>
&lt;li>🌰 去除停用词。&lt;code>stop word&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>有些应用会去除 stop word，它是 the、a、of 等最高频的单词，这些词几乎出现在所有的句子里，对这个二分类问题几乎是没有帮助。&lt;/p>
&lt;ul>
&lt;li>🌰 拼写纠错。&lt;/li>
&lt;/ul>
&lt;p>用户发电影评论的时候，大部分情况下并不会仔细检查，所以写的东西难免会有拼写错误，所以做拼写纠错通常是有用的。&lt;/p>
&lt;p>这里只是举了几个例子，实际上做 &lt;code>Tokenization&lt;/code> 的时候需要做大量的工作，&lt;code>Tokenization&lt;/code> 看似简单，但实际上并不容易。&lt;/p>
&lt;h3 id="-build-dictionary">🧰 Build Dictionary&lt;/h3>
&lt;p>第二步是建立一个字典。可以先统计词频，去掉一些低频词，让后让每个单词对应一个正整数，比如让 &lt;code>the -&amp;gt; 1; cat -&amp;gt; 2; sat -&amp;gt; 3&lt;/code>。有了这个字典，就可以把每个单词对应到一个整数，这样一来，一句话就可以用正整数的列表表示，这个列表被称为序列 &lt;code>Sequences&lt;/code>。&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/02/2.png"
loading="lazy"
alt="2"
>&lt;/p>
&lt;p>如果有必要的话，还得进一步做 &lt;code>one-hot encoding&lt;/code>，把单词表示成 &lt;code>one-hot&lt;/code> 向量。&lt;/p>
&lt;p>在电影评论的例子里，数据是 5 万条电影评论，每条电影评论可以表示成一个字符串。做完 &lt;code>Tokenization&lt;/code> 和 &lt;code>Encoding&lt;/code> 后，每条电影评论都会转换成一个 &lt;code>Sequences&lt;/code>，也就是一个正整数的列表。&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/02/3.png"
loading="lazy"
alt="3"
>&lt;/p>
&lt;p>电影评论有长有短，有人只能写几个字的评论，有人能洋洋洒洒写几千字，所以得到的这些 &lt;code>Sequences&lt;/code> 的长度也各不相同。比如这两条 &lt;code>Sequences&lt;/code> 的长度分别是 52 和 90。&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/02/4.png"
loading="lazy"
alt="4"
>&lt;/p>
&lt;p>这就造成了一个问题，训练数据没有对齐，每条 &lt;code>Sequences&lt;/code> 都有不同的长度。做机器学习的时候，我们需要把数据存储到矩阵或者张量里，每条序列都得有相同的长度，需要把序列对齐。&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/02/5.png"
loading="lazy"
alt="5"
>&lt;/p>
&lt;p>解决方案是这样的：我们可以固定长度为 $w$。假如一个序列长度太长，超过了 $w$ 个词，就砍掉前面的词，只保留最后面 $w$个词（当然保留最前面 $w$ 个词也同样可以）；假如一个序列太短，不到 $w$ 个词，那么就做 &lt;code>zero padding&lt;/code> 用 0 来补齐，把长度增加到 $w$。&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/02/6.png"
loading="lazy"
alt="6"
>&lt;/p>
&lt;p>这样一来，所有序列的长度都是 $w$，可以存储到一个矩阵里。&lt;/p>
&lt;h2 id="-词嵌入">🍀️ 词嵌入&lt;/h2>
&lt;p>文本处理已经完成了，现在每个词都用一个正整数来表示，下一步是 &lt;code>Word Embedding&lt;/code>，把每个词都表示为一个一维向量。&lt;/p>
&lt;p>现在每个单词都用一个数字来表示，该怎么把这些 &lt;code>Categorical&lt;/code> 特征表示为数值向量呢？&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>显然可以做 &lt;code>one-hot encoding&lt;/code>，用一个 &lt;code>one-hot&lt;/code> 向量来表示一个单词。&lt;!-- raw HTML omitted -->
比如 &lt;code>good: index = 2&lt;/code>，于是使用标准正交积 $e_2$ 来表示，它的第二个元素是 1，其余元素都是 0，$e_2=[0, 1, 0, 0, &amp;hellip;, 0]$&lt;/p>
&lt;p>假如 &lt;code>vocabulary = v&lt;/code>，也就是说字典里一共有 $v$ 个单词，那么就需要维度 &lt;code>dimension = v&lt;/code> 的 &lt;code>one-hot&lt;/code> 向量，要是字典里有 1 万个单词，那么这些 &lt;code>one-hot&lt;/code> 向量都是 1 万维的，这样的向量维度是在太高了。下一节介绍 RNN 的时候你会看到，RNN 的参数数量正比于输入向量的维度，我们肯定不想让输入的向量是 1 万维的，否则一层 RNN 将会有好几十万个参数。所以我们要做 &lt;code>Word Embedding&lt;/code>，把这些高维 &lt;code>one-hot&lt;/code> 向量映射到低维向量。&lt;/p>
&lt;p>具体做法是吧 &lt;code>one-hot&lt;/code> 向量 $e_i$ 乘到参数矩阵 $P^T$ 上，矩阵 $P^T$ 的大小是 $d*v$。其中 $d$ 是词向量的维度，由用户自己决定；$v$ 是 &lt;code>vocabulary&lt;/code>，表示字典里单词的数量。&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/02/7.png"
loading="lazy"
alt="7"
>&lt;/p>
&lt;p>矩阵的乘法的结果记做向量 $x_i$，$x_i$ 就是一个词向量，维度是 $d*1$，如果 &lt;code>one-hot&lt;/code> 向量 $e$ 的第三个元素是 1，那么 $x_i$ 就是 $P^T$ 矩阵的第三列，可以看出，$P^T$ 矩阵每一列都是一个词向量。&lt;/p>
&lt;p>同理，下面这个参数矩阵 $P$ 的每一行都是一个词向量。这个矩阵的行数是 $v$，也就是 &lt;code>vocabulary&lt;/code>；每一行对应一个单词，矩阵的列数是 $d$，$d$ 是用户决定的，$d$ 的大小会影响机器学习模型的表现，应该用 交叉验证 &lt;code>Cross Validation&lt;/code> 用来选择一个比较好的 $d$。&lt;/p>
&lt;p>字典里的第一个词的是 &lt;code>movie&lt;/code>，那么第一行就是 &lt;code>movie&lt;/code> 的词向量；字典里的第二个词是 &lt;code>good&lt;/code>，那么第二行就是 &lt;code>good&lt;/code> 的词向量。&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/02/8.png"
loading="lazy"
alt="8"
>&lt;/p>
&lt;p>我们的任务是判断电影评论是正面的还是负面的，这个参数矩阵是从训练数据中学习出来的，所以这些词向量都带有感情色彩，假如这些词向量都是二维的，我们就可以在平面坐标系中标出这些词向量。&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/02/9.png"
loading="lazy"
alt="9"
>&lt;/p>
&lt;p>&lt;code>fantastic; good; fun&lt;/code> 这些词向量都带有正面情感，所以这三个词的词向量学出来都比较接近；同理，&lt;code>poor; boring; mediocre&lt;/code> 这些词带有负面情感，所以学出来的词同样也应该比较接近，但是这些词的词向量应该远离正面色彩的词向量。像 &lt;code>movie; is&lt;/code> 这样的中性词，没有感情色彩，它们应该在中间。&lt;/p>
&lt;h2 id="-总结">🎐 总结&lt;/h2>
&lt;p>最后总结一下这一章的内容。&lt;/p>
&lt;p>这一节上半部分，说明了文本处理是什么样的。给我们一条电影评论，首先做 &lt;code>Tokenization&lt;/code>，把电影评论分割成很多单词，然后把很多单词编码成数字，这样一整条电影评论就可以很多正整数来表示，我们把这个正整数序列叫做 &lt;code>Sequences&lt;/code>，就是神经网络中 &lt;code>Embedding&lt;/code> 层的输入。由于电影评论的长短不一，得到的 &lt;code>Sequence&lt;/code> 的长短也不一样，没办法存储在一个矩阵里，解决方案是 &lt;code>Alignment&lt;/code> 对齐。假设最大长度为 &lt;code>20&lt;/code>，如果长度大于&lt;code>20&lt;/code>，就只保留最后 &lt;code>20&lt;/code> 个单词；如果长度不到 &lt;code>20&lt;/code>，就用 &lt;code>0&lt;/code> 补齐，把长度增加到 &lt;code>20&lt;/code>。这样一来，每个 &lt;code>Sequences&lt;/code> 长度都相同。&lt;/p></description></item><item><title>数据处理基础</title><link>https://blog.letout.cn/p/nlp-in-action/data-processing/</link><pubDate>Sat, 30 Apr 2022 20:36:14 +0800</pubDate><guid>https://blog.letout.cn/p/nlp-in-action/data-processing/</guid><description>&lt;img src="https://nlp.letout.cn/img/nlp/banner.png" alt="Featured image of post 数据处理基础" />&lt;h2 id="-类别特征">🌱 类别特征&lt;/h2>
&lt;p>机器学习的数据通常有 &lt;strong>类别特征&lt;/strong> &lt;code>Categorical Features&lt;/code> ，我们需要把类别特征 &lt;code>Categorical Features&lt;/code> 转化成机器学习模型能理解的数值特征，下面使用一个例子来具体讲解类别特征数据的处理。&lt;/p>
&lt;p>这张表的每一行是一个人的数据，包括：年龄、性别、国籍，我们需要把这些数据变成机器学习模型可以理解的数值特征。&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/01/table.png"
loading="lazy"
alt="table"
>&lt;/p>
&lt;p>表格的第一列是年龄，年龄本身就是数值特征，所以可以不用做处理，数值特征的特点是可以比较大小，比如 &lt;code>35&lt;/code> 岁的人比 &lt;code>31&lt;/code> 岁的年龄大。&lt;/p>
&lt;p>第二列是性别，性别是二元特征，我们可以用一个数来表示性别。用 &lt;code>0&lt;/code> 表示女性，用 &lt;code>1&lt;/code> 表示男性。这样一来，性别就表示为一个标量：&lt;code>0&lt;/code> / &lt;code>1&lt;/code>。&lt;/p>
&lt;p>第三列是国籍，比如中国，美国，印度。国籍是类别特征，机器学习并不理解国籍，所以我们要把国籍编码成数值向量。世界上约有 &lt;code>197&lt;/code> 个国家，我们先用一个 &lt;code>[1 - 197]&lt;/code> 的整数表示一个国家。可以建立一个字典，把国籍映射成一个 &lt;code>[1 - 197]&lt;/code> 的整数。比如：&lt;code>China:1; US:2; India:3; Japan:4; Germany:5&lt;/code>。&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>&lt;strong>我们要从 1 开始计算，而不能从 0 开始计算。&lt;/strong>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>做这种映射，国籍就表示成 &lt;code>[1 - 197]&lt;/code> 之间的整数。仅仅把国籍表示成 &lt;code>[1 - 197]&lt;/code> 的整数还是不行，一个整数只是一种类别，它们之间不能比较大小。&lt;code>US:2; India:3&lt;/code> 这个数字并不表示印度大于美国，这些整数只是类别而已，并不是真正的数值特征。&lt;/p>
&lt;p>所以要进一步对国籍做 &lt;code>one-hot encoding&lt;/code> ，用 &lt;code>one-hot&lt;/code> 向量来表示国籍：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">China -&amp;gt; 1 -&amp;gt; [1,0,0,0,...,0]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">US -&amp;gt; 2 -&amp;gt; [0,1,0,0,...,0]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>比如，中国对应 1，所以用 197 维的 &lt;code>one-hot&lt;/code> 向量 &lt;code>[1,0,0,0...,0]&lt;/code> 来表示，其中第一个元素为 1，其余元素都是 0；美国对应 2，这个 197 维的向量 &lt;code>[0,1,0,0...,0]&lt;/code> 第二个元素是 1，其余元素都是 0。这样一来，每个国籍就由一个 &lt;code>one-hot&lt;/code> 向量表示，一共有 197 个国家，所以每个向量都是 197 维的。&lt;/p>
&lt;p>&lt;!-- raw HTML omitted -->我们要从 1 开始计算，而不能从 0 开始计算。&lt;!-- raw HTML omitted -->
因为我们要把 0 保留，用来表示未知或者缺失的国籍。数据库里面经常会有缺失的数据（比如用户没有填写国籍），这样缺失的国籍就用 0 来表示，它的 &lt;code>one-hot&lt;/code> 向量就是一个全 0 的向量&lt;code>[0,0,0,0...,0]&lt;/code>。&lt;/p>
&lt;p>下面这个例子中，我们用一个 199 维表示一个人的特征。比如这个人 28 岁，女性，国籍是中国。&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/01/1.png"
loading="lazy"
alt="1"
>&lt;/p>
&lt;p>其中，一个维度表示年龄，一个维度表示性别，一个 197 维的 &lt;code>one-hot&lt;/code> 向量表示国籍。&lt;/p>
&lt;p>这个例子里，这个 36 岁，男性，国籍未知的人的特征是这个 199 维的向量，我们用一个 197 维的全 0 向量表示未知国籍。&lt;/p>
&lt;h2 id="-为什么要用-one-hot-向量表示特征">🔖 为什么要用 one-hot 向量表示特征&lt;/h2>
&lt;p>在处理类别特征的时候，我们使用 &lt;code>one-hot&lt;/code> 向量表示国籍，每个国籍都用 197 维的向量表示。为什么要用 &lt;code>one-hot&lt;/code> 向量而不用一个数字表示呢？比如用 1 表示中国，2 表示美国，3 表示印度。这样一来，名字就变成了数字，可以做数值计算，而且用一个数字表示的话，可以节省 197 倍的存储空间。当然这是不行的。否则我们就不需要 &lt;code>one-hot encoding&lt;/code> 了。&lt;/p>
&lt;p>假设我们使用 &lt;code>1 -&amp;gt; China; 2 -&amp;gt; US; 3 -&amp;gt; India&lt;/code>。那么将中国 &lt;code>1&lt;/code> 和美国 &lt;code>2&lt;/code> 的特征加起来：&lt;code>1+2=3&lt;/code> ，相当于 “中国 + 美国 = 印度”。这样的特征完全不合理。&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/01/2.png"
loading="lazy"
alt="2"
>&lt;/p>
&lt;p>使用 &lt;code>one-hot&lt;/code> 特征向量更合理。将 China 和 US 的 &lt;code>one-hot&lt;/code> 向量加起来，得到 &lt;code>[1,1,0,0,...,0]&lt;/code>，第一个和第二个元素都是 1，其余元素都是 0，这个特征向量的解释是：既有中国国籍，又有美国国籍。&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/01/3.png"
loading="lazy"
alt="3"
>&lt;/p>
&lt;p>所以做机器学习的时候，不能用一个标量来表示一个类别特征，这种特征做法求和等数值计算是没有意义的。正确的做法是使用 &lt;code>one-hot&lt;/code> 向量来表示类别特征。&lt;/p>
&lt;h2 id="-处理文本数据的流程">🚀 处理文本数据的流程&lt;/h2>
&lt;p>在自然语言处理的应用中，数据就是文本 &lt;code>document&lt;/code>，文本可以分割成很多单词，我们需要把单词表示成数值向量。其中每个单词都是一个类别，如果字典里有一万个单词，那么就有一万的类别，显然单词就是类别特征。我们需要使用处理类别特征的方法，把单词变成数值向量。&lt;/p>
&lt;p>文本处理主要分为三个步骤：&lt;/p>
&lt;ul>
&lt;li>🔔 把文本分割成单词&lt;/li>
&lt;li>🔔 计算每个单词出现的次数&lt;/li>
&lt;li>🔔 进行 one-hot 编码&lt;/li>
&lt;/ul>
&lt;p>文本处理的第一步是把文本分割成单词。一段话，一篇文章或者一本书可以表示为一个字符串，可以把文本分割成很多单词，这个步骤称为 &lt;code>Tokenization&lt;/code>。&lt;/p>
&lt;p>比如说这句话 &lt;code>... to be or not to be ...&lt;/code>， 可以分割成这些单词 &lt;code>[to, be, or, not, to, be]&lt;/code>。Tokenization 就是把文本变成单词的列表。&lt;/p>
&lt;p>文本处理的第二步是计算词频，也就是每个单词出现的次数。我们可以用一个哈希表 &lt;code>hash Map&lt;/code> 来计算，计算开始之前，哈希表是空的，我们根据以下方式更新哈希表：如果单词 &lt;code>w&lt;/code> 不在表里面，说明到目前为止，&lt;code>w&lt;/code> 还没有出现在文本里，所以我们要把 &lt;code>w&lt;/code> 加入哈希表，并让它的词频等于 1；如果 &lt;code>w&lt;/code> 在哈希表里面，说明 &lt;code>w&lt;/code> 之前在文本里出现过，只需要把 &lt;code>w&lt;/code> 的词频加 1 即可。&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/01/4.png"
loading="lazy"
alt="4"
>&lt;/p>
&lt;p>接下来举个例子，我们将挨个处理这个列表里的单词。当处理到单词 &lt;code>to&lt;/code> 的时候，首先查一下哈希表，发现哈希表里面有 &lt;code>to&lt;/code>，它的词频是 398，说明 to 在文章里已经出现过 398 次了，现在这个单词又出现了一次，于是把表里的词频加 1，变成了 399；当处理到单词 &lt;code>or&lt;/code>的时候，在表里找不到，这说明文章里还没有出现过 &lt;code>or&lt;/code> 这个单词，第一次出现在文章里，于是我们把 &lt;code>or&lt;/code> 插入表里，将词频设置为 1。&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/01/5.png"
loading="lazy"
alt="5"
>&lt;/p>
&lt;p>完成统计词频之后，需要把哈希表做一个排序，按照词频递减的顺序进行排列，表的最前面是词频最高的，表最后是词频最低的。然后就把词频换成下标 &lt;code>index&lt;/code>，从 1 开始数计数，词频最高的词的 &lt;code>index&lt;/code> 是 1。这个例子里，一共有 8 个单词，每个词对应一个 [1, 8] 之间的正整数。这个表称为字典 ，可以把单词映射为一个数字。&lt;/p>
&lt;p>字典里单词的个数称为词汇量 &lt;code>vocabulary&lt;/code>。这例子里词汇量等于 8。&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/01/6.png"
loading="lazy"
alt="6"
>&lt;/p>
&lt;p>英语里大概有 1 万个常用词，但是统计词频之后，你会发现字典会有几十万甚至上百万个单词。统计词频的目的就是保留常用词，去掉低频词。比如，我们可以保留词频最高的 1 万个单词，删掉其余单词。&lt;/p>
&lt;p>&lt;strong>为什么要删掉低频词呢？&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>🌰 低频词通常没有意义&lt;/li>
&lt;/ul>
&lt;p>很多低频词都是名字实体 &lt;code>name entities&lt;/code>，比如我们的名字就是个名字实体，假如我们的名字出现在一个数据集里面，他的频率肯定会很低，在大多数的应用里名字实体没有意义。&lt;/p>
&lt;p>低频词很多都是拼写错误造成的，如果把 prince 的 c 误写成 s，prinse，那么就创造了一个新的单词，这种词的频率也很低，在很多应用里，去掉这种词没有危害。&lt;/p>
&lt;ul>
&lt;li>🌰 去掉低频词的另一个原因是我们不希望 &lt;code>vocabulary&lt;/code> 太大。&lt;/li>
&lt;/ul>
&lt;p>下一个步骤做 &lt;code>one-hot encoding&lt;/code> 的时候，向量的维度就是字典的大小。字典越大，向量的维度就越高，这会让计算变慢。下一节详细说明词嵌入 &lt;code>Word Embedding&lt;/code> 的时候就会看到，字典越大，模型的参数就越会越多，就会容易造成过拟合 &lt;code>overfitting&lt;/code>，删掉低频词就会大幅减小 &lt;code>vocabulary&lt;/code>。&lt;/p>
&lt;p>文本处理的第三步就是对单词做 &lt;code>one-hot encoding&lt;/code>，通过查字典，把单词映射成一个正整数，一个单词的列表就映射成了一个正整数的列表；如果有必要就继续把这些正整数变成 &lt;code>one-hot&lt;/code> 向量。这些 &lt;code>one-hot&lt;/code> 向量的维度正好等于 &lt;code>vocabulary&lt;/code>，在这个例子里面，字典的长度是 8，所以 &lt;code>one-hot&lt;/code> 维度就等于 8。&lt;/p>
&lt;p>&lt;img src="https://nlp.letout.cn/img/nlp/01/7.png"
loading="lazy"
alt="7"
>&lt;/p>
&lt;p>上面说过，字典里的低频词可能会被删掉，所以有些词在字典里找不到，例如把 be 错误拼写成单词 bi，这个词在字典里找不到，&lt;code>one-hot encoding&lt;/code> 时，可以忽略这个词，也可以把它编码成全 0 向量。&lt;/p>
&lt;h2 id="-总结">🎐 总结&lt;/h2>
&lt;p>最后总结一下这一节的内容。&lt;/p>
&lt;p>部分机器学习的数据会具备类别特征 &lt;code>Categorical Features&lt;/code>，机器学习模型无法理解，我们需要将其转换成数值特征。类别特征的类别会被映射成一个从 &lt;code>1&lt;/code> 开始计算的整数，&lt;code>0&lt;/code> 被用来表示缺失或者未知的类别，并且使用 &lt;code>one-hot&lt;/code> 向量，能很好的表示类别特征的意义。&lt;/p>
&lt;p>文本处理主要有三个步骤，第一步 &lt;code>tokenization&lt;/code> 把文本分割成单词的列表；第二步建立了一个字典&lt;code>vocabulary&lt;/code>，把单词映射成一个正整数；第三步进行 &lt;code>one-hot encoding&lt;/code>，将分割后的单词列表映射成正整数的列表或变成 &lt;code>one-hot&lt;/code> 向量。&lt;/p></description></item></channel></rss>