[{"content":"本节主要内容为 文本处理 Text Processing 和 词嵌入 Word Embedding。本节和下面两节内容都会使用 IMDb 电影评论的数据，用来搭建机器学习模型分析电影评论。\n🌱 IMDb IMDb 是最有名的电影评论网站，用户可以在 IMDb 上给电影打分，1 分是非常讨厌，10 分是非常喜欢，如果乐意，还可以写一段电影评论。\n如果不给你看分数，只给你看评论，你大概能猜到用户打的分数，但你的猜测可能不太准确。如果换种方式，让你判断电影评论是 正面 positive 的还是 负面 negative 的，你应该会有很高的准确率。有人从 IMDb 上爬了 5 万条电影评论，这些评论都是很极端的，都是强烈的喜欢，或者强烈反感。这个二分类问题对于人来说很简单，人读一下电影评论就能轻易知道这是正面评价还是负面评价，人应该能有 100% 的准确率，这个数据集被分成两半，2 万 5 千条作为训练数据 Train，另外 2 万 5 千条作为测试数据 Test。\n你可以在下面的链接中获取到数据集：https://ai.stanford.edu/~amaas/data/sentiment/\n🔖 文本处理文本处理 在词嵌入 Word Embedding 和搭建机器学习模型之前，首先要进行文本处理，将文本变成序列 Sequence，文本处理很无聊，但我们应该重视它，文本处理的好坏，会直接影响机器学习模型的准确率。\n🚀 Tokenization 文本处理的第一步是 Tokenization，把文本分割成很多 tokens，这里我们把文本分割成很多单词，一个 token 就是一个单词（假如你把文本分割成字符，那么一个 token 就是一个字符），做完 Tokenization，一个很长的字符串就被分割成一个很多单词的列表。Tokenization 看起来很简单，但是讲究很多。比如：\n 🌰 是否应该把大写变成小写？  通常情况下应该把大写变成小写，大小写单词通常是一个意思；但有时候会混淆单词（Apple -\u0026gt; apple），比如 Apple 是苹果公司，apple 是水果，大小写的 apple 并不是相同的单词。\n 🌰 去除停用词。stop word  有些应用会去除 stop word，它是 the、a、of 等最高频的单词，这些词几乎出现在所有的句子里，对这个二分类问题几乎是没有帮助。\n 🌰 拼写纠错。  用户发电影评论的时候，大部分情况下并不会仔细检查，所以写的东西难免会有拼写错误，所以做拼写纠错通常是有用的。\n这里只是举了几个例子，实际上做 Tokenization 的时候需要做大量的工作，Tokenization 看似简单，但实际上并不容易。\n🧰 Build Dictionary 第二步是建立一个字典。可以先统计词频，去掉一些低频词，让后让每个单词对应一个正整数，比如让 the -\u0026gt; 1; cat -\u0026gt; 2; sat -\u0026gt; 3。有了这个字典，就可以把每个单词对应到一个整数，这样一来，一句话就可以用正整数的列表表示，这个列表被称为序列 Sequences。如果有必要的话，还得进一步做 one-hot encoding，把单词表示成 one-hot 向量。\n在电影评论的例子里，数据是 5 万条电影评论，每条电影评论可以表示成一个字符串。做完 Tokenization 和 Encoding 后，每条电影评论都会转换成一个 Sequences，也就是一个正整数的列表。电影评论有长有短，有人只能写几个字的评论，有人能洋洋洒洒写几千字，所以得到的这些 Sequences 的长度也各不相同。比如这两条 Sequences 的长度分别是 52 和 90。这就造成了一个问题，训练数据没有对齐，每条 Sequences 都有不同的长度。做机器学习的时候，我们需要把数据存储到矩阵或者张量里，每条序列都得有相同的长度，需要把序列对齐。解决方案是这样的：我们可以固定长度为 $w$。假如一个序列长度太长，超过了 $w$ 个词，就砍掉前面的词，只保留最后面 $w$个词（当然保留最前面 $w$ 个词也同样可以）；假如一个序列太短，不到 $w$ 个词，那么就做 zero padding 用 0 来补齐，把长度增加到 $w$。这样一来，所有序列的长度都是 $w$，可以存储到一个矩阵里。\n🍀️ 词嵌入 文本处理已经完成了，现在每个词都用一个正整数来表示，下一步是 Word Embedding，把每个词都表示为一个一维向量。\n现在每个单词都用一个数字来表示，该怎么把这些 Categorical 特征表示为数值向量呢？显然可以做 one-hot encoding，用一个 one-hot 向量来表示一个单词。比如 good: index = 2，于是使用标准正交积 $e_2$ 来表示，它的第二个元素是 1，其余元素都是 0，$e_2=[0, 1, 0, 0, \u0026hellip;, 0]$\n假如 vocabulary = v，也就是说字典里一共有 $v$ 个单词，那么就需要维度 dimension = v 的 one-hot 向量，要是字典里有 1 万个单词，那么这些 one-hot 向量都是 1 万维的，这样的向量维度是在太高了。下一节介绍 RNN 的时候你会看到，RNN 的参数数量正比于输入向量的维度，我们肯定不想让输入的向量是 1 万维的，否则一层 RNN 将会有好几十万个参数。所以我们要做 Word Embedding，把这些高维 one-hot 向量映射到低维向量。\n具体做法是吧 one-hot 向量 $e_i$ 乘到参数矩阵 $P^T$ 上，矩阵 $P^T$ 的大小是 $d*v$。其中 $d$ 是词向量的维度，由用户自己决定；$v$ 是 vocabulary，表示字典里单词的数量。\n矩阵的乘法的结果记做向量 $x_i$，$x_i$ 就是一个词向量，维度是 $d*1$，如果 one-hot 向量 $e$ 的第三个元素是 1，那么 $x_i$ 就是 $P^T$ 矩阵的第三列，可以看出，$P^T$ 矩阵每一列都是一个词向量。\n同理，下面这个参数矩阵 $P$ 的每一行都是一个词向量。这个矩阵的行数是 $v$，也就是 vocabulary；每一行对应一个单词，矩阵的列数是 $d$，$d$ 是用户决定的，$d$ 的大小会影响机器学习模型的表现，应该用 交叉验证 Cross Validation 用来选择一个比较好的 $d$。\n字典里的第一个词的是 movie，那么第一行就是 movie 的词向量；字典里的第二个词是 good，那么第二行就是 good 的词向量。\n我们的任务是判断电影评论是正面的还是负面的，这个参数矩阵是从训练数据中学习出来的，所以这些词向量都带有感情色彩，假如这些词向量都是二维的，我们就可以在平面坐标系中标出这些词向量。\nfantastic; good; fun 这些词向量都带有正面情感，所以这三个词的词向量学出来都比较接近；同理，poor; boring; mediocre 这些词带有负面情感，所以学出来的词同样也应该比较接近，但是这些词的词向量应该远离正面色彩的词向量。像 movie; is 这样的中性词，没有感情色彩，它们应该在中间。\n🎐 总结 最后总结一下这一章的内容。\n这一节上半部分，说明了文本处理是什么样的。给我们一条电影评论，首先做 Tokenization，把电影评论分割成很多单词，然后把很多单词编码成数字，这样一整条电影评论就可以很多正整数来表示，我们把这个正整数序列叫做 Sequences，就是神经网络中 Embedding 层的输入。由于电影评论的长短不一，得到的 Sequence 的长短也不一样，没办法存储在一个矩阵里，解决方案是 Alignment 对齐。假设最大长度为 20，如果长度大于20，就只保留最后 20 个单词；如果长度不到 20，就用 0 补齐，把长度增加到 20。这样一来，每个 Sequences 长度都相同。\n","date":"2022-04-30T21:14:45+08:00","image":"https://nlp.letout.cn/img/nlp/banner.png","permalink":"https://blog.letout.cn/p/nlp-in-action/text-processing-and-word-embedding/","title":"文本处理与词嵌入"},{"content":"🌱 类别特征 机器学习的数据通常有 类别特征 Categorical Features ，我们需要把类别特征 Categorical Features 转化成机器学习模型能理解的数值特征，下面使用一个例子来具体讲解类别特征数据的处理。\n这张表的每一行是一个人的数据，包括：年龄、性别、国籍，我们需要把这些数据变成机器学习模型可以理解的数值特征。\n表格的第一列是年龄，年龄本身就是数值特征，所以可以不用做处理，数值特征的特点是可以比较大小，比如 35 岁的人比 31 岁的年龄大。\n第二列是性别，性别是二元特征，我们可以用一个数来表示性别。用 0 表示女性，用 1 表示男性。这样一来，性别就表示为一个标量：0 / 1。\n第三列是国籍，比如中国，美国，印度。国籍是类别特征，机器学习并不理解国籍，所以我们要把国籍编码成数值向量。世界上约有 197 个国家，我们先用一个 [1 - 197] 的整数表示一个国家。可以建立一个字典，把国籍映射成一个 [1 - 197] 的整数。比如：China:1; US:2; India:3; Japan:4; Germany:5。\n我们要从 1 开始计算，而不能从 0 开始计算。\n做这种映射，国籍就表示成 [1 - 197] 之间的整数。仅仅把国籍表示成 [1 - 197] 的整数还是不行，一个整数只是一种类别，它们之间不能比较大小。US:2; India:3 这个数字并不表示印度大于美国，这些整数只是类别而已，并不是真正的数值特征。\n所以要进一步对国籍做 one-hot encoding ，用 one-hot 向量来表示国籍：\n1 2  China -\u0026gt; 1 -\u0026gt; [1,0,0,0,...,0] US -\u0026gt; 2 -\u0026gt; [0,1,0,0,...,0]   比如，中国对应 1，所以用 197 维的 one-hot 向量 [1,0,0,0...,0] 来表示，其中第一个元素为 1，其余元素都是 0；美国对应 2，这个 197 维的向量 [0,1,0,0...,0] 第二个元素是 1，其余元素都是 0。这样一来，每个国籍就由一个 one-hot 向量表示，一共有 197 个国家，所以每个向量都是 197 维的。\n我们要从 1 开始计算，而不能从 0 开始计算。因为我们要把 0 保留，用来表示未知或者缺失的国籍。数据库里面经常会有缺失的数据（比如用户没有填写国籍），这样缺失的国籍就用 0 来表示，它的 one-hot 向量就是一个全 0 的向量[0,0,0,0...,0]。\n下面这个例子中，我们用一个 199 维表示一个人的特征。比如这个人 28 岁，女性，国籍是中国。\n其中，一个维度表示年龄，一个维度表示性别，一个 197 维的 one-hot 向量表示国籍。\n这个例子里，这个 36 岁，男性，国籍未知的人的特征是这个 199 维的向量，我们用一个 197 维的全 0 向量表示未知国籍。\n🔖 为什么要用 one-hot 向量表示特征 在处理类别特征的时候，我们使用 one-hot 向量表示国籍，每个国籍都用 197 维的向量表示。为什么要用 one-hot 向量而不用一个数字表示呢？比如用 1 表示中国，2 表示美国，3 表示印度。这样一来，名字就变成了数字，可以做数值计算，而且用一个数字表示的话，可以节省 197 倍的存储空间。当然这是不行的。否则我们就不需要 one-hot encoding 了。\n假设我们使用 1 -\u0026gt; China; 2 -\u0026gt; US; 3 -\u0026gt; India。那么将中国 1 和美国 2 的特征加起来：1+2=3 ，相当于 “中国 + 美国 = 印度”。这样的特征完全不合理。\n使用 one-hot 特征向量更合理。将 China 和 US 的 one-hot 向量加起来，得到 [1,1,0,0,...,0]，第一个和第二个元素都是 1，其余元素都是 0，这个特征向量的解释是：既有中国国籍，又有美国国籍。\n所以做机器学习的时候，不能用一个标量来表示一个类别特征，这种特征做法求和等数值计算是没有意义的。正确的做法是使用 one-hot 向量来表示类别特征。\n🚀 处理文本数据的流程 在自然语言处理的应用中，数据就是文本 document，文本可以分割成很多单词，我们需要把单词表示成数值向量。其中每个单词都是一个类别，如果字典里有一万个单词，那么就有一万的类别，显然单词就是类别特征。我们需要使用处理类别特征的方法，把单词变成数值向量。\n文本处理主要分为三个步骤：\n 🔔 把文本分割成单词 🔔 计算每个单词出现的次数 🔔 进行 one-hot 编码  文本处理的第一步是把文本分割成单词。一段话，一篇文章或者一本书可以表示为一个字符串，可以把文本分割成很多单词，这个步骤称为 Tokenization。\n比如说这句话 ... to be or not to be ...， 可以分割成这些单词 [to, be, or, not, to, be]。Tokenization 就是把文本变成单词的列表。\n文本处理的第二步是计算词频，也就是每个单词出现的次数。我们可以用一个哈希表 hash Map 来计算，计算开始之前，哈希表是空的，我们根据以下方式更新哈希表：如果单词 w 不在表里面，说明到目前为止，w 还没有出现在文本里，所以我们要把 w 加入哈希表，并让它的词频等于 1；如果 w 在哈希表里面，说明 w 之前在文本里出现过，只需要把 w 的词频加 1 即可。\n接下来举个例子，我们将挨个处理这个列表里的单词。当处理到单词 to 的时候，首先查一下哈希表，发现哈希表里面有 to，它的词频是 398，说明 to 在文章里已经出现过 398 次了，现在这个单词又出现了一次，于是把表里的词频加 1，变成了 399；当处理到单词 or的时候，在表里找不到，这说明文章里还没有出现过 or 这个单词，第一次出现在文章里，于是我们把 or 插入表里，将词频设置为 1。\n完成统计词频之后，需要把哈希表做一个排序，按照词频递减的顺序进行排列，表的最前面是词频最高的，表最后是词频最低的。然后就把词频换成下标 index，从 1 开始数计数，词频最高的词的 index 是 1。这个例子里，一共有 8 个单词，每个词对应一个 [1, 8] 之间的正整数。这个表称为字典 ，可以把单词映射为一个数字。\n字典里单词的个数称为词汇量 vocabulary。这例子里词汇量等于 8。\n英语里大概有 1 万个常用词，但是统计词频之后，你会发现字典会有几十万甚至上百万个单词。统计词频的目的就是保留常用词，去掉低频词。比如，我们可以保留词频最高的 1 万个单词，删掉其余单词。\n为什么要删掉低频词呢？\n 🌰 低频词通常没有意义  很多低频词都是名字实体 name entities，比如我们的名字就是个名字实体，假如我们的名字出现在一个数据集里面，他的频率肯定会很低，在大多数的应用里名字实体没有意义。\n低频词很多都是拼写错误造成的，如果把 prince 的 c 误写成 s，prinse，那么就创造了一个新的单词，这种词的频率也很低，在很多应用里，去掉这种词没有危害。\n 🌰 去掉低频词的另一个原因是我们不希望 vocabulary 太大。  下一个步骤做 one-hot encoding 的时候，向量的维度就是字典的大小。字典越大，向量的维度就越高，这会让计算变慢。下一节详细说明词嵌入 Word Embedding 的时候就会看到，字典越大，模型的参数就越会越多，就会容易造成过拟合 overfitting，删掉低频词就会大幅减小 vocabulary。\n文本处理的第三步就是对单词做 one-hot encoding，通过查字典，把单词映射成一个正整数，一个单词的列表就映射成了一个正整数的列表；如果有必要就继续把这些正整数变成 one-hot 向量。这些 one-hot 向量的维度正好等于 vocabulary，在这个例子里面，字典的长度是 8，所以 one-hot 维度就等于 8。\n上面说过，字典里的低频词可能会被删掉，所以有些词在字典里找不到，例如把 be 错误拼写成单词 bi，这个词在字典里找不到，one-hot encoding 时，可以忽略这个词，也可以把它编码成全 0 向量。\n🎐 总结 最后总结一下这一节的内容。\n部分机器学习的数据会具备类别特征 Categorical Features，机器学习模型无法理解，我们需要将其转换成数值特征。类别特征的类别会被映射成一个从 1 开始计算的整数，0 被用来表示缺失或者未知的类别，并且使用 one-hot 向量，能很好的表示类别特征的意义。\n文本处理主要有三个步骤，第一步 tokenization 把文本分割成单词的列表；第二步建立了一个字典vocabulary，把单词映射成一个正整数；第三步进行 one-hot encoding，将分割后的单词列表映射成正整数的列表或变成 one-hot 向量。\n","date":"2022-04-30T20:36:14+08:00","image":"https://nlp.letout.cn/img/nlp/banner.png","permalink":"https://blog.letout.cn/p/nlp-in-action/data-processing/","title":"数据处理基础"}]