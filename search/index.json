[{"content":" 线程隔离\n程序计数器 虚拟机栈 本地方法栈 线程共享\n方法区 堆 🎋 程序计数器 当前线程执行字节码的行号指示器。\n唯一一个不会出现 OutOfMemoryError 的内存区域，随线程的创建而创建，随线程的结束而死亡。\n🎋 虚拟机栈 每一次方法的调用都会有一个对应的栈帧被压入栈中，每一个方法调用结束后，都会有一个栈帧被弹出。\n当线程请求栈的深度超过当前 Java 虚拟机最大深度时，会抛出 StackOverFlowError。\n局部变量表 存放编译期可知的各种数据类型，对象引用地址。\n操作数栈 方法执行过程中的计算结果，计算过程中产生的临时变量。\n动态链接 服务于一个方法需要调用其他方法的场景。\n方法返回地址 正常返回 异常返回 🎋 本地方法栈 本地方法栈是虚拟为执行 Native 方法服务。\n🎍 方法区 方法区是 Java 虚拟机的抽象概念。实现方式在不同虚拟机中可能不同。\nJDK 8 之前，HotSpot 虚拟机使用永久带实现方法区\nJDK 8 及之后，使用元空间（直接内存）实现\n类的元信息 类名，访问修饰符，字段描述，方法描述等\n运行时常量池 存放编译后的各种字面量，符号引用，常量（final）池表等\n字符串常量池 JVM 为了减少字符串对内存的消耗，并提升性能，专门开辟的区域\n静态变量 类变量（static）\n🎍 堆 用来存放和管理对象实例，几乎所有的对象实例都在这里分配（逃逸分析，栈上分配）。\nJava 堆是垃圾回收的主要区域，因此也被称为 GC 堆。\nJDK 8 之前，堆被分为\n新生代（Eden、Survivor1、Survivor2） 老年代 永久代 JDK 8 之后，永久代被取消\n大部分情况下，对象首先会在 Eden 区进行分配；在一次垃圾回收后，如果对象还存活，会进入 S0 或 S1；当对象年龄增长到一定程度（默认 15），或累积的年龄超过了 Survivor 的一半，会晋升到老年代。\n","date":"2022-08-08T15:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/27.jpeg","permalink":"https://emerywan.github.io/blog/p/java/jvm/memory-area/","title":"JVM 内存区域"},{"content":"🌴 对象存活判定 🪨 引用计数法 给每个对象添加一个引用计数器：\n当有地方引用该对象，计数器 +1 当引用失效，计数器 -1 当计数器为 0 时，表示对象不可用 该方法简单高效，但主流的虚拟机都没有选择该方法管理内存，难以解决对象之间循环引用的问题。\n🪨 可达性分析法 通过一系列称为 GC Root 的对象作为起点，从这些起点向下搜索，节点走过的路径称为引用链，当一个对象与 GC Root 之间没有任何引用链相连的话，说明对象不可用，需要被回收。\n可以作为 GC Root 的对象：\n虚拟机栈（栈帧中局部变量表）引用的对象 本地方法栈中引用的对象 类静态属性引用的对象 方法区中常量引用的对象 所有被同步锁（synchronized）持有的对象 🌴 垃圾回收算法 🪵 标记-清除 标记出所有需要回收的对象 标记结束后，统一回收掉所有被标记的对象 执行效率不稳定，大部分对象需要回收时，需要大量标记和清除操作 内存空间碎片化问题 🪵 标记-复制 将内存分为两块，每次只使用其中的一块 当一块内存使用完后，将存活的对象复制到另外一块去 将以使用的空间一次性清除掉 可用内存变少，有空间浪费问题 🪵 标记-整理 标记出所有需要回收的对象 让这些存活对象向内存的一端移动 直接清除掉边界以外的内存 🪵 分代收集 根据对象存活周期的不同，将 Java 堆分为新生代和老年代，依据各个区域的特点，选择回收算法。\n在新生代中，每次收集都会有大量的对象死去，选择 标记-复制 算法 在老年代中，存活对象的几率较高，没有额外的空间进行分配担保，选择 标记-清除 或 标记-整理 算法 🌴 垃圾收集方式 🪨 部分收集 收集堆部分区域。\n新生代收集 Minor GC / Young GC 目标只是新生代的垃圾收集。\n老年代收集 Major GC / Old GC 目标只是老年代的垃圾收集，目前只有 CMS 收集器会有单独收集老年代的行为。\n混合收集 目标是整个新生代和部分老年代的垃圾收集，目前只有 G1 收集器会有这种行为。\n🪨 整堆收集 收集整个堆和方法区。Full GC\n晋升到老年代的对象大于老年代的剩余空间 Minor GC 后，新生代存活对象超过了老年代的空间（分配担保） 手动 System.gc() (Java8之前) 永久代空间不足 🌴 垃圾收集器 🪵 Serial / Serial Old “单线程”的垃圾收集器，在进行垃圾收集时，必须暂停其他所有工作线程stop the world，直到收集介绍。\n新生代采用标记-复制算法，老年代采用标记-整理算法 简单高效 依旧是现在 HotSpot 虚拟机运行在客户端模式下的默认新生代垃圾回收方式 🪵 ParNew Serial 的多线程版本，可以使用多条线程进行垃圾回收（新生代）。\n采用标记复制算法 🪵 Parallel Scavenge / Parallel Old 基于 标记-复制 算法实现的新生代垃圾收集器，能够并行收集的多线程收集器，目标是达到一个可控制的“吞吐量” 基于 标记-整理 算法实现的老年代多线程收集器。 Java 8 默认的收集器为 Parallel Sacvenge + Parallel Old 🪵 CMS Concurrent Mark Swap 获取最短停顿时间的收集器。\n是 HotSpot 虚拟机第一款真正意义上的垃圾收集器，实现了用户线程和垃圾回收线程同时工作。基于 标记-清除 算法。（老年代）\nCMS 收集器整个过程分为四个步骤：\n🐾 初始标记 标记与 GC Root 直接关联的对象。（stop the world）暂停用户线程，但速度较快。\n🐾 并发标记 用户线程和垃圾回收线程同时工作，从 GC Root 直接关联的对象开始遍历，根据可达性分析，找出存活对象。\n🐾 重新标记 修正并发标记过程中，由于程序继续运行，导致的标记错误。暂停用户线程。\n🐾 并发清除 并发地清理未被标记的区域。\n缺点：\n对 CPU 资源敏感 无法处理浮动垃圾：运行过程中，有一些垃圾在当次无法处理，需要等待下次垃圾回收 会产生内存碎片，连续空间减少，导致 Full GC 🪵 G1 G1 开创了收集器面向局部收集的设计思路和基于 Region 的内存布局形式。\n将 Java 堆分为 2048 个大小相同的独立 Region，每个 Region 的大小依据堆空间的大小而定，在 JVM 生命周期中不会改变。\n每个 Region 都可以更新需要，作为新生代的 Eden 区，Survivor 区，或者是老年代；新生代和老年代在内存中不是连续的空间，收集器会根据区域的不同采取不同的策略。\nRegion 中还会有一类特殊的巨大区域 Humongous，专门用来存储大对象。当对象的大小超过了一个 Region 的一半，就被判定为大对象。\nG1 收集器回收的步骤：\n🐾 初始标记 标记 GC Root 能够直接关联到的对象，耗时较短。\n🐾 并发标记 从 GC Root 开始对堆中的对象进行可达性分析，找出需要回收的对象，与用户线程同时执行。\n🐾 最终标记 处理并发标记结束后，遗留的\n🐾 筛选回收 对各个 Region 的回收价值和成本进行排序，选择会后价值最大的区域进行回收。\n将回收部分的 Region 中的存活对象，复制到空的 Region 中，再清理掉整个旧的 Region 区域。\n涉及对象的移动，必须暂停用户线程，有多条垃圾收集器并发执行。\n🌴 对象分配和回收原则 🪨 对象优先在 Eden 区分配 大多数情况下，对象在新生代 Eden 区分配，当 Eden 区没有足够空间时，虚拟机会发起一次 Minor GC。\n🪨 大对象直接进入老年代 需要大量连续内存空间的 Java 对象（数组，很长的字符串），直接在老年代分配，避免在 Eden 区和两个 Survivor 区之间来回复制，产生大量的内存复制操作。\n🪨 长期存活的对象进入老年代 虚拟机给每个对象一个对象年龄计数器，每经过一次 MinorGC，年龄就会 +1，\n当年龄增加到一定程度（默认 15），或某个年龄超过了 Survivor 区的一半时，会晋升到老年代。\n🪨 空间分配担保 确保在 Minor GC 之前，老年代还有容纳新生代所有对象的剩余空间。\n进行一次 Minor GC 之后，Eden 区中任然存在大量对象，Survivor 中无法容纳，直接送到老年代，让老年代进行空间分配担保。\n","date":"2022-08-07T12:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/26.jpeg","permalink":"https://emerywan.github.io/blog/p/java/jvm/garbage-collection/","title":"JVM 垃圾回收"},{"content":"JMM Java 内存模型屏蔽了不同操作系统和各种硬件的内存访问差异，实现了 Java 程序在各种平台下，都能达到一致性的访问效果。\nJMM 规定了：\n所有的变量都存储在主存中 每条线程有自己的工作内存 线程对变量的所有操作（读取、赋值等），都必须在工作内存中进行，不能直接读写主存中的数据 原子性、可见性、有序性 Java 内存模型是围绕着在并发过程中如何处理原子性、可见性和有序性三个特征来建立的。\n原子性 一次或多次操作，要么全部执行且不受到任何干扰，要么全不执行。\n利用锁，任一时刻只有一个线程访问代码块。synchronized Lock 利用 CAS 原子类 保证原子操作 可见性 一个线程修改了共享变量的值，其他线程能够立即得知这个修改。\nvolatile 规定每次写操作，都立即同步到主存；每次读操作，都从主存中读取。\nsynchronized lock 对一个变量执行执行 unlock 之前，必须把变量同步回主存中。\nfinal 有序性 指令重排问题，代码的执行顺序不一定。\nvolatile 包含禁止指令重排序的语义。\nsynchronized 只有一条线程能够进入临界区。\nvolatile Java 虚拟机提供的轻量级内存同步机制。\n作用：\n保障了此变量对所有线程的可见性 volatile 的写操作，都会刷新到主内存中，并使其他线程中的 volatile 变量失效；volatile 的读操作，都会从主存中读取。\n禁止指令重排优化（有序性）\n❗️无法保障原子性\n一条字节码在执行时，需要运行多条指令才能实现。\n实现方式：内存屏障\n保证：之前的执行一定全部执行，之后的指令一定都没有执行，并且前面语句的结果对后面的语句可见。\nhappens-before 对于两个操作 A 和 B，这两个操作可以在不同的线程中执行。如果 A happens-before B（A 优先于 B 执行），那么可以保证，当 A 操作完后，A 的操作对于 B 操作是可见的。\n指令重排提高了并发性能，但是 Java 虚拟机会对指令重排做一些规则限制，并不能让所有的指令都随意改变执行位置。\nJava 内存模型天然的先行发生关系：\n程序顺序规则 在一个线程内，前面的代码操作优于后面的代码操作\n锁定规则 一个 unlock 操作优于后面对于同一个锁的 lock 操作\nvolatile 规则 一个 volatile 变量的写操作，优于后面这个变量的读操作\n","date":"2022-08-07T06:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/28.jpeg","permalink":"https://emerywan.github.io/blog/p/java/jvm/memory-model/","title":"JVM 内存模型"},{"content":"类加载器 BootstrapClassLoader 启动类加载器\n最顶层的加载类，由 C++ 实现，负责加载 %JAVA_HOME%/lib 目录下的 jar 包和类或者被 -Xbootclasspath 参数指定的路径中的所有类。\nExtensionClassLoader 扩展类加载器。Java 系统类库的扩展机制。\n负责加载 %JRE_HOME%/lib/ext 目录下的 jar 包和类，或被 java.ext.dirs 系统变量所指定的路径下的 jar 包。\nApplicationClassLoader 面向用户的加载器，负责加载当前应用 classpath 下的所有 jar 包和类。\n双亲委派模型 每个类加载器都有自己的命名空间，用不同的类加载器加载了同一个限定名的类，JVM 也会认为是两个不同的类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 protected Class\u0026lt;?\u0026gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // (1) 判断是否加载过该类 Class\u0026lt;?\u0026gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) { c = parent.loadClass(name, false); } else { // (2) parent == null 约定为：parnet 为 Bootstracp ClassLoader c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if class not found // from the non-null parent class loader } if (c == null) { // (3) 说明 parent 加载不了，当前 loader 尝试加载 class long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; } } 默认情况下，一个限定名的类只会被一个类加载器加载并解析使用，在程序中是唯一的，不会产生歧义。\n在加载时，首先会判断当前类是否被加载过，如果已经加载，会直接返回，否则才会尝试加载。\n尝试加载时，不会自己进行加载，而是将请求委派给父类加载器，当父类加载器无法加载时，才会由自己处理。\n","date":"2022-08-06T16:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/25.jpeg","permalink":"https://emerywan.github.io/blog/p/java/jvm/classloader/","title":"类加载器"},{"content":"Java 类生命周期 类加载过程 类加载主要有三步：加载 - 链接 - 初始化。\n🥎 加载 加载是一个读取 Class 文件，将其转化为某种静态数据结构存储在方法区内，并在堆中生成一个便于调用的 java.lang.Class 类型的对象的过程。\n分为三步进行：\n获取定义类的二进制字节流（不限制从哪里获取，文件、网络、即时生成）\n将字节流代表的静态存储结构，转换成方法区中运行时数据结构\n在堆中生成一个代表该类的 java.lang.Class 对象，作为方法区这个类的各种数据访问入口\n🥎 链接 ⚾️ 验证 文件格式验证 验证字节流是否符合 Class 文件格式的规范，能被当前版本的虚拟机处理。\n元数据、字节码验证 对字节码进行语法、语义的分析，保证其符合 Java 虚拟机的规范，不会有危害虚拟机的行为。\n符号引用验证 确保解析行为能够正常执行。发生在解析阶段。\n⚾️ 准备 为类中定义的变量（静态变量 static）分配内存并设置类变量初始值的阶段。\nstatic -\u0026gt; 赋零值 static final -\u0026gt; 赋定义的常量值 ⚾️ 解析 解析是将常量池中的符号引用替换为直接引用。\n符号引用：描述引用对象的符号\n直接引用：指向目标实际地址的指针\n🎾 解析部分是灵活的，可以在初始化环节后再进行，实现所谓的“后期绑定”。（方法调用直到运行时才会解析，因为无法在编译时确定方法调用所需的所有信息，所以方法定义和方法调用直到运行时才绑定。）\n当一个类被编译成 Class 之后，假设这个类称为 A，并且在 A 中引用了类 B。\n在编译阶段，A 无法确定 B 是否被编译（现在 B 一定未加载），此时 A 无法知道 B 的实际地址，所有在 A.Class 中，会使用一个字符串代表 B。这个字符串被称为符号引用。\n在运行阶段，A 发生了加载，在解析时，发现其中的 B 还未被加载，就会触发类 B 的加载，将 B 加载到虚拟机中。此时，A 中的符号引用会被替换为 B 中的实际地址（直接引用）。\n静态解析 类 B 是具体的实现类，解析的对象十分明确，即会进行静态解析。\n动态解析 Java 通过后期绑定的方式实现多态，通过动态解析实现。\n如果类 B 是抽象类或者接口，有具体的实现类 C、D，当前具体的实现方式并不明确，无法确定使用哪个具体实现。\n直到运行过程中发生了调用，虚拟机调用栈中会得到具体类的信息，再进行解析，就有明确的直接引用代替符号引用。\n🥎 初始化 虚拟机真正开始执行类中编写的代码，完成一些主动的资源初始化动作。\n执行的是类层面的初始化。 只有显式调用 new 才会执行构造函数的初始化。\n成员变量的赋值 静态变量的赋值 静态代码块 参考 https://javaguide.cn/java/jvm/class-loading-process.html\n深入理解 Java 虚拟机\nhttps://www.bilibili.com/video/BV14U4y1L75q/?spm_id_from=333.788\n","date":"2022-08-06T13:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/24.jpeg","permalink":"https://emerywan.github.io/blog/p/java/jvm/class-loading/","title":"类加载机制"},{"content":"事务 事务是一组逻辑上的数据库操作，这些操作要么全部成功，要么全部失败。\n特性 原子性：事务是不可分割的最小操作单元，要么全部成功，要么全部失败。\n一致性：事务完成时，必须使所有的数据都保持一致状态。\n隔离性：数据库系统提供的隔离机制，保证事务在不受外部并发操作影响的独立环境下运行。\n持久性：事务一旦提交或回滚，它对数据库中的数据的改变就是永久的。\nredo log 重做日志，记录的是“某个数据页上做了什么修改”。属于物理日志。\nredo log 用来保障数据库的持久性。\n在一个事务中，执行多个增删改的操作时，InnoDB 引擎会先操作缓冲池中的数据，如果缓冲区没有对应的数据，会通过后台线程将磁盘中的数据加载出来，存放在缓冲区中，再将缓冲池中的数据修改，修改后的数据页我们称为脏页。\n脏页则会在一定的时机，通过后台线程刷新到磁盘中，从而保证缓冲区与磁盘的数据一致。\n缓冲区的脏页数据并不是实时刷新的，而是一段时间之后将缓冲区的数据刷新到磁盘中（随机磁盘 IO）。\n当对缓冲区的数据进行增删改之后，会首先将操作的数据页的变化，记录在 redo log buffer 中。在事务提交时，会将 redo log buffer 中的数据刷新到 redo log 磁盘文件中（顺序磁盘 IO）。\n如果刷新缓冲区的脏页到磁盘时，发生错误，此时就可以借助于 redo log 进行数据恢复，这样就保证了事务的持久性。\nundo log 回滚日志。记录操作数据库的逆操作，用于进行回滚。\n作用：\n提供回滚（保障事务的原子性） MVCC undo log 是逻辑日志，可以认为当 delete 一条记录时，undo log 中会记录一条对应的 insert 记录；当 update 一条记录时，它记录一条对应相反的 update 记录。\nMVCC 多版本并发控制，同一条记录在数据库中，可以存在多个版本。\n隐式字段 DB_TRX_ID 事务 id DB_ROLL_PTR 回滚指针（记录这条记录的上一个版本） undo log readview 当前读 读取的是记录的最新版本。\nupdate insert delete select ... for update select ... lock in share mode 都是进行当前读。\n快照读 简单的不加锁 select 就是快照读。读取的是快照版本，通过 MVCC 进行并发控制。\n一致性 数据库通过原子性、隔离性、持久性来保证一致性。\nC(一致性)是目的，A(原子性)、I(隔离性)、D(持久性)是是为了保证一致性，数据库提供的手段。\n","date":"2022-07-31T13:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/23.jpeg","permalink":"https://emerywan.github.io/blog/p/mysql/innodb/","title":"MySQL 存储引擎"},{"content":"MySQL 体系结构 连接层 主要完成一些类似于连接处理、授权认证、及相关的安全方案。\n为通过认证安全接入的客户端提供线程，服务器也会为安全接入的每个客户端验证它所具有的操作权限。\n服务层 主要完成大多数的核心服务功能，如SQL接口，并完成缓存的查询，SQL的分析和优化，部分内置函数的执行。\n所有跨存储引擎的功能也在这一层实现，如 过程、函数等。在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定表的查询的顺序，是否利用索引等，最后生成相应的执行操作。\n如果是select语句，服务器还会查询内部的缓存，如果缓存空间足够大，这样在解决大量读操作的环境中能够很好的提升系统的性能。\n引擎层 真正的负责了MySQL中数据的存储和提取，服务器通过API和存储引擎进行通信。不同的存储引擎具有不同的功能，这样我们可以根据自己的需要，来选取合适的存储引擎。数据库中的索引是在存储引擎层实现的。\n存储层 数据存储层， 主要是将数据（如: redolog、undolog、数据、索引、二进制日志、错误日志、查询日志、慢查询日志等）存储在文件系统之上，并完成与存储引擎的交互。\n存储引擎 存储引擎就是存储数据，建立索引，更新、查询数据等技术的具体实现方式。\n存储引擎是基于表的，而不是基于库的，所以存储引擎可以被称为表类型，可在创建表的时候，指定选择的存储引擎。\n当前数据库支持的存储引擎 1 show engines; InnoDB InnoDB 是 MySQL 5.5 之后默认的 MySQL 存储引擎。\n特点：\nDML 操作遵循 ACID 模型，支持事务 支持行级锁，提高并发访问性能 支持外键约束，保证数据的完整性和正确性 InnoDB 的逻辑存储结构：\n表空间 InnoDB 存储引擎逻辑结构的最高层，idb 文件就是表空间文件。在表空间中可以包含多个 Segment 段。\n段 表空间是由各个段组成的，常见的段有数据段、索引段、回滚段等。InnoDB 对于段的管理，都是引擎自身完成，不需要人为控制，一个段中包含多个区。\n区（1M） 区是表空间的单元结构，每个区的大小为 1M。默认情况下，InnoDB 存储引擎页的大小为 16K，即一个区中一共有 64 个连续的页。\n页（16K） 页是组成区的最小单元，也是 InnoDB 存储引擎磁盘管理的最小单元，每个页默认为 16K。为了保证页的连续性，InnoDB 存储引擎每次从磁盘申请 4-5 个区。\n行 InnoDB 存储引擎是面向行的，也就是说数据是按行进行存放的，在每一行中，除了定义表时指定的字段外，还包括隐藏字段。\nMyISAM MyISAM 是 MySQL 早期的默认存储引擎。\n特点：\n不支持事务，不支持外键 支持表锁，不支持行锁 适合 select 存储文件：\n.sdi 存储表结构信息 .MYD 存储数据 .MYI 存储索引 Memory Memory 引擎的表数据是存放在内存中的，由于受到硬件、断电等问题的影响，只能将这些表作为临时表或缓存使用。\n特点：\n内存存放数据，宕机后数据会丢失 hash 索引（默认） 文件：\n.sdi 存储表结构信息 ","date":"2022-07-29T19:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/22.jpeg","permalink":"https://emerywan.github.io/blog/p/mysql/engine/","title":"MySQL 存储引擎"},{"content":"insert 优化 一次性往数据库中插入多条数据，可从以下三方面优化：\n批量插入数据 1 insert into tb_test values (1, \u0026#39;Tom\u0026#39;), (2, \u0026#39;Cat\u0026#39;), (3, \u0026#39;Jerry\u0026#39;); 手动控制事务 1 2 3 4 5 start transaction; insert into tb_test values (1, \u0026#39;Tom\u0026#39;), (2, \u0026#39;Cat\u0026#39;), (3, \u0026#39;Jerry\u0026#39;); insert into tb_test values (4,\u0026#39;Tom\u0026#39;), (5,\u0026#39;Cat\u0026#39;), (6,\u0026#39;Jerry\u0026#39;); insert into tb_test values (7,\u0026#39;Tom\u0026#39;), (8,\u0026#39;Cat\u0026#39;), (9,\u0026#39;Jerry\u0026#39;); commit; 主键顺序插入 1 2 主键乱序插入 : 8 1 9 21 88 2 4 15 89 5 7 3 主键顺序插入 : 1 2 3 4 5 7 8 9 15 21 88 89 主键优化 满足业务需求的情况下，尽量降低主键的长度，减少磁盘 IO 插入数据尽量选择顺序插入，选择使用 auto_increment 自增主键 尽量不要使用 UUID 或其他自然主键（如身份证号） 作为主键 业务操作避免对主键的修改 在 InnoDB 存储引擎中，表数据是根据主键顺序组织存放的。这种存储方式为索引组织表（index organized table IOT）。\n在 InnoDB 引擎中，数据行是记录在逻辑结构 page 页中的，而每一个页的大小是固定的，默认 16K。 那也就意味着，一个页中所存储的行也是有限的，如果插入的数据行在该页存储不下，将会存储到下一个页中，页与页之间会通过指针连接。\n页分裂 页可以为空，也可以填充一半，也可以填充100%。每个页包含了 2-N 行数据(如果一行数据过大，会行 溢出)，根据主键排列。\n乱序插入：\n页合并 order by 优化 MySQL的排序，有两种方式：\nUsing filesort : 通过表的索引或全表扫描，读取满足条件的数据行，然后在排序缓冲区 sort buffer 中完成排序操作，所有不是通过索引直接返回排序结果的排序都叫 FileSort 排序\nUsing index : 通过有序索引顺序扫描直接返回有序数据，这种情况即为 using index，不需要 额外排序，操作效率高\n优化原则：\n根据排序字段建立合适的索引，多字段排序时，也遵循最左前缀法则\n尽量使用覆盖索引，否则不生效(filesort)\n多字段排序, 一个升序一个降序，此时需要注意联合索引在创建时的规则（ASC/DESC）\n如果不可避免的出现 filesort，大数据量排序时，可以适当增大排序缓冲区大小 sort_buffer_size(默认 256k)\nlimit 优化 在数据量比较大时，如果进行 limit 分页查询，在查询时，越往后，分页查询效率越低。\n当在进行分页查询时，如果执行 limit 2000000,10 ，此时需要MySQL排序前2000010 记 录，仅仅返回 2000000 - 2000010 的记录，其他记录丢弃，查询排序的代价非常大 。\n优化思路: 一般分页查询时，通过创建覆盖索引能够比较好地提高性能，可以通过覆盖索引加子查 询形式进行优化。\n1 2 3 select * from tb_sku t, (select id from tb_sku order by id limit 2000000, 10) a where t.id=a.id; count 优化 count(主键)\nInnoDB 引擎会遍历整张表，把每一行的 主键id 值都取出来，返回给服务层。服务层拿到主键后，直接按行进行累加(主键不可能为null) count(字段)\n没有 not null 约束: InnoDB 引擎会遍历整张表把每一行的字段值都取出来，返回给服务层，服务层判断是否为null，不为null，计数累加。 有 not null 约束：InnoDB 引擎会遍历整张表把每一行的字段值都取出来，返回给服务层，直接按行进行累加。 count(数字)\nInnoDB 引擎遍历整张表，但不取值。服务层对于返回的每一行，放一个数字“1”进去，直接按行进行累加。 count(*)\nInnoDB引擎并不会把全部字段取出来，而是专门做了优化，不取值，服务层直接按行进行累加。 update 优化 InnoDB的行锁是针对索引加的锁，不是针对记录加的锁，并且该索引不能失效，否则会从行锁 升级为表锁。\n开启多个事务，在执行 SQL 时，防止行锁升级为了表锁。\n","date":"2022-07-01T18:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/20.jpeg","permalink":"https://emerywan.github.io/blog/p/mysql/sql-optimize/","title":"MySQL 优化"},{"content":"日志 错误日志 错误日志是 MySQL 中最重要的日志之一，它记录了当 mysqld 启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。\n当数据库出现任何故障导致无法正常使用时，建议首先查看此日志。\n查看日志位置：\n1 show variables like \u0026#39;%log_error%\u0026#39;; 二进制日志（归档日志） 二进制日志（BINLOG）记录了所有的 DDL（数据定义语言）语句和 DML（数据操纵语言）语句。\n作用：\n灾难时数据恢复 MySQL 主从复制 1 show variables like \u0026#39;%log_bin%\u0026#39;; log_bin_basename\n当前数据库服务器的binlog日志的基础名称(前缀)，具体的binlog文件名需要再该basename的基础上加上编号(编号从000001开始)。 log_bin_index\nbinlog的索引文件，里面记录了当前服务器关联的binlog文件有哪些。 binlog 格式 MySQL 服务器中提供了多种格式来记录二进制日志：\n配置二进制日志的格式，只需要在 /etc/my.cnf 中配置 binlog_format 参数即可。\nstatement\n基于 SQL 语句的日志记录，记录的是 SQL 语句，对数据进行修改的 SQL 都会记录在日志文件中。 row （默认）\n基于行的日志记录，记录的是每一行的数据变更。 mixed\n混合了 statememt 和 row 两种格式，默认采用statement，在某些特殊情况下会自动切换为 row 进行记录。 1 show variables like \u0026#39;%binlog_format%\u0026#39;; 查看 binlog 日志是以二进制方式存储的，不能直接读取，需要通过二进制日志查询工具 mysqlbinlog 来查看。\n1 2 3 4 5 6 7 mysqlbinlog [参数] logfilename 参数选项： -d 指定数据库名称，只列出指定的数据库相关操作 -o 忽略掉日志中的前 n 行命令 -v 将行事件(数据变更)重构为SQL语句 -vv 将行事件(数据变更)重构为SQL语句，并输出注释信息 清理 binlog reset master\n删除全部 binlog 日志，删除之后，日志编号，将从 binlog.000001重新开始 purge master logs to 'binlog.000005'\n删除 000005 之前的所有日志 purge master logs before 'yyyy-mm-dd hh24:mi:ss'\n删除日志为 \u0026ldquo;yyyy-mm-dd hh24:mi:ss\u0026rdquo; 之前产生的所有日志 可以在 mysql 的配置文件中配置二进制日志的过期时间，设置了之后，二进制日志过期会自动删除。\n1 show variables like \u0026#39;%binlog_expire_logs_seconds%\u0026#39;; 查询日志 查询日志中记录了客户端的所有操作语句。（binlog 不包含查询数据的 SQL 语句）\n默认情况下，查询日志是未开启的。\n如果需要开启查询日志，可以修改MySQL的配置文件 /etc/my.cnf 文件，添加如下内容：\n1 2 3 4 5 6 7 # 该选项用来开启查询日志 # 可选值 0 关闭 1 开启 general_log=1 # 设置日志的文件名 # 如果没有指定，默认的文件名为 host_name.log general_log_file=mysql_query.log 开启了查询日志之后，/var/lib/mysql/ 目录下就会出现 mysql_query.log 文件。\n所有的客户端的增删改查操作都会记录在该日志文件之中，长时间运行后，该日志文件将会非常大。\n慢查询日志 慢查询日志记录了：\n执行时间超过 long_query_time 扫描记录数大于 min_examined_row_limit 的所有的SQL语句的日志，默认未开启。 如果需要开启慢查询日志，需要在 MySQL 的配置文件 /etc/my.cnf 中配置如下参数：\n1 2 3 4 5 # 慢查询日志 slow_query_log=1 # 执行时间参数 long_query_time=2 默认情况下，不会记录管理语句，也不会记录不使用索引进行查找的查询。\n1 2 3 4 5 # 记录执行较慢的管理语句 log_slow_admin_statements=1 # 记录执行较慢的未使用索引的语句 log_queries_not_using_indexes= 1 ","date":"2022-06-30T18:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/19.jpeg","permalink":"https://emerywan.github.io/blog/p/mysql/log/","title":"MySQL 日志"},{"content":"索引 索引（index）是帮助 MySQL 高效获取数据的数据结构，提高查询效率。\n优点\n提高数据的查询速度 降低数据库的 IO 成本 缺点\n占用磁盘空间 增删改需要维护索引，性能可能会下降 索引的分类 1 2 -- 查看表中的索引 show index from table_name; 主键索引 当一张表，把某个列设为主键的时候，则该列就是主键索引。\n1 2 3 create table demo ( id int primary key auto_increment ); 普通索引 用表中的普通列构建的索引，没有任何限制。\n1 2 3 4 5 6 7 8 9 create table demo ( id int primary key auto_increment, name varchar(128), index demo_index_name(name) ); --- create index demo_index_name on demo_table(name); 唯一索引 索引列的值必须唯一，允许有多个空值 null（和主键不同，主键不允许为空）。\n1 2 3 4 5 6 7 8 9 create table demo ( id int primary key auto_increment, name varchar(128), unique index new_unique_index_name(name) ); --- create unique index new_unique_index_name on demo_table(name); 全文索引 1 2 3 4 5 6 create table demo ( id int primary key auto_increment, name varchar(128), content text, fulltext index demo_fulltext_index_content(content) ); 组合索引 组合索引 / 联合索引\n1 create index demo_name_age_index on demo(name, age); 聚簇索引 索引和数据放在一起，索引结构的叶子节点保存了行数据，找到了索引就找到了数据。\n访问数据更快，聚簇索引将索引和数据保存在一个B+树上。\n插入速度严重依赖于插入顺序。\n更新聚簇索引列的代价很高。\n非聚簇索引（二级索引） 将数据与索引分开存储，索引结构的叶子节点关联的是对应的主键。\n二级索引访问需要两次索引查找，要回表。 索引的结构 B Tree B 树是一种多叉平衡查找树，其主要特点：\n树的节点中存储着多个元素，每个内节点有多个分叉。\n在所有的节点都储存数据。每个节点中包含键值和数据，节点中的键值从小到大排列。\n父节点当中的元素不会出现在子节点中。\n所有的叶子结点都位于同一层，叶节点具有相同的深度，叶节点之间没有指针连接。\nB+ Tree B+ 树是 B 树的升级版。B+ 树在结构上和B树的区别在于：\n只有叶子节点才会存储数据，非叶子节点只存储键值，用于查找。\n叶子节点之间使用双向指针连接，最底层的叶子节点形成了一个双向有序链表。\n叶子节点保存了父节点所有关键字记录的指针。\n相比与 B 树，B+ 树有以下提升：\n扫表能力更强（进行全表扫描，只需要遍历叶子节点即可）\n具有更好的磁盘读写能力（非叶子节点不存储数据，使树更矮，IO 操作更少，一次磁盘加载扫描范围更大）\n查询效率稳定（数据都存储在叶子节点，IO 次数稳定）\n增删数据效率更高（数据在叶子节点以有序链表存储，可提高数据增删效率）\n数据库中，B+ 树的高度一般都在 2~4 层。由于 MySQL 的 INNODB 引擎在设计时是将 B+ 树的根节点常驻内存的，因此在查找某一键值的行记录时，最多只需要 1~3 次磁盘IO。\nHash 支持 hash 索引的是 Memory 存储引擎。采用一定的 hash 算法，将键值换算成新的 hash 值，映射到对应的槽位上，然后存储在 hash 表中。\nhash\n特点：\n只能用于对等比较（=, in），不支持范围查询（between, \u0026gt;, \u0026lt;） 无法利用索引完成排序操作 查询效率高，通常只需要一次检索（无 hash 冲突的情况） 创建索引的原则 🙆 建议创建索引 ✅ select 语句，频繁作为 where 条件的字段 1 2 3 4 5 6 7 8 select * from employees where first_name=\u0026#34;Georgi\u0026#34;; -- index(first_name) select * from employees where first_name=\u0026#34;Georgi\u0026#34; and last_name=\u0026#34;Cools\u0026#34;; -- 联合索引 -- index(first_name, last_name) -- 注意满足：最左前缀原则 -- index(last_name, first_name) -\u0026gt; where first_name=\u0026#34;Georgi\u0026#34; 无法使用索引 ✅ update/delete 语句的 where 条件 1 2 3 update employees set first_name=\u0026#34;Jim\u0026#34; where emp_no=\u0026#34;100001\u0026#34;; delete from employees where fitst_name=\u0026#34;Georgi\u0026#34;; ✅ 需要分组，排序的字段 1 select dept_no, count(*) from dept_emp group by dept_no; ✅ 使用 distinct 的字段 1 select distinct(first_name) from employees; ✅ 字段需要唯一性约束\n唯一索引 主键索引 ✅ 多表查询，连接字段应创建索引\n类型务必保持一致，避免隐式装换 1 2 3 4 5 6 7 select emp.*, d.dept_name from employees emp left join dept_emp de on emp.emp_no=de.emp_no left join departments d on de.dept_no=d.dept_no where de.emp_no=\u0026#34;100001\u0026#34;; -- employees.emp_no dept_emp.emp_no 类型务必保持一致 -- dept_emp.dept_no departmants.dept_no 类型务必保持一致 🙅 不建议创建索引 ❌ where 子句中用不到的字段 索引的作用是快速定位到想要的数据，用不到就没有必要创建数据了。\n❌ 表里的记录非常少 如只有 100 条数据，有无影响不大。\n❌ 有大量重复数据，选择性低 如：性别字段，导致大量回表。\n索引的选择性越高，查询效率越好，可以在查找过程中过滤更多的行\n❌ 频繁更新的字段，创建索引要考虑维护索引的开销 频繁修改但查询少的字段，不推荐创建索引\n索引失效与解决方案 ⚠️ 索引列不独立，进行了计算或变成了参数 索引字段进行了表达式计算 1 2 3 explain select * form employees where emp_no + 1 = 100003 应事先计算好表达式的值，再传入，避免在 SQL where 的左侧做计算。\n索引字段是函数的参数 1 2 3 4 explain select * from employees where substring(fitst_name, 1, 2) = \u0026#34;Geo\u0026#34;; 事先计算好再传入。\n或者使用一些等价的 SQL。\n1 2 3 4 5 6 explain select * from employees where first_name list \u0026#34;Geo%\u0026#34;; -- first_name 需要是索引的情况下有效（根据实际场景） -- 注意 like 索引失效的情况 ⚠️ 使用了左模糊 1 2 3 4 5 6 7 explain select * from employees where first_name like \u0026#34;%Geo%\u0026#34;; -- 无法使用索引 explain select * from employee where first_name like \u0026#34;Geo%\u0026#34;; -- 可以使用索引 如果无法避免，且有较高需求，可以考虑使用搜索引擎。\n⚠️ or 查询的部分字段没有索引 1 2 3 4 5 explain select * from employees where first_name=\u0026#34;Georgi\u0026#34; or last_name=\u0026#34;Georgi\u0026#34;; -- 假设当前 first_name 有索引，last_name 没有索引，该条 sql 无法使用索引 -- index(fitst_name) 解决：根据实际业务，添加相应的索引。\n添加索引后，索引合并，避免了全表扫描。\n⚠️ 字符串条件没有使用 \u0026quot;\u0026quot; 1 2 3 explain select * from dept_emp where dept_no=3; -- 这里 dept_no 在数据库定义为 varchar ⚠️ 不符合最左前缀原则的查询 1 2 3 4 explain select * from employees where first_name=\u0026#34;Facello\u0026#34;; -- index(last_name, first_name) 1 2 3 4 5 explain select * from employees where first_name=\u0026#34;Facello\u0026#34;; -- 更换索引顺序 -- index(first_name, last_name) 1 2 3 4 select * from employees where last_name=\u0026#34;\u0026#34; and first_name=\u0026#34;\u0026#34; -- index(first_name, last_name) -- MySQL 的引擎为更好利用复合索引，会动态调整字段顺序 ⚠️ null 条件查询 单列索引无法存储 null 值，复合索引无法存储全为 null 的值\n查询时，采用 is null is not null 条件，根据表中的数据分布，MySQL 判断是否走索引，并不固定\nMySQL 官方建议尽量把字段定义为 NOT NULL\n1 2 3 explain select * from users where mobile is null; ⚠️ 隐式转换导致索引失效 1 2 3 4 5 6 7 select emp.*, d.dept_name from employees emp left join dept_emp de on emp.emp_no=de.emp_no left join departments d on de.dept_no=d.dept_no where de.emp_no=\u0026#34;100001\u0026#34;; -- employees.emp_no dept_emp.emp_no 类型不一致时会导致隐式转换 -- dept_emp de departments.dept_no 类型不一致时会导致隐式转换 ⚠️ MySQL 评估 MySQL 在查询时，会评估使用索引的效率与走全表扫描的效率，如果走全表扫描更快，则放弃 索引，走全表扫描。\n因为索引是用来索引少量数据的，如果通过索引查询返回大批量的数据，则还不 如走全表扫描来的快，此时索引就会失效。\n最左前缀法则 最左前缀法则指的是查询从索引的最左列开始，并且不跳过索引中的列。\n如果跳跃某一列，索引将会部分失效(后面的字段索引失效)。\n中间不能跳过某一列，否则该列后面的字段索引将失效。\n范围查询 联合索引中，出现范围查询(\u0026gt;, \u0026lt;)，范围查询右侧的列索引失效。\n覆盖索引 对于索引 X，SELECT 的字段只需要从索引中就能获得，而无需回表获取。\n1 2 3 4 -- index(name, age, pos) select pos from staffs where name=\u0026#34;July\u0026#34; and age=14; 前缀索引 当字段类型为字符串（varchar，text，longtext等）时，有时候需要索引很长的字符串，这会让 索引变得很大，查询时，浪费大量的磁盘 IO， 影响查询效率。\n可以只将字符串的一部分前缀，建立索引，这样可以大大节约索引空间，从而提高索引效率。\n1 2 -- 为 email 字段，建立索引长度为 5 的前缀索引 create index index_email_5 on tb_user(email(5)); explain 查看 SQL 语句的执行计划。字段含义：\nid\n查询中执行 select 子句或者是操作表的顺序 id 相同，执行顺序从上到下；id 不同，值越大，越先执行 select_type\n表示 select 的类型 SIMPLE (简单 select,不使用 union 或子查询等) PRIMARY (查询中若包含任何复杂的子部分,最外层的 select 被标记为 PRIMARY) UNION (union 中的第二个或后面的 select 语句) SUBQUERY (子查询中的第一个 select) table\n这一行的数据是关于哪张表 type\nMySQL在表中找到所需行的方式 性能由好到差的连接类型为：NULL、system、const、eq_ref、ref、range、index、all possible_keys\n可能应用在这张表上的索引，一个或多个 key -实际使用的索引，如果为 NULL，则没有使用索引\nkey_len\n表示索引中使用的字节数，该值为索引字段最大可能长度，并非实际使用长度，在不损失精确性的前提下，长度越短越好 ref\n上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows\nMySQL 认为必须要执行查询的行数，在 innodb 引擎的表中，是一个估计值，可能并不总是准确的 Extra\nMySQL解决查询的详细信息 参考 https://juejin.cn/post/6936032090546765837 ","date":"2022-06-30T14:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/15.jpeg","permalink":"https://emerywan.github.io/blog/p/mysql/index/","title":"MySQL 索引"},{"content":"Linux 是一个开源的系统，使得 Linux 发行版本有很多，利用 Linux 开发产品的团队也有很多，如果任由每个人都按照自己的想法来配置 Linux 系统文件目录，后期可能会产生诸多的管理问题。\n为了避免诸多使用者对 Linux 系统目录结构天马行空，Linux 基金会发布了 FHS 标准。多数 Linux 发行版系统都遵循这一标准。\nFHS（Filesystem Hierarchy Standard），文件系统层次化标准，该标准规定了 Linux 系统中所有一级目录以及部分二级目录（/usr 和 /var）的用途。发布此标准的主要目的就是为了让用户清楚地了解每个目录应该存放什么类型的文件。\n几乎所有的 Linux 发行版都包含以下的目录结构：\n不同的发行版可能会添加一下独有的目录，例如 Ubuntu 中包含 /snap 来放置 snap 应用。\n/bin Binary\n二进制可执行文件和可执行文件的快捷方式（软链接），存放着常用的命令。\n例如常用的 ls cp cd 等命令都存放在这里。\n/boot 启动 Linux 时使用的一些核心文件，包括一些连接文件以及镜像文件。\n如果是以 UEFI 方式安装的系统，其中 efi 文件夹中的为 EFI 分区中的引导。\n/dev Device\n存放着所有的设备文件。在 Linux 中，所有东西都是以文件的形式存在的，包括硬件设备。\n/etc Etcetera\n存放程序的配置信息，大多数安装的应用配置信息都会在这里。\n例如：\n/etc/apt apt 源配置目录 /ect/vim vim 配置目录 /etc/nginx nginx 配置目录 /ect/redis redis 配置目录 /home 用户的主目录。\n在 Linux 中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名。\n/lib Library\n系统最基本的动态连接共享库，其作用类似于 Windows 里的 DLL 文件。\n几乎所有的应用程序都需要用到这些共享库。包含 bin 和 sbin 中可执行文件的依赖。\n/media 自动挂载的设备，比如 U 盘，移动硬盘，网络设备等的目录。\n比如现在有一个 U 盘，插到电脑上之后，系统会把 U 盘自动挂载到 /media/$USER 文件夹中。\n/mnt 提供给用户临时手动挂载设备文件夹，一般是空文件夹。\n比如现在要手动挂载一个硬盘，可用如下挂载到 /mnt：\n1 mount /dev/sdb1 /mnt/disk /opt Optional\n一些第三方软件安装的目录。\n比如 Chrome 浏览器，WPS 等软件，都会安装在这里。\n/root 这是 root 用户的家目录。\n/proc Processes\n/proc 是一种伪文件系统（也即虚拟文件系统），存储的是当前内核运行状态的一系列特殊文件，这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。\n/proc 里面有一大堆数字命名的文件夹，这个数字其实是 Process ID（PID），对应着运行的服务。\n/sbin Superuser Binaries\n这里存放的是系统管理员使用的系统管理程序。\n例如常见的 groupadd groupdel。\n/srv Service\n主要用来存放服务数据。对于桌面版 Linux 系统，这个文件夹一般是空的，但是对于 Linux 服务器，Web 服务或者 FTP 文件服务的资源可以存放在这里。\n/tmp Temporary\n存储一些程序的临时文件。\n临时文件可能起到很重要的作用。比如有时候 Word 文档崩溃了，好不容易写的东西全没了，Linux 的很多文本编辑器都会在 /tmp 放一份当前文本的 copy 作为临时文件，如果你的编辑器意外崩溃，有机会在 /tmp 找一找临时文件抢救一下。\n/usr Unix Shared Resources\n共享资源，这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于 windows 下的 program files 目录。\n/usr/bin /usr/sbin 系统用户 / 超级管理员 使用的应用程序。\n在 Ubuntu 中，/bin /sbin 其实就是 /usr/bin /usr/sbin 的一个链接。\n/usr/src 内核源代码默认的放置目录。\n/usr/lib 包含所有那些用户不能直接执行的库文件。包含着所有 \u0026lsquo;/usr/bin\u0026rsquo; 和 \u0026lsquo;/usr/sbin\u0026rsquo; 目录中可执行命令程序需调用的二进制库文件。\n例如使用 apt 安装 openjdk 的目录在 /usr/lib/jvm。\n/usr/share 包含独立于架构的共享数据。\n例如系统桌面图标目录 /user/share/applications。\n/usr/local 用于本地安装软件的目标目录。从源代码安装的用户程序都将安装到这里。\n/var Variable\n存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。\n例如日志文件。\n参考 https://segmentfault.com/a/1190000038497705 https://www.runoob.com/linux/linux-system-contents.html ","date":"2022-06-23T02:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/10.jpeg","permalink":"https://emerywan.github.io/blog/p/linux-dir/","title":"Linux 目录结构介绍"},{"content":"🌱 介绍 许多平台都提供了免费的静态页面托管的服务，如 Github Pages，Vercel，Netlify等。但在国内由于一些“原因”，这些国外的服务在国内的访问并不稳定。\n国内的免费托管平台如 Gitee 限制很多，不可以自定义域名而且之前出现了防盗链问题，访问也不是很快，不太推荐作为托管平台。\n腾讯云推出的云开发 CloudBase 也有静态页面托管服务。其实它相比这些静态托管平台，有更多强大的功能，甚至能够搭一整套系统。虽然是付费服务，但是在按量付费的情况下资费不是很高，在博客访问量不是很高的情况下十分合适。\n目前在腾讯云中暂时还没有 Hugo 的模板。目前有两种方式可以达到自动部署的功能：\n🌰 使用 Github Actions 编译，通过 Tencent CloudBase Github Action 自动部署到 CloudBase。\n🌰 使用 Github Actions 编译，推送到 Web 应用托管（webify） 的简易静态页面模板。\n🏖 使用 CloudBase 使用 CloudBse 时，使用按量计费环境会有一些免费用量。\n☁️ 腾讯云 创建环境 在 云开发 CloudBase 新建一个应用，选择 空模板，根据自身需求填写信息。\n创建成功后，获得 环境ID。\n获取 API 密钥 为部署新建一个密钥对。在 访问管理 -\u0026gt; 用户列表 -\u0026gt; 新建用户 -\u0026gt; 自定义创建 -\u0026gt; 可访问资源并接收消息。🔗 传送门\n根据自己的需要，新建用户名后选择 编程访问，点击下一步。\n在自定义策略中勾选：\nQcloudAccessForTCBRole：授予云开发（TCB）对云资源的访问权限； QcloudAccessForTCBRoleInAccessCloudBaseRun：供云开发（TCB）服务角色（TCB_QcsRole）进行关联，用于 TCB 访问其他云服务资源。包含私有网络 VPC、云服务器 CVM 相关操作权限。 点一下一步后，新建用户成功。可以获得 SecretId 和 SecretKey。\n⚙️ Github Actions 设置 Github Secrets 在项目的 Settings -\u0026gt; Secrets -\u0026gt; Actions 中添加上述得到的 ENV_ID，SECRET_ID，SECRET_KEY（名称可以自定义）。\n添加 workflows 可以在仓库的 Actions 中 new workflow，或者在项目中 .github/workflows 添加。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 name: Tencent CloudBase on: push: branches: - main jobs: hugo-publish: name: publish content to static website runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 with: submodules: true fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: latest extended: true - name: Build run: hugo --minify --gc # 使用云开发 Github Action 部署 - name: Deploy to Tencent CloudBase uses: TencentCloudBase/cloudbase-action@v2 with: secretId: ${{ secrets.QCLOUD_SECRET_ID }} secretKey: ${{ secrets.QCLOUD_SECRET_KEY }} envId: ${{ secrets.QCLOUD_ENV_ID }} 🚧 提示：\n这里使用了 TencentCloudBase/cloudbase-action@v2，需要在项目根目录添加 cloudbaserc.json。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \u0026#34;envId\u0026#34;: \u0026#34;{{env.ENV_ID}}\u0026#34;, // 这里需要更改为你的 环境ID，或者在 .env 文件中配置 \u0026#34;$schema\u0026#34;: \u0026#34;https://framework-1258016615.tcloudbaseapp.com/schema/latest.json\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;framework\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hugo-blog\u0026#34;, \u0026#34;plugins\u0026#34;: { \u0026#34;client\u0026#34;: { \u0026#34;use\u0026#34;: \u0026#34;@cloudbase/framework-plugin-website\u0026#34;, \u0026#34;inputs\u0026#34;: { \u0026#34;outputPath\u0026#34;: \u0026#34;public\u0026#34;, \u0026#34;ignore\u0026#34;: [ \u0026#34;.git\u0026#34;, \u0026#34;.github\u0026#34;, \u0026#34;cloudbaserc.js\u0026#34; ] } } } } } 🚧 提示：\n这里也可以选择 Tencent CloudBase Github Action V1。🙋‍♂️ （推荐）\nV2 比 V1 有更多功能，比如拉取代码在腾讯云端编译，但目前没有 Hugo 模板。我们已经在 Github Actions 产出了静态文件，直接推送即可。所以目前对于 hugo 来说，没什么区别，使用甚至 V1 更快更简洁。\n1 2 3 4 5 6 7 8 # 将上述 jobs 内替换为 - name: Deploy to Tencent CloudBase uses: TencentCloudBase/cloudbase-action@v1.1.1 with: secretId: ${{ secrets.QCLOUD_SECRET_ID }} secretKey: ${{ secrets.QCLOUD_SECRET_KEY }} envId: ${{ secrets.QCLOUD_ENV_ID }} staticSrcPath: public 🏝 使用 webify 使用 Web 应用托管（webify）主要是利用 Github Actions 生成 静态页面到另一个分支，再托管这个分支的内容。\n☁️ 创建服务 在 Web 应用托管 -\u0026gt; 新建应用 新建一个简易静态页面模板。根据需求填写信息。选择纯静态页面。\n在 应用列表 -\u0026gt; 应用设置 中配置仓库信息，并根据自身情况选择静态文件的部署分支。（如果当前没有生成静态页面的分支，可完成后面操作后再进行此步骤）。\n⚙️ Github Actions 设置 Github Token 在用户的 Settings -\u0026gt; Developer settings -\u0026gt; Personal access tokens -\u0026gt; Generate new token 获取一个 Repo Token。🔗 传送门\n在项目的 Settings -\u0026gt; Secrets -\u0026gt; Actions 中添加上述得到的 Token。\n添加 workflows 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 name: Tencent CloudBase on: push: branches: - main jobs: hugo-publish: name: publish content to static website runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 with: submodules: true fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: latest extended: true - name: Build run: hugo --minify --gc - name: Deploy to Branch uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.ACCESS_TOKEN }} keep_files: false publish_branch: gh-pages # 更改为你想要生成的分支 publich_dir: ./public commit_message: ${{ github.event.head_commit.message }} ⛓ 参考 🔗 https://blog.wangjunfeng.com/post/hugo-cloudbase/ 🔗 https://cloud.tencent.com/document/product/1210/43389 🔗 https://github.com/TencentCloudBase/cloudbase-action 🔗 https://github.com/TencentCloudBase/cloudbase-action/blob/3354b442713265aa9d7c5bf03b0b8cb0173f546f/README.md ","date":"2022-06-22T21:14:45+08:00","image":"https://emerywan.github.io/blog/imgs/3.jpeg","permalink":"https://emerywan.github.io/blog/p/hugo-in-cloudbase/","title":"使用 Github Actions 在腾讯云 CloudBase 部署 Hugo"},{"content":"使用 ZooKeeper 实现 原理 持久节点（红色）\n瞬时节点（黄色）：不可再有子节点，会话结束后瞬时节点会自动消失\n利用 Zookeeper 的瞬时有序节点的特性，多线程并发创建瞬时节点时，得到有序的序列\n序号最小的线程获得锁，其他的线程则监听自己序号的前一个序号；当前一个线程执行完成，删除自己序号的节点；下一个序号的线程得到通知，将继续执行\n创建节点时，已经确定了线程的执行顺序\n实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 public class ZkLock implements AutoCloseable, Watcher { private ZooKeeper zooKeeper; private String znode; // 当前线程创建的路径 public ZkLock(String connectString, int sessionTimeout) throws IOException { this.zooKeeper = new ZooKeeper(connectString, sessionTimeout, this); } /** * try-catch-with-resource */ public void clode() throws Exception { zooKeeper.delete(znode, -1); zooKeeper.close(); log.info(\u0026#34;释放锁\u0026#34;); } /** * 观察器 */ @Override public void process(WatchedEvent watchedEvent) { // 当前一个节点被删除时，可唤醒获取锁的等待，表示当前线程获取了锁 if (watchedEvent.getType() == Event.EventType.NodeDeleted) { synchronized (this) { notify(); } } } /** * 获取锁 */ public boolean getLock(String businessCode) { try { String path = \u0026#34;/\u0026#34; + businessCode; Stat stat = zooKeeper.exists(path, false); if (null == stat) { // 当前业务若没有持久节点 // 先创建一个持久节点，在持久节点里创建瞬时节点 zooKeeper.create( path, businessCode.getBytes(StandardCharsets.UTF_8), ZooDefs.Ids.OPEN_ACL_UNSAFE, // 公开权限，不使用用户名密码就能访问这个节点 CreateMode.PERSISTENT ); } // 创建瞬时有序节点 String subPath = \u0026#34;/\u0026#34; + businessCode + \u0026#34;/\u0026#34; + businessCode + \u0026#34;_\u0026#34;; znode = zooKeeper.create( subPath, businessCode.getBytes(StandardCharsets.UTF_8), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL ); // 获取该业务下的所有瞬时节点，进行比较排序 List\u0026lt;String\u0026gt; childrenNodes = zooKeeper.getChildren(path, false); Collections.sort(childrenNodes); // 如果列表中的第一个是当前创建的节点，直接获取锁 String firstNode = childrenNodes.get(0); if (znode.endsWith(firstNode)) { return true; } // 监听前一个节点，等待获取锁 String lastNode = firstNode; for (String node : childrenNode) { // 找到当前节点，监听前一个节点 if (znode.endWith(node)) { zooKeeper.exists(\u0026#34;/\u0026#34; + businessCode + \u0026#34;/\u0026#34; + lastNode, this); break; } else { lastNode = node; } } synchronized (this) { // 让当前线程阻塞，并且会释放锁 // 上面的同步代码块才会获得锁并执行 notify() wait(); } return true; } catch (KeeperException e) { e.printStackTrace(); } catch (InterruptedException e) { e.printStackTrace(); } return false; } } 测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Slf4j @RestController public class ZkLockController { @GetMapping(\u0026#34;/zkLock\u0026#34;) public String zkLock() { log.info(\u0026#34;进入方法\u0026#34;); try (ZkLock zklock = new ZkLock(\u0026#34;127.0.0.1:2181\u0026#34;, 10000)) { if (zkLock.getLock(\u0026#34;order\u0026#34;)) { log.info(\u0026#34;获取了锁\u0026#34;); try { TimeUtil.SECONDS.sleep(10); // 模拟业务 } catch (InterruptedException e) { e.printStackTrace(); } } log.info(\u0026#34;完成业务\u0026#34;); } catch (Exception e) { e.printStackTrace(); } return \u0026#34;success\u0026#34;; } } Curator 依赖配置 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.curator\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;curator-recipes\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @Slf4j public class DemoTest { @Test public void test() { String zookeeperConnectionString = \u0026#34;127.0.0.1:2181\u0026#34;; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); CuratorFramework client = CuratorFrameworkFactory.newClient(zookeeperConnectionString, retryPolicy); client.start(); String lockPath = \u0026#34;/order\u0026#34;; InterProcessMutex lock = new InterProcessMutex(client, lockPath); try { // 获取互斥锁，阻塞直到可用，或给定时间到期 // 同一个线程调用 acquire() 可重入 if (lock.acquire(30, TimeUnit.SECONDS)) { try { log.info(\u0026#34;获取锁\u0026#34;); } finally { // 必须通过 release() 释放锁 lock.release(); } } } catch (Exception e) { e.printStackTrace(); } } } ","date":"2022-05-11T21:14:45+08:00","image":"https://emerywan.github.io/blog/imgs/2.jpeg","permalink":"https://emerywan.github.io/blog/p/distributed-lock/zookeeper/","title":"使用 ZooKeeper 实现分布式锁"},{"content":"Redis NX 实现 利用 NX 的原子性，多个线程并发时，只有一个线程可以设置成功，设置成功即获取了锁。\n如果没有获取锁，不会阻塞当前方法，直接跳过任务。\n获取锁 1 2 # set key unique_value NX PX 30000 set product:stock:clothes UUID NX PX 30000 key 根据不同的业务，区分不同的锁 unique_value 保证每个线程的随机值都不同，用于释放锁时的校验 NX key 不存在时设置成功，key 存在则不成功 PX 自动失效时间。若出现异常，没有主动释放锁，可以保证超时后，锁可以过期失效（毫秒） 释放锁 释放锁将该 key 删除，在释放锁之前需要校验设置的随机数，相同才表示是该线程加的锁，能释放。\n需要采用 LUA 脚本，del 命令没有提供校验值的功能。\nredis 执行命令是按照一条指令完成之后，再执行下一条，用 lua 脚本，能保证 redis 执行完这个脚本才执行下一条，所以能保证判断 和 删除是原子性的\n1 2 3 4 5 if redis.call(\u0026#34;get\u0026#34;, KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;, KEYS[1]) else return 0 end 实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 @Slf4j @RestController public class RedisLockController { @Autowired private RedisTemplate redisTemplate; @GetMapping(\u0026#34;/redisLock\u0026#34;) public String redisLock() { // 获取分布式锁 Boolean lock = redisTemplate.execute((RedisCallback\u0026lt;Boolean\u0026gt;) redisConnection -\u0026gt; { Expiration expiration = Expiration.seconds(30); // NX RedisStringCommands.SetOption setOption = RedisStringCommands.SetOption.ifAbsent(); // 需要使用 redisTemplate 中的序列化器 byte[] redisKey = redisTemplate.getKeySerializer().serialize(key); byte[] redisValue = redisTemplate.getValueSerializer().serialize(value); return redisConnection.set(redisKey, redisValue, expiration); }); if (lock) { // 获取到了锁 log.info(\u0026#34;获取到了锁\u0026#34;); try { TimeUnit.SECONDS.sleep(15); // 模拟业务处理 } catch (InterruptedException e) { e.printStackTrace(); } finally { String script = \u0026#34;if redis.call(\\\u0026#34;get\\\u0026#34;,KEYS[1])==ARGV[1] then\\n\u0026#34; + \u0026#34;\\treturn redis.call(\\\u0026#34;del\\\u0026#34;,KEYS[1])\\n\u0026#34; + \u0026#34;else\\n\u0026#34; + \u0026#34;\\treturn 0\\n\u0026#34; + \u0026#34;end\u0026#34;; RedisScript\u0026lt;Boolean\u0026gt; redisScript = RedisScript.of(script, Boolean.class); boolean result = redisTemplate.execute(redisScript, Arrays.asList(key), value); log.info(\u0026#34;释放锁 {}\u0026#34;, result); } } log.info(\u0026#34;业务完成\u0026#34;); return \u0026#34;success\u0026#34;; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @Slf4j @RestController public class RedisLock { private RedisTemplate redisTemplate; // 锁名称，不同业务可能锁不同 private String key; private String value; // 锁过期时间，单位秒 private int expireTime; public RedisLock(RedisTemplate redisTemplate, String key, int expireTime) { this.redisTemplate = redisTemplate; this.key = key; this.expireTime = expireTime; // value 可以不暴露出去，每个线程都是不一样的 this.value = UUID.randomUUID().toString(); } /** * 获取锁 */ public boolean getLock() { // 获取分布式锁 Boolean lock = (Boolean) redisTemplate.execute((RedisCallback\u0026lt;Boolean\u0026gt;) redisConnection -\u0026gt; { Expiration expiration = Expiration.seconds(expireTime); // NX RedisStringCommands.SetOption setOption = RedisStringCommands.SetOption.ifAbsent(); // 由于这里需要接受 byte, 不能暴力的使用 string.getBytes() // 要使用模板里面的 key\\value 序列化器来实现 byte[] redisKey = redisTemplate.getKeySerializer().serialize(key); byte[] redisValue = redisTemplate.getValueSerializer().serialize(value); Boolean result = redisConnection.set(redisKey, redisValue, expiration, setOption); return result; }); return lock; } /** * 释放锁 */ public boolean unLock() { // lua 脚本 String script = \u0026#34;if redis.call(\\\u0026#34;get\\\u0026#34;,KEYS[1])==ARGV[1] then\\n\u0026#34; + \u0026#34;\\treturn redis.call(\\\u0026#34;del\\\u0026#34;,KEYS[1])\\n\u0026#34; + \u0026#34;else\\n\u0026#34; + \u0026#34;\\treturn 0\\n\u0026#34; + \u0026#34;end\u0026#34;; RedisScript\u0026lt;Boolean\u0026gt; redisScript = RedisScript.of(script, Boolean.class); Boolean result = (Boolean) redisTemplate.execute(redisScript, Arrays.asList(key), value); return result; } } Redisson 依赖配置 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.redisson\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;redisson-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.16.7\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @Slf4j @RestController public class RedissonLockController { @Autowired private RedissonClient redissonClient; @GetMapping(\u0026#34;/redissonLock\u0026#34;) public String redissonLock() { log.info(\u0026#34;执行方法\u0026#34;); final String key = \u0026#34;redisson\u0026#34;; RLock lock = redissonClient.getLock(key); // 锁超时时间，如果未获得锁，会阻塞等待获取到锁（-1 表示没有超时时间） lock.lock(30, TimeUnit.SECONDS); log.info(\u0026#34;获取锁\u0026#34;); try { TimeUnit.SECONDS.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } finally { log.info(\u0026#34;释放锁\u0026#34;); lock.unlock(); } log.info(\u0026#34;完成业务\u0026#34;); return \u0026#34;success\u0026#34;; } } ","date":"2022-05-11T12:14:45+08:00","image":"https://emerywan.github.io/blog/imgs/1.jpeg","permalink":"https://emerywan.github.io/blog/p/distributed-lock/redis/","title":"使用 Redis 实现分布式锁"},{"content":"原理 CyclicBarrier 通过可重入锁 CyclicBarrier 实现，内部使用静态内部类 Generation 维护 broken 标记，表示屏障是否可用。\n通过构造方法传入等待线程数 parties。\n每次有一个线程到达屏障， count - 1，直到 count = 0，会释放屏障，唤醒所有等待的线程，并使用 nextGeneration() 对 count 和 generation 进行重置，进行下一轮循环。\n有三种情况会造成屏障破坏：\nCyclicBarrier Runnable command 方法抛出异常\n在释放条件还未达到时，某个线程抛出了异常\nawait() 等待超时\n源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 public class CyclicBarrier { // 内部类，存放 broken 标记，表示屏障是否被损坏，损坏后无法正常工作 private static class Generation { boolean broken = false; } // 可重入锁 private final ReentrantLock lock = new ReentrantLock(); private final Condition trip = lock.newCondition(); // 屏障阻挡的线程数，构造方法传入的初始化值 private final int parties; // 屏障释放时执行的方法 private final Runnable barrierCommand; // 当前的 Generation 对象，每一轮都会有一个新的 Generation 对象，存放 broken 标记 private Generation generation = new Generation(); // 当前还需要阻挡几个线程，每次 -1 private int count; // 开启下一轮屏障（count==0 或 reset） private void nextGeneration() { // 唤醒所有等待线程 trip.signalAll(); // 重置 count count = parties; generation = new Generation(); } // 破坏当前屏障，变为不可用状态，可以使用 reset 恢复 private void breakBarrier() { generation.broken = true; count = parties; trip.signalAll(); } private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException { final ReentrantLock lock = this.lock; // 加锁，有多个线程会调用 await() 只有一个线程能够进入同步方法 lock.lock(); try { final Generation g = generation; if (g.broken) // 屏障是否损坏 throw new BrokenBarrierException(); if (Thread.interrupted()) { breakBarrier(); throw new InterruptedException(); } int index = --count; if (index == 0) { // 达到条件，可打破屏障 boolean ranAction = false; try { final Runnable command = barrierCommand; if (command != null) // 执行打破屏障的命令 command.run(); ranAction = true; nextGeneration(); // 开启下一轮循环，里面会唤醒所有等待的线程 return 0; } finally { if (!ranAction) // (1) 假如 command 出现异常，破坏屏障 breakBarrier(); } } // 当前还未达到冲破屏障的的条件 // 一直循环等待，直到到达打破屏障的条件，中断，或超时 for (;;) { try { if (!timed) trip.await(); // 未达到超时时间，进行等待，直到被唤醒 else if (nanos \u0026gt; 0L) nanos = trip.awaitNanos(nanos); } catch (InterruptedException ie) { // (2) 等待到达条件时，线程被中断，破坏屏障 if (g == generation \u0026amp;\u0026amp; ! g.broken) { breakBarrier(); throw ie; } else { // We\u0026#39;re about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // \u0026#34;belong\u0026#34; to subsequent execution. Thread.currentThread().interrupt(); } } if (g.broken) // 线程被唤醒之后，屏障被破坏，直接抛出异常 throw new BrokenBarrierException(); if (g != generation) // 返回当前线程是第几个到达的线程 return index; if (timed \u0026amp;\u0026amp; nanos \u0026lt;= 0L) { // (3) 等待超时，破坏屏障 breakBarrier(); throw new TimeoutException(); } } } finally { lock.unlock(); } } public CyclicBarrier(int parties, Runnable barrierAction) { if (parties \u0026lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction; } public CyclicBarrier(int parties) { this(parties, null); } public int getParties() { return parties; } // 开始等待 返回当前线程是第几个到达的线程 public int await() throws InterruptedException, BrokenBarrierException { try { return dowait(false, 0L); } catch (TimeoutException toe) { throw new Error(toe); // cannot happen } } public int await(long timeout, TimeUnit unit) throws InterruptedException, BrokenBarrierException, TimeoutException { return dowait(true, unit.toNanos(timeout)); } // 判断屏障是否被破坏 public boolean isBroken() { final ReentrantLock lock = this.lock; lock.lock(); // 需加锁访问，其他线程可能在执行 dowait() try { return generation.broken; } finally { lock.unlock(); } } // 重置操作 先破坏屏障，再进行下一轮循环屏障 public void reset() { final ReentrantLock lock = this.lock; lock.lock(); try { breakBarrier(); // break the current generation nextGeneration(); // start a new generation } finally { lock.unlock(); } } // 获取等待线程数 public int getNumberWaiting() { final ReentrantLock lock = this.lock; lock.lock(); try { return parties - count; } finally { lock.unlock(); } } } ","date":"2022-05-03T23:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/14.jpeg","permalink":"https://emerywan.github.io/blog/p/thread/cyclicbarrier/","title":"Java 并发 - CyclicBarrier 实现原理"},{"content":"基本实现思路： 利用 共享锁 实现\n初始化时，state=count 即已经上了 count 次共享锁\nawait() 即加共享锁，必须 state=0 时才能加锁成功，否则按照 AQS 机制，进入等待队列阻塞\ncountDown() 解一次锁，直到为 0\n源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 public class CountDownLatch { // 内部类实现 AQS private static final class Sync extends AbstractQueuedSynchronizer { private static final long serialVersionUID = 4982264981922014374L; Sync(int count) { // 使用 AQS 的 state 作为计数器 setState(count); } int getCount() { return getState(); } // 采用 **共享锁** 机制，因为可以被不同的线程 countdown protected int tryAcquireShared(int acquires) { return (getState() == 0) ? 1 : -1; } protected boolean tryReleaseShared(int releases) { // 每次执行将 state-1，直到 state=0 for (;;) { int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) // 通过 CAS 改变 state 的值，失败会进行下一轮循环 return nextc == 0; } } } private final Sync sync; public CountDownLatch(int count) { if (count \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;count \u0026lt; 0\u0026#34;); this.sync = new Sync(count); } // 通过 acquireSharedInterruptibly 获取共享锁 // 若 state!=0 会被持续阻塞 public void await() throws InterruptedException { sync.acquireSharedInterruptibly(1); // 1 这里是随意的参数，在 coutdownlatch 中无意义 } public boolean await(long timeout, TimeUnit unit) throws InterruptedException { return sync.tryAcquireSharedNanos(1, unit.toNanos(timeout)); } // 解锁一次 public void countDown() { sync.releaseShared(1); } // 获取当前计数 public long getCount() { return sync.getCount(); } public String toString() { return super.toString() + \u0026#34;[Count = \u0026#34; + sync.getCount() + \u0026#34;]\u0026#34;; } } ","date":"2022-05-03T14:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/6.jpeg","permalink":"https://emerywan.github.io/blog/p/thread/countdownlatch/","title":"Java 并发 - CountDownLatch 实现原理"},{"content":"说明 Ubuntu 有些第三方软件只提供二进制执行文件（压缩包或 AppImage），而不是安装包或者 apt 源，这时候需要我们去自己解压文件，放在自定义目录，并进行配置。\n平常使用桌面环境的话，这样安装后，更新还是有点麻烦的，一更新就要修改很多地方。\n这里介绍我平常的配置方式，每次更新后，只需要修改软链接即可，感觉还是挺方便的。\n方式 以 cfw 安装为例，Linux 提供 .tar.gz 的执行文件。\n我通常都把这样的软件都统一放到 ~/Applications 中。\n🏕️ 首先创建一个安装的软件的文件夹 1 mkdir -p ~/Application/clash 🏞️ 将下载的文件解压到该文件夹中 1 tar -xzvf Clash.for.Windows-0.19.12 -C ~/Application/clash 🏜️ 创建启动文件的软连接 1 ln -s ./Clash\\ for\\ Windows-0.19.12-x64-linux/cfw ./clash 通过这种方式，每次启动应用都使用这个软链接进行操作，当需要升级应用时，只需要创建新的软链接，指向新的启动文件。最后的效果如下：\n1 2 3 4 5 $ tree . -L 1 . ├── clash -\u0026gt; ./Clash for Windows-0.19.12-x64-linux/cfw ├── Clash for Windows-0.19.12-x64-linux └── Clash for Windows-0.19.2-x64-linux 🏖️ 添加桌面图标 1 vim ~/.local/share/applications/clw.desktop 添加以下内容：\n1 2 3 4 5 6 7 8 9 [Desktop Entry] Name=Clash for Linux Icon=/home/emery/Applications/icon/clash_icon.png # 替换为自己的目录 Comment=Clash for Linux Exec=\u0026#34;/home/emery/Applications/Clash/clash\u0026#34; %u # 替换为自己的目录 Type=Application Categories=Network Terminal=false StartupNotify=true ","date":"2022-05-02T02:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/4.jpeg","permalink":"https://emerywan.github.io/blog/p/ubuntu-software-install/","title":"Ubuntu 安装软件推荐方式"},{"content":"Nvdia 驱动 首先，请确保在系统中安装了英伟达显卡驱动，可以使用 nvidia-msi 命令查看显卡对应信息。\n如果没有安装驱动，可以参考该章节。传送门➡️\n推荐使用系统自带软件 软件和更新 安装显卡驱动。打开应用后，点击 附加驱动 选项，会对所有可选的附加驱动进行搜索。\n其中，Nouveau 为英伟达显卡开源驱动（默认安装），选择需要的 NVIDIA drive 驱动选项，点击 应用更改。\n等待进度条结束后，重启电脑，即完成了显卡驱动的安装。\nCUDA 你可以在 这里 看到显卡驱动版本与 CUDA 版本的对应关系。\n🌰 在这里的显卡驱动版本为 470.103.01，根据对应表可知，最高可选的 CUDA Toolkit 的版本号为 CUDA 11.4 Update 4。\n之后，可在官网 该页面 中，选择对应的 CUDA Toolkit。\n可根据当前系统环境，选择对应的下载选项（推荐下载 .drunfile，.deb 会覆盖系统安装的显卡驱动）：\n下载完成后，使用以下命令进行安装：\n1 2 3 4 cd ${DOWNLOAD_DIR} chmod +x ./cuda_{Version}_linux.run sudo sh ./cuda_{Version}_linux.run ::: warning 警告 🚧 注意：在安装过程中，请取消勾选安装驱动选项。 :::\n配置环境变量，在对应配置文件中添加如下内容：\n1 2 3 # bash -\u0026gt; .bashrc / zsh -\u0026gt; .zshrc export CUDA_HOME=/usr/local/cuda-{Version} export PATH=${CUDA_HOME}/bin:${PATH} cuDNN 你可以在官网 此链接 下载 CUDNN，需要注册开发者账号，可能还需要一个“良好的网络环境”。\n选择与 CUDA 相对应用的 cuDNN，推荐下载压缩包格式。\n下载完成后，解压文件夹，并根据以下命令复制解压后文件夹中的 lib64 include 文件夹：\n1 2 3 4 tar -xzvf cudnn-linux-{Version}.tar.gz sudo cp cudnn-linux-{Version}/lib64/* /usr/local/cuda/lib64 sudo cp cudnn-linux-{Version}/include/* /usr/local/cuda/include 复制完成后，可以通过以下命令查看 cuDNN 版本信息：\n1 cat /usr/local/cuda-10.1/include/cudnn.h | grep CUDNN_MAJOR -A 2 ","date":"2022-05-02T02:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/11.jpeg","permalink":"https://emerywan.github.io/blog/p/cuda/","title":"在 Ubuntu 中搭建 CUDA 环境"},{"content":"AQS AQS AbstractQueuedSynchronized 是一个用来构建锁和同步器的框架，是对 CAS 的一种封装和丰富，AQS 引入了独占锁，共享锁等性质。\n使用 AQS 能够基于 Java API，简单高效地构建出应用广泛的大量同步器，用于 Java 多线程之间的同步。\n核心思想 AQS 使用一个 int 成员变量表示同步状态。\n使用 CAS 对该同步状态进行原子操作，实现对其值的修改。\n1 2 // 共享变量，使用 volatile 保障可见性 private volatile int state; AQS 通过内置的 FIFO 双向队列来完成获取资源线程的排队工作。\n1 2 private transient volatile Node head; private transient volatile Node tail; 当请求的共享资源空闲时，将当前请求资源的线程设置为有效的工作线程，并将共享资源设置为锁定状态。\n如果被请求的共享资源被占用，将暂时获取不到锁的线程加入到队列中。\n资源的共享方式 独占 当一个线程以独占模式获取锁时，其他任何线程都必须等待。\nReentrantLock 共享 当一个线程以共享模式获取锁时，其他也想以共享模式获取锁的线程也能够一起访问共享资源，但其他想以独占模式获取锁的线程需要等待。\nCountDownLatch CyclicBarrier Semaphore 实现 CountDownLatch CountDownLatch 同步工具允许一条或多条线程，等待其他线程中的一组操作完成后，再继续执行。\nCountDownLatch 任务分为 n 个子线程执行，state 将被初始化为 n，这 n 个线程将会并发执行，每个子线程执行完后调用 countDown() 方法，state 会通过 CAS - 1。当所有的子线程都执行完之后，state=0，会 unpack 主线程，继续后续动作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 public class Demo { static class SearchTask implements Runnable { private Integer id; private CountDownLatch latch; public SearchTask(int id, CountDownLatch latch) { this.id = id; this.latch = latch; } // **子线程** // 各自寻找相应目标的龙珠 @Override public void run() { System.out.println(\u0026#34;🏃‍♂️ 开始寻找\u0026#34; + id + \u0026#34;号龙珠\u0026#34;); int seconds = ThreadLocalRandom.current().nextInt(20); try { Thread.sleep(seconds * 1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;👏 花了\u0026#34; + seconds + \u0026#34;秒，找到了\u0026#34; + id + \u0026#34;号龙珠\u0026#34;); // 每当一个任务完成，就调用一次 countDown() 方法 latch.countDown(); } } // **主线程** // 等子线程都执行完毕后，找到所有龙珠，召唤神龙 public static void main(String[] args) { List\u0026lt;Integer\u0026gt; idList = Arrays.asList(1, 2, 3, 4, 5, 6, 7); // state 初始化为任务的个数 CountDownLatch latch = new CountDownLatch(idList.size()); for (int id : idList) { Thead thread = new Thead(new SearchTask(id, latch)); thread.start(); } try { latch.await(); // 等待其他线程完成 } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;找到所有龙珠了！召唤神龙 🐲\u0026#34;); } } 在这个例子中，任务分为 7 个子线程执行，将 CountDownLatch 的值初始化为与线程相同的次数。主线程通过 await() 等待其他任务完成。\n这几个子任务并发执行，每个子线程执行完成后 countDown() 一次，state 会通过 CAS 操作 -1。\n直到所有的子线程都执行完毕之后，state=0，主线程会结束等待，继续执行后续任务。\nCyclicBarrier CyclicBarrier 用于多个线程相互等待对方执行到某个状态后，这些线程再继续执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 public class Demo { private static int count = 5; // CyclicBarrier 可以设置子线程全部到达 barrier，执行的任务 private static CyclicBarrier barrier = new CyclicBarrier(count, () -\u0026gt; { System.out.println(\u0026#34;👌 人员到齐！准备爬山！\u0026#34;); }); public static void main(String[] args) { // 子线程，模拟 5 个同学相约爬山 for (int i = 1; i \u0026lt;= 5; i++) { final String name = i + \u0026#34;同学\u0026#34;; new Thread(() -\u0026gt; { System.out.println(\u0026#34;👋 \u0026#34; + name + \u0026#34;准备出发去集合点了。\u0026#34;); int time = ThreadLocalRandom.current().nextInt(10); try { TimeUnit.SECONDS.sleep(time); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;🦶 \u0026#34; + name + \u0026#34;花了 \u0026#34; + time + \u0026#34; 秒到达集合点。\u0026#34;); try { // 等待所有的同学（子线程）运行到此处 barrier.await(); // 当 await 的线程足够，再继续执行线程后续任务 } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } System.out.println(name + \u0026#34;和大家一起爬山了。⛰\u0026#34;); }, i + \u0026#34;同学\u0026#34;).start(); } } } 在这个例子中，有 5 个子线程分别开始执行（5 个同学从家里出发），每个线程到达某个状态后 await() 进行等待（每个同学到达集合点会等待其他同学到达），当线程全部达到之后，“屏障”会释放，线程将继续执行（人员到齐后开始爬山）。\nCyclicBarrier 可以多次使用，可以自动或手动重置计数。 使用 barrier.reset() 会进行重置，等待的线程会抛出 BrokenBarrierException\n1 2 3 4 5 // CyclicBarrier 的计数 count = 5 // 子线程的数量 i = 10 将子线程的数量设置为 10，循环屏障设置为 5，将会得到如下结果：每有 5 个，线程到达“屏障”点，就会释放一次。\nSemaphore 通过使用 Semaphore，我们可以决定某个资源同一时间能够被访问的最大线程数，它相当于对某个资源的访问进行了流量控制。\n是一个可以被 n 个线程占用的排它锁，可以在最开始设定 Semaphore 许可证数量，每个线程都可以获得 1 个或多个许可证，当许可证耗尽或不足以供其他线程获取时，其他线程将被阻塞。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public static void main(String[] args) throws ExecutionException, InterruptedException { Semaphore semaphore = new Semaphore(2); for (int i = 0; i \u0026lt; 3; i++) { new Thread(() -\u0026gt; { try { semaphore.acquire(); // 无参为申请一个许可证 System.out.println(\u0026#34;申请许可证成功\u0026#34;); TimeUnit.SECONDS.sleep(5); // 模拟任务耗时 semaphore.release(); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); } } ReentrantLock ReentrantReadWriteLock ","date":"2022-04-13T14:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/6.jpeg","permalink":"https://emerywan.github.io/blog/p/thread/aqs/","title":"Java 并发 - AQS"},{"content":"单例模式 单例模式所要实现的目标：保持一个类有且只有一个实例。\n出于性能的考虑，不少单例模式都会采用延迟加载的方式。\n根据是否延迟加载，可以分为：\n饿汉式\n懒汉式\n单线程的单例模式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public class Singleton { private static Singleton instance = null; // 构造器私有，其他类无法通过 new 初始化 private Singleton() { } // 创建并返回该类的唯一实例 public static Singleton getInstance() { if (null == instance) { instance = new Singleton(); } } public void someService() { // 一些业务代码 } } 加锁的单例模式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class Singleton { private static Singleton instance = null; private Singleton() { } public static Singleton getInstance() { sychronized (Singleton.class) { // 每次判空操作，都要获取锁，开销极大 if (null == instance) { instance = new Singleton(); } } return instance; } } 使用双重检验判断\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class Singleton { private volatile static Singleton instance = null; private Singleton() { } public static Singleton getInstance() { if (instance == null) { synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); } } } return instance; } } 使用 volatile 修饰实例，利用了以下两个作用：\n(1) 保障可见性：\n一个线程初始化了 instance 的值，其他线程可以读取到相应的值。\n(2) 保障有序性：\n保障一个线程读取到的 instance 引用的实例已经初始化完毕。\n对象实例化会分解成以下几个子操作：\n分配对象所需的空间 初始化 instance 引用的对象 将对象的引用写入共享变量 由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-\u0026gt;3-\u0026gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。\n例如：线程 T1 执行了 1 和 3 后，此时 T2 调用 getInstance() 后发现 instance 不为空，因此直接返回，但此时 instance 还未被初始化。\n静态内部类的单例模式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class Singleton { private Singleton() { } private static class InstanceHolder { // 静态内部类 // 保存外部类的唯一实例 final static Singleton INSTANCE = new Singleton(); } public static Singleton getInstance() { return InstanceHolder.INSTANCE; } public void someService() { // ... } public static void main(String[] args) { Singleton.getInstace().someService(); } } 当外部类加载时，内部类不会加载，并且静态变量只会有默认值（null）。\n类的静态变量被初次访问才会触发 Java 虚拟机对该类进行初始化，会变为其初始值。\n枚举单例 1 2 3 4 5 6 7 8 9 10 public enum Singleton { INSTANCE; Singleton() { // 私有构造器 } pubilc void someService() { // 业务到吗 } } 字段 INSTANCE 是枚举类的唯一实例。在 Singleton.INSTANCE 初次被引用时才会被初始化。\n","date":"2022-04-11T18:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/8.jpeg","permalink":"https://emerywan.github.io/blog/p/thread/singleton/","title":"线程安全的单例模式"},{"content":"在并发情况下，多个线程会对同一个资源进行争抢，可能会导致数据不一致的问题。\n为了解决这个问题，通过引入锁机制，使用一种抽象的锁，对资源进行锁定，达到同步访问。\n实现 Java 采用了两种实现方式：\n基于 Object 的悲观锁\n基于 CAS 的乐观锁\n悲观锁 在 Java 中，每个对象（Object），都拥有一把锁，存放在对象头中，记录了当前对象被哪个线程所占用。\n内部锁 synchronized synchronized 关键字可以用来同步线程，其被编译后，会生成 monitorenter 和 moniterexit 两个字节码指令，用来进行线程的同步。\nsynchronized 是非公平锁。一把锁只能被一个线程获取，没有获得锁的线程只能等待。线程对内部锁的申请和释放由 Java 虚拟机负责实施。\n加锁方式：\n对象锁\n修饰实例方法 pubilc sychronized void method() { ... }\n代码块锁住当前对象 synchronized(this)\n类锁\n修饰静态方法 public static synchronized void method() { ... }\n代码块锁定 class 对象 synchronized(MyClass.class) { ... }\nsynchronized 修饰的方法，无论方法正常执行完还是抛出异常，都会释放锁。\nJDK 1.6 对 synchronized 的优化 synchronized 依赖于 JVM，使用的是操作系统底层的 Mutex Lock 实现。\nJava 线程的实现是对操作系统线程的映射，每当唤醒或挂起一个线程的时候，都要切换到操作系统的内核态，代价是非常昂贵的。\nJDK 1.6 对锁的实现引入了大量的优化，如 锁粗化，锁消除，轻量级锁，偏向锁，适应性自旋等。\n对象锁一共有四种状态：\n无锁 -\u0026gt; 偏向锁 -\u0026gt; 轻量级锁 -\u0026gt; 重量级锁\n会随着竞争情况逐渐升级（不可以降级），提高获取锁和释放锁的效率。\n🔒 无锁状态\n某些资源不会出现多线程竞争的情况，随意多个线程调用\n利用其他一些方式控制同步。如：CAS\n不对资源进行操作系统级别的锁定。\n🔒 偏向锁\n适合只有一个线程访问同步代码块的场景 在一些情况下，没有多线程的竞争，每次都是同一个线程多次获取锁，那么对象锁会“记住”这个线程，只要是这个线程过来，就直接把锁交出去。\n如果对象发现目前不是只有一个线程，而是有多个线程在竞争锁，偏向锁就会升级为轻量级锁。\n🔒 轻量级锁\n适合同步代码块的执行速度非常快的场景 当锁升级为轻量级锁的时候，其他线程会通过 CAS 进行自旋等待来获取锁，不会阻塞，从而提高性能。\n🔒 重量级锁\n适合同步代码块执行速度较长的场景 对象锁状态被标记为重量级锁，需要通过 Monitor 来对线程进行控制。\n乐观锁 CAS 在一些情况下，同步代码块执行的时间远远小于线程切换的耗时。所以希望能够在用户态中对线程的切换进行管理，这样效率更高。\n我们让线程“乐观地”反复尝试获取共享资源，当发现空闲时便进行使用，否则继续“乐观地”进行重试。\n基于以上想法，诞生了 CAS (Compare And Swap) 算法：比较并交换。\nCAS算法涉及到三个操作数：\n需要读写的内存值 addr 进行比较的值 oldValue 要写入的新值 newValue 1 2 3 4 5 6 7 8 int cas(long *addr, long oldValue, long newValue) { /* Executes atomically. */ if(*addr != old) return 0; *addr = new; return 1; } 这个 cas 函数看起来没有任何同步措施，似乎还是存在线程不安全的问题。\n当 A 线程比较了 oldValue 的值是想要的，但是这个瞬间，B 线程突然抢到了时间片，更改了值；但是 A 线程并不知道，将值改成了 newValue，这就出现了线程安全问题，A B 两个线程同时获得了资源。\n所以，CAS 的操作必须是 原子性 的。这个原子操作，由计算机处理器指令集提供，直接由硬件保障。\n原子变量类 基础数据类型\nAtomicInteger AtomicLong AtomicBoolean 数组类型\nAtomicIntegerArray AtomicLongArray AtomicReferenceArray 字段更新类型\nAtomicIntegerFieldUpdater AtomicLongFieldUpdater AtomicReferenceFieldUpdater 引用类型\nAtomicReference AtomicStampedReference AtomicMarkableReference 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class Demo { private static AtomicInteger n = new AtomicInteger(0); public static void main(String[] args) { for (int i = 0; i \u0026lt; 3; i++) { new Thread(() -\u0026gt; { while (n.get() \u0026lt; 1000) { System.out.println( Thread.currentThread().getName() + \u0026#34; : \u0026#34; + n.incrementAndGet() ); } }, \u0026#34;Thread\u0026#34; + i).start(); } } } AtomicInteger 主要由一个 Unsafe 类型的实例 unsafe 和 Long 类型的 offset 实现。\nJava 通过 Unsafe 的 CAS 操作来对 volatile int 值进行更新。根据 value 在对象中的偏移量，CAS 操作内存数据，执行一些底层，和平台相关的方法。\n1 2 3 4 5 6 7 8 9 10 11 // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset(AtomicInteger.class.getDeclareFiele(\u0026#34;value\u0026#34;)); } catch (Exception e) { throw new Error(ex); } } private volatile int value; 例如 incrementAndGet()。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // AtomicInteger.java public final int incrementAndGet() { return unsafe.getAndAddInt(this, valueOffset, 1) + 1; } // Unsafe.java public final int getAndAddInt(Object o, long offset, int delta) { int v; do { v = getIntVolatile(o, offset); } while (!compareAndSwapInt(o, offset, v, v + delta)); // 自旋，可以通过启动参数配置，默认是 10 return v; } public final native boolean compareAndSwapInt(Object o, long offset, int expected, int x); 缺点：\n循环开销大\n只能处理一个共享变量\n封装成对象 AtomicReference ABA\nABA 问题 CAS 需要在操作值的时候检查内存值是否发生变化，没有发生变化才会更新内存值。\n如果内存值原来是 A，后来变成了 B，然后又变成了 A，那么 CAS 进行检查时会发现值没有发生变化，但是实际上是有变化的。\nA -\u0026gt; B -\u0026gt; A\n可以为共享变量引入一个修订号（时间戳），每次更新都会更新相应的修订号，判断变量的值是否被其他线程给修改过。AtomicStampedReference\n[A, 0] -\u0026gt; [B, 1] -\u0026gt; [A, 2]\n1 2 3 4 5 6 7 8 9 10 11 public static void main(String[] args) { String a = \u0026#34;Hello\u0026#34;; String b = \u0026#34;World\u0026#34;; // initialRef initialStamp AtomicStampedReference\u0026lt;String\u0026gt; reference = new AtomicStampedReference\u0026lt;\u0026gt;(a, 1); // expectedReference newStamp reference.attemptStamp(a, 2); // 对版本号进行修改 // expectedReference newReference expectedStamp newStamp reference.compareAndSet(a, b, 2, 3); // CAS 操作需要同时提供 预期值 新值 预期版本号 新版本号 } ","date":"2022-04-11T14:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/7.jpeg","permalink":"https://emerywan.github.io/blog/p/thread/lock/","title":"Java 并发 - 锁机制"},{"content":"创建 1 2 3 4 5 6 7 8 9 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { // ... } corePoolSize 核心线程数\n每次向线程池提交一个多线程任务，都会创建一个新的核心线程，无论是否有空闲线程，直到核心线程数的数量，才会开始复用线程资源。 使用 prestartAllCoreThreads() 直接初始化全部 maximumPoolSize 最大线程数\n当所有线程都处于运行状态，且等待队列已满，会开始创建非核心线程，但不超过最大线程数 keepAliveTime 线程最大空闲时间\n非核心线程超过最大空闲时间，会自动销毁 unit 最大空闲时间单位\nworkQueue 线程等待队列\n核心线程数已满时，会将任务暂存到等待队列，直到线程资源可用 threadFactory 线程创建工厂\nhandler 拒绝策略\n当等待队列已满，并且达到最大线程数，新的任务会进行拒绝处理 线程池的大小 CPU 密集型 主要是执行计算任务，相应很快，CPU 利用率高。推荐：核心线程数 = CPU 核心数 + 1\nIO 密集型 主要是进行 IO 操作，如硬盘读取数据等，IO 操作时间长，CPU 利用不高，推荐：核心线程数 = 2 * CPU 核心数。\n拒绝策略 AbortPolicy\n抛出异常拒绝任务 DiscardPolicy\n不抛出异常，拒绝任务 DiscardOldestPolicy\n丢弃最老的任务 CallerRunsPolicy\n将任务交给调用线程执行 使用 1 2 3 4 // execute executor.execute(() -\u0026gt; { // do ... }); 1 2 3 4 5 6 7 // submit Future\u0026lt;String\u0026gt; future = executor.submit(() -\u0026gt; { // do... return \u0026#34;返回值\u0026#34;; }); String res = future.get(); 常见的线程池 newSingleThreadExecutor 只有一个线程的线程池 唯一线程可以保证所提交的任务顺序执行 1 2 3 4 5 6 public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService( new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;())); } newFixedThreadPool 固定线程数的线程池 1 2 3 4 5 public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThread, nThread, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;()); } newCachedThreadPool 根据需要创建新线程的线程池 1 2 3 4 5 public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;()); } 当提交新任务时，如果线程池为空或没有空闲线程，则创建新线程执行任务。\n当线程空闲时间超过 keepAliveTime=60s，会自动释放线程资源。长时间空闲的 CachedThreadPool 不会持有任务任何线程资源。\nScheduledThreadPoolExecutor ScheduledThreadPoolExecutor 可以用来提交定时任务，extends ThreadPoolExecutor。\n1 2 3 public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize); } ScheduledThreadPoolExecutor 最大线程池容量为 Integer.MAX_VALUE；都是采用的DelayedWorkQueue 作为等待队列。\n1 2 3 4 5 ScheduledExecutorService executor = Executors.newScheduledThreadPool(1); // 延迟 3 s 后执行任务 executor.schedule(() -\u0026gt; System.out.println(\u0026#34;定时任务\u0026#34;), 3, TimeUnit.SECONDS); executor.shutdown(); 1 2 3 4 ScheduledExecutorService executor = Executors.newScheduledThreadPool(2); // 延迟 3 s 后，每 1 s 执行一次任务 executor.scheduleAtFixedRate(() -\u0026gt; System.out.println(\u0026#34;定时任务\u0026#34;), 3, 1, TimeUnit.SECONDS); ForkJoinPool Fork -\u0026gt; 拆分 Pool -\u0026gt; 合并\n把大任务拆分为多个小任务，最后汇总多个小任务的结果，得到整大任务的结果。这些小任务都是同时进行，可提高运算效率。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 public class Demo { public static void main(String[] args) { ForkJoinTask\u0026lt;Integer\u0026gt; task = new SubTask(1, 1000); int res = ForkJoinPool.commonPool().invoke(task); System.out.println(res); } private static class SubTask extends RecursiveTask\u0026lt;Integer\u0026gt; { private final int start; private final int end; public SubTask(int start, int end) { this.start = start; this.end = end; } @Override protected Integer compute() { if (end - start \u0026lt; 125) { // 任务足够小，直接计算 System.out.println(Thread.currentThread().getName() + \u0026#34; 开始计算 \u0026#34; + start + \u0026#34;-\u0026#34; + end + \u0026#34;的值\u0026#34;); int result = 0; for (int i = start; i \u0026lt;= end; i++) { result += i; } return result; } // 任务太大，拆分成子任务 SubTask subTask1 = new SubTask(start, (end + start) / 2); subTask1.fork(); SubTask subTask2 = new SubTask((end + start) / 2 + 1, end); subTask2.fork(); return subTask1.join() + subTask2.join(); } } } 工作窃取：\n子任务会被放到不同的队列中，每个队列会有对应的线程运行任务。\n若某个队列中的任务计算完毕，但其他的任务队列中还有任务，会从其他队列中“窃取”任务继续执行。\n参考 https://segmentfault.com/a/1190000039267451 ","date":"2022-03-30T16:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/9.jpeg","permalink":"https://emerywan.github.io/blog/p/thread/threadpool/","title":"Java并发 - 线程池"},{"content":"进程与线程 进程是程序向操作系统申请资源的基本单位。\n线程是进程中可独立执行的最小单位。\n一个进程可以包括多个线程，同一进程中所有线程共享该进程中的资源。\n无处不在的线程 main 线程 垃圾回收线程 Web 服务器请求处理线程 线程的创建 extends Thread 1 2 3 4 5 6 class MyThread extends Thread { @Override public void run() { System.out.println(Thread.currentThread().getName()); } } implement Runnable 1 2 3 4 5 6 7 8 9 10 class RunnableThread implement Runnable { @Override public void run() { System.out.println(Thread.currentThread().getName()); } public static void main(String[] args) { new Thread(new RunnableThread()).start(); } } implement Callable\u0026lt;\u0026gt; Callable 可以有返回值，返回值通过 FutureTask 进行封装。\n1 2 3 4 5 6 7 8 9 10 11 12 class CallbaleThead implement Callbale\u0026lt;Integer\u0026gt; { @Override public Integer call() throws Exception { return 666; } public static void main(String[] args) { FutureTask\u0026lt;Integer\u0026gt; futureTask = new FutureTask\u0026lt;\u0026gt;(new CallbaleThead()); new Thread(futureTask).start(); int result = futureTask.get(); } } 线程池 1 2 3 4 5 6 7 8 9 public class Demo { public static void main(String[] args) { ExecutorService executorService = Executors.newFixThreadPool(10); for (int i = 0; i \u0026lt; 10; i++) { executorService.execute(() -\u0026gt; System.out.println(Thread.currentThread().getName())); } } } 1 2 3 4 5 6 7 8 9 10 11 12 public class Demo { public static void main(String[] args) { ExecutorService executorService = Executors.newFixThreadPool(10); Future\u0026lt;Integer\u0026gt; future = executorService.submit(() -\u0026gt; { Thread.sleep(1000); return 666; }); int result = future.get(); } } 线程的生命周期 1 2 3 4 5 6 7 8 9 10 public class Thread implements Runnable { public enum State { NEW, RUNNABLE, BLOCKED, WAITING, TIMED_WAITING, TErMINATED; } } 新建 New 创建后尚未启动。\n可运行 Runnable 可能正在运行，也可能正在等待 CPU 时间片。\n阻塞 Blocking 等待获取一个排它锁，如果其线程释放了锁就会结束此状态。\n无限期等待 Waiting 等待其它线程显式地唤醒，否则不会被分配 CPU 时间片。\n限期等待 Timed Waiting 无需等待其它线程显式地唤醒，在一定时间之后会被系统自动唤醒。\n死亡 Terminated 线程结束任务之后自己结束，或者产生了异常而结束。\n参考 https://pdai.tech/md/java/thread/java-thread-x-thread-basic.html#%e5%ae%9e%e7%8e%b0-callable-%e6%8e%a5%e5%8f%a3 ","date":"2022-03-30T14:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/9.jpeg","permalink":"https://emerywan.github.io/blog/p/thread/thread/","title":"Java 多线程"},{"content":"☘️ 通用程序设计 Effective Java 阅读笔记。本章主要讨论了 Java 语言的具体细节。包括：\n局部变量 控制结构 类库 数据结构 不是由语言本身提供的机制 优化和命名惯例。 57. 将局部变量的作用域最小化 🌸 建议 🌱 最小化局部变量的范围，可提高代码的可读性和可维护性，降低出错的可能性。在第一次使用它的地方声明变量。\n🍀 每个局部变量声明都应该包含一个初始化表达式。try-catch 语句除外。\n如果一个变量初始化，会抛出一个 checked 异常，必须在 try 中初始化（除非所包含的方法可以抛异常）。\n如果该值必须在 try 块之外使用，那么它必须在 try 块之前声明，此时它还不能“合理地初始化“。\n1 2 3 4 5 Set\u0026lt;String\u0026gt; set = null; try { set = cons.newInstance(); } catch () { } 🍀 循环结束后不再需要循环变量，for 循环就优于 while 循环（for 循环允许声明循环变量）。\n🌱 保持方法小而集中，每个操作都用一个方法来完成。避免一个操作相关的局部变量可能在另一个操作中。\n🌻 案例 🌾 遍历集合的首选习惯用法\n1 2 3 for (Element e : c) { // do something with e } 💐 需要访问 Iterator / 调用 Iterator 的 remove()，首选 for\n1 2 3 4 for (Iterator\u0026lt;Element\u0026gt; i = c.iterator(); i.hasNext(); ) { // i is loop variable Element e = i.next(); // do something with i and e } 💐 循环习惯用法：最小化局部变量的范围\n1 2 3 4 // 每次循环都会调用 expensiveComputation()，且返回结果相同 for (int i = 0, n = expensiveComputation(); i \u0026lt; n; i++) { // do something with i } 它有两个循环变量：i 和 n，都具有完全正确的作用域。第二个变量 n 用于存储第一个变量的极限，避免了每次迭代中冗余计算的成本。 如果循环涉及一个方法调用，并且保证在每次迭代中返回相同的结果，应该使用这个习惯用法。\n58. for-each 循环优于传统的 for 循环 🌸 建议 🌱 for-each 隐藏迭代器或索引变量，可消除混乱和出错。\n🌱 : 表示 “在\u0026hellip;里面”。使用 for-each 循环不会降低性能。\n🥀 以下情况不应该使用 for-each：\n🌰 破坏性过滤。如果需要遍历一个集合并删除选定元素，需要使用显式的迭代器，以便调用其 remove()。\n🌰 转换。遍历时需要替换其中元素的值，需要 List 迭代器或数组索引来替换元素的值。\n🌰 并行迭代。如果需要并行遍历多个集合，那么需要显式地控制迭代器或索引变量，以便所有迭代器或索引变量都可以同步执行。\n🌻 案例 💐 使用 Collection.removeIf() 可避免显式的遍历（Java8）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public interface Collection\u0026lt;E\u0026gt; extends Iterable\u0026lt;E\u0026gt; { default boolean removeIf(Predicate\u0026lt;? super E\u0026gt; filter) { Objects.requireNonNull(filter); boolean removed = false; final Iterator\u0026lt;E\u0026gt; each = interator(); while (each.hasNext()) { if (filter.test(each.next)) { each.remove(); removed = true; } } return removed; } } 59. 了解并使用库 🌸 建议 能用库方法就用库方法。\n具有算法背景的高级工程师花了大量时间设计、实现和测试库方法，然后将库方法展示给该领域的几位专家，以确保库方法是正确的。 这些库方法经过 beta 测试、发布，并被数百万程序员广泛使用了近 20 年。\n🌱 使用标准类库的好处：\n🌰 利用编写它的专家的知识和以前使用它的人的经验。 🌰 不必浪费时间为那些与你的工作无关的问题编写专门的解决方案。 🌰 随着时间的推移，它们的性能会不断提高，无需付出任何努力。（版本更新） 🌰 可以将代码放在主干中。这样的代码更容易被开发人员阅读、维护和复用。 🌱 在每个主要版本中，都会向库中添加许多特性，了解这些新增特性是值得的。\n🍀 每个程序员都应该熟悉 java.lang、java.util 和 java.io 的基础知识及其子包。 其他库的知识可以根据需要获得。尤其是：\n🌰 java.util.Collections 🌰 java.util.Streams 🌰 java.util.concurrent 🍀 如果你在 Java 平台库中找不到你需要的东西，你的下一个选择应该是寻找高质量的第三方库，比如谷歌的优秀的开源 Guava 库。如果你无法在任何适当的库中找到所需的功能，只能自己实现它。\n🌻 案例 💐 从 Java 7 开始，就不应该再使用 Random。\n🌰 在大多数情况下，选择的随机数生成器现在是 ThreadLocalRandom。 它能产生更高质量的随机数，而且速度非常快。ThreadLocalRandom.current().nextInt(100)\n🌰 对于 Fork Join Pool 和并行 Stream，使用 SplittableRandom。\n假设你想要生成 0 到某个上界之间的随机整数，许多程序员会编写一个类似这样的小方法：\n1 2 3 4 5 // 有严重缺陷 static Random rnd = new Random(); static int random(int n) { return Math.abs(rad.nextInt()) % n; } 它有三个缺点:\n🌩 n 是小的平方数，随机数序列会在相当短的时间内重复 🌩 如果 n 不是 2 的幂，那么平均而言，一些数字将比其他数字更频繁地返回 🌩 在极少数情况下会返回超出指定范围的数字 Integer.MAX_VALUE 应该直接使用类库 Ramdom.nextInt(n) ，而不是自己封装\n1 public int nextInt(int n); 🌾 Linux curl in Java 9\n假设你想编写一个程序来打印命令行中指定的 URL 的内容（这大致是 Linux curl 命令所做的）。在 Java 9 之前，这段代码有点乏味，但是在 Java 9 中，transferTo() 被添加到 InputStream 中。这是一个使用这个新方法执行这项任务的完整程序：\n1 2 3 4 5 public static void main(String[] args) throws IOException { try (InputStream in = new URL(args[0]).openStream()) { in.transferTo(System.out); } } 60. 若需要精确答案就应避免使用 float double 🌸 建议 🌱 float 和 double 类型特别不适合进行货币计算，因为不能将 0.1（或 10 的任意负次幂）精确地表示。 1 2 // 0.6100000000000001 System.out.println(1.03 - 0.42); 🍀 数值不是太大，可以使用 int 或 long 🌰 如果数值不超过 9 位小数，可以使用 int 🌰 如果不超过 18 位，可以使用 long 🌰 如果数量可能超过 18 位，则使用 BigDecimal 例如：在微信支付的 Java SDK 中，货币的类型为 int，结算单位为 分； 在支付宝支付的 Java SDK 中，货币的类型为 String，结算单位为 元\n🌱 使用 BigDecimal 类型代替 double / float。注意，使用 BigDecimal 的 String 构造函数而不是它的 double / float 构造函数。 61. 基本数据类型优于包装类 🌸 建议 🌱 Java 类型系统由两部分组成：\n🌰 基本类型（primitive type）：int double boolean 🌰 引用类型（reference type）：String List 每个基本类型都有一个对应的引用类型，称为包装类型。 🌱 基本类型和包装类型之间有三个主要区别：\n🌰 基本类型只有它们的值，而包装类型具有与其值不同的标识 🌰 基本类型只有全功能值，而每个包装类型除了对应的基本类型的所有功能值外，还有一个非功能值，即 null 🌰 基本类型比包装类型更节省时间和空间 🍀 要有选择，就应该优先使用基本类型，而不是包装类型。基本类型更简单、更快。\n🥀 以下情况必须使用包装类型：\n🌰 作为集合中的元素、键和值 🌰 必须使用包装类型作为类型参数，Java 不允许使用基本类型。如 ThreadLocal\u0026lt;Integer\u0026gt; 🌰 进行反射方法调用时，必须使用包装类型 🌻 案例 🌾 以下三个例子为常见的包装类型问题：\n🌩 将 == 操作符应用于包装类型几乎都是错误的\n1 2 🙅‍♂️ Comparator\u0026lt;Integer\u0026gt; naturalOrder =(i, j) -\u0026gt; (i \u0026lt; j) ? -1 : (i == j ? 0 : 1); 表达式 i \u0026lt; j 会使 i 和 j 引用的 Integer 实例自动拆箱。 表达式 i == j 对两个对象引用执行比较，返回 false。\n1 2 3 4 5 🙆 Comparator\u0026lt;Integer\u0026gt; naturalOrder = (iBoxed, jBoxed) -\u0026gt; { int i = iBoxed, j = jBoxed; // 自动拆箱 return i \u0026lt; j ? -1 : (i == j ? 0 : 1); }; 🌩 NullPointerException\n1 2 3 4 5 6 7 public class Unbelievable { static Integer i; public static void main(String[] args) { if (i == 42) System.out.println(\u0026#34;Unbelievable\u0026#34;); } } 计算表达式 i == 42 时抛出 NullPointerException。空对象引用自动拆箱，将得到一个 NullPointerException。\n🌩 严重的性能问题\n1 2 3 4 5 6 7 public static void main(String[] args) { Long sum = 0L; for (long i = 0; i \u0026lt; Integer.MAX_VALUE; i++) { sum += i; } System.out.println(sum); } 局部变量 sum，它是包装类型 Long，而不是基本类型 long。变量被反复装箱和拆箱（超出缓存部分时），导致产生明显的性能下降。\n62. 其他类型更合适时，应避免使用字符串 🌸 建议 🌱 当存在或可以编写更好的数据类型时，应避免将字符串用来表示对象。\n🌱 如果使用不当，字符串比其他类型更麻烦、灵活性更差、速度更慢、更容易出错。\n🍀 字符串不适合代替其他的值类型。\n🌰 如果是数值类型，则应将其转换为适当的数值类型，如 int、float 或 BigInteger。 🌰 如果是问题的答案，如“是”或“否”这类形式，则应将其转换为适当的 Enum 或 boolean。 🌰 更一般地，如果有合适的值类型，无论是基本类型还是对象引用，都应该使用它；如果没有，应该写一个。 🌱 字符串不适合代替枚举类型。\n🌱 字符串不适合代替聚合类型。\n如果一个实体有多个组件，将其表示为单个字符串通常是很不合适的。 例如，下面这行代码来自一个真实的系统标识符：\n1 String compoundKey = className + \u0026#34;#\u0026#34; + i.next(); 这种方法有很多缺点：如果用于分隔字段 # 的字符出现在其中一个字段中，可能会导致混乱。要访问各个字段，你必须解析字符串。不能提供 equals、toString 或 compareTo 方法，但必须接受 String 提供的行为。 更好的方法是编写一个类来表示聚合，通常是一个私有静态成员类。（Item-24）。\n🍀 字符串不适合代替 capabilities\n有时，字符串用于授予对某些功能的访问权。\n例如，考虑线程本地变量机制的设计。这样的机制提供了每个线程都有自己的变量值。 这种方法的问题在于：字符串键表示线程本地变量的共享全局名称空间。 为了使这种方法有效，客户端提供的字符串键必须是唯一的：如果两个客户端各自决定为它们的线程本地变量使用相同的名称，它们无意中就会共享一个变量，这通常会导致两个客户端都失败。 而且安全性很差。恶意客户端可以故意使用与另一个客户端相同的字符串密钥来非法访问另一个客户端的数据。\n63. 当心字符串连接引起的性能问题 🌸 建议 🌱 不要使用字符串连接操作符合并多个字符串，除非性能无关紧要。使用字符串串联运算符 +，重复串联 n 个字符串，需要 n 的平方级时间。\n🍀 使用 StringBuilder 代替 String\n64. 通过接口引用对象 🌸 建议 🍀 优先使用接口而不是类来引用对象。如果存在合适的接口类型，那么应该使用接口类型声明参数、返回值、变量和字段。\n🌱 使用接口作为类型的习惯，程序将更加灵活。\n🥀 以下情况使用 类 来引用对象：\n🌰 没有合适的接口存在 String 和 BigInteger 🌰 框架的基本类型是类，不是接口 java.io 🌰 实现接口的类提供了接口中不存在的额外方法 PriorityQueue 中实现了 Queue 不存在的 comparator 🌻 案例 🌾 优先使用接口作为类型 1 2 3 4 5 🙆 Set\u0026lt;Son\u0026gt; sonSet = new LinkedHashSet\u0026lt;\u0026gt;(); 🙅‍♂️ LinkedHashSet\u0026lt;Son\u0026gt; sonSet = new LinkedHashSet\u0026lt;\u0026gt;(); 65. 接口优先反射机制 🌸 建议 🌱 核心反射机制 java.lang.reflect 提供对任意类的编程访问。给定一个 Class 对象，可以获得 Constructor、Method、Filed 实例。\n🌱 通过过调用 Constructor、Method、Filed 实例上的方法，可以构造底层的实例、调用底层类的方法、访问底层类中的字段。\n🌰 Method.invoke() 🥀 反射允许一个类使用另一个类，即使在编译前者时后者并不存在。然而，这种能力是有代价的：\n🌰 失去了编译时类型检查的所有好处，包括异常检查。 🌰 执行反射访问所需的代码既笨拙又冗长。写起来很乏味，读起来也很困难。 🌰 性能降低。 🍀 如果你对应用程序是否需要反射有任何疑问，那么它可能不需要。\n🌱 如果必须用到在编译时无法获取的类，在编译时存在一个适当的接口或超类来，可以用反射方式创建实例，并通过它们的接口或超类正常地访问它们。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 public static void main(String[] args) { // Translate the class name into a Class object Class\u0026lt;? extends Set\u0026lt;String\u0026gt;\u0026gt; cl = null; try { cl = (Class\u0026lt;? extends Set\u0026lt;String\u0026gt;\u0026gt; cl = null) Class.forNmae(args[0]) // Unchecked cast } catch (ClassNotFoundException e) { fatalError(\u0026#34;Class not found.\u0026#34;); } // Get the constructor Constructor\u0026lt;? extends Set\u0026lt;String\u0026gt;\u0026gt; cons = null; try { cons = cl.getDeclaredConstructor(); } catch (NoSuchMethodException e) { fatalError(\u0026#34;No parameterless constructor\u0026#34;); } // Instantiate the set Set\u0026lt;String\u0026gt; s = null; // 🙋‍♂️ use interface try { s = cons.newInstance(); } catch (IllegalAccessException e) { fatalError(\u0026#34;Constructor not accessible\u0026#34;); } catch (InstantiationException e) { fatalError(\u0026#34;Class not instantiable.\u0026#34;); } catch (InvocationTargetException e) { fatalError(\u0026#34;Constructor threw \u0026#34; + e.getCause()); } catch (ClassCastException e) { fatalError(\u0026#34;Class doesn\u0026#39;t implement Set\u0026#34;); } // Exercise the set s.addAll(Arrays.asList(args).subList(1, args.length)); System.out.println(s); } 🥀 上例中反射机制的缺点\n🌰 产生 6 个运行时异常 🌰 根据类名生产实例需要 25 行冗长代码，而调用构造器只需要一行。 🌱 反射的合法用途（很少）是管理类对运行时可能不存在的其他类、方法或字段的依赖关系。\n🌱 如果编写的程序必须在编译时处理未知的类，则应该尽可能只使用反射实例化对象，并使用在编译时已知的接口或超类访问对象。\n66. 谨慎地使用本地方法 🌸 建议 🌱 非常不建议使用本地方法。JVM 变得更快了，并且 Java 平台的成熟，它提供了对许多以前只能在宿主平台中上找到的特性。 例如，Java 9 中添加的流 API 提供了对 OS 流程的访问。\n🥀 使用本地方法有严重的缺点： 🌰 本地语言不安全（Item-50），使用本地方法的应用程序不再能免受内存毁坏错误的影响。 🌰 Java 更依赖于平台，因此使用本地方法的程序的可移植性较差。 🌰 它们更难调试。 🌰 如果不小心，本地方法可能会降低性能，因为垃圾收集器无法自动跟踪本地内存使用情况，而且进出本地代码会产生相关的成本。 🌰 本地方法需要“粘合代码”，这很难阅读，而且编写起来很乏味。 67. 谨慎地进行优化 🌸 建议 🍀 优化弊大于利，尤其是如果过早地进行优化。在此过程中，你可能会生成既不快速也不正确且无法轻松修复的软件。\n🍀 不要为了性能而牺牲合理的架构。努力编写好的程序，而不是快速的程序。\n🍀 好的程序体现了信息隐藏的原则：在可能的情况下，它们在单个组件中本地化设计决策，因此可以在不影响系统其余部分的情况下更改单个决策\n🍀 不优化不意味着在程序完成之前可以忽略性能问题。实现上的问题可以通过以后的优化来解决，但是对于架构缺陷，如果不重写系统，就不可能解决限制性能的问题，特别是在设计API、线路层协议和持久数据格式时。\n🍀 再多的底层优化也不能弥补算法选择的不足。\n68. 遵守被广泛认可的命名约定 🌸 建议 🌱 Java 平台有一组完善的命名约定，其中许多约定包含在《The Java Language Specification》\n🌰 typographical 字面约定 🌰 grammatical 语法约定 🌱 字面惯例示例：\nIdentifier Type Example Package or module org.junit.jupiter.api, com.google.common.collect Class or Interface Stream, FutureTask, LinkedHashMap, HttpClient Method or Field remove(), groupingBy(), getCrc() Constant Field MIN_VALUE, NEGATIVE_INFINITY Local Variable i, denom, houseNum Type Parameter T, E, K, V, X, R, U, V, T1, T2 🌱 语法命名约定比排版约定更灵活，也更有争议。\n","date":"2022-03-30T14:02:02+08:00","image":"https://emerywan.github.io/blog/p/effective-java/general-programming/effective-java_hu8285674e0bb74a2ea271ae0a19ba75ab_128063_120x120_fill_box_smart1_3.png","permalink":"https://emerywan.github.io/blog/p/effective-java/general-programming/","title":"通用程序设计"},{"content":"这一节，将介绍 LSTM (Long Shorter Term Memory)，以及用 pytorch 实现 LSTM 。\nLSTM 是一种 RNN 模型，是对 simple RNN 的改进，LSTM 可以避免梯度消失的问题，可以有更长的记忆。LSTM 的论文在 1997 年发表。\nHochreiter and Schmidhuber.\tLong short-term\tmemory. Neural computation, 1997.\n🔖 LSTM LSTM 也是一种循环神经网络，原理跟 simple RNN 差不多，每当读取一个新的输入 $x$，就会更新状态 $h$。\nLSTM 的结构比 simple RNN 要复杂很多，simple RNN 只有一个参数矩阵， LSTM 有四个参数矩阵。接下来我们具体来看看 LSTM 的内部结构。\n🚠 传送带 LSTM 最重要的设计是这个传送带 Conveyor belt，即为向量 $C$。过去的信息通过传送带，直接送到下一个时刻，不会发生太大的变化。LSTM 就是靠传送带来避免梯度消失的问题。\nLSTM 中有很多个门 gate，可以有选择的让信息通过。\n🚪 Forgate Gate 首先介绍 forget gate 遗忘门。遗忘门由 ☘️ sigmoid 函数，和 🍀 元素积 element wise multiplication 两部分组成。\n🌼 输入 sigmoid 的是一个向量 $a$，sigmoid 作用到向量 $a$ 的每一个元素上，把每一个元素都压到 0 和 1 之间。\n举个例子，假如向量 $a$ 是：[1, 3, 0, -2]，那么，sigmoid 函数将分别作用在这四个元素上。然后分别输出：[0.73, 0.95, 0.5, 0.12] 。\n输入的向量 $a$，与输出的向量 $f$ 应该有相同的维度，这个例子里，向量 $a$ 是四维的，向量 $f$ 也会是四维的。\n🌸 算出 $f$ 向量之后，计算传送带向量 $c$ 与遗忘门向量 $f$ 的元素积。元素积 element wise multiplication 是这样算的：\n$c$ 和 $f$ 都是四维的向量，将它们的每一个元素分别相乘。所以元素积的结果也是个四维的向量。\n这个遗忘门 $f$，有选择的让传送带 $c$ 的值通过：\n🌰 假如 $f$ 的一个元素是 $0$，那么 $c$ 对应的元素不能通过，对应的输出是 $0$；\n🌰 假如 $f$ 的一个元素是 $1$，那么 $c$ 对应的元素就全部通过，对应的输出是 $c$ 本身。\n遗忘门 $f$ 具体是这么算出来的：首先看这张结构图，$f_t$ 是上一个状态 $h_{t-1}$，与当前输入 $x$ 的函数。\n把状态 $h_{t-1}$ 与输入 $x_t$ 做拼接 concatnation，得到更高维度的向量。然后计算矩阵 $w_f$ 与这个向量的乘积，得到一个向量，再用 sigmoid 函数，得到向量 $f_t$，$f_t$ 的每一个元素都介于 0 和 1 之间，遗忘门有一个参数矩阵 $w_f$，需要通过 反向传播 从训练数据里学习。\n🚪 Input Gate 刚才讲了遗忘门，现在来看一看 input gate 输入门。在这张结构图里，输入门 $i_t$，依赖于旧的状态向量 $h_{t-1}$，和新的输入 $x_t$。\n输入门 $i_t$ 的计算类似于遗忘门，把旧的状态 $h_{t-1}$，与新的输入 $x_t$ 做拼接 concatnation，得到更高维的向量。\n然后计算矩阵 $w_i$ 与这个向量的乘积得到一个向量，最后使用激活函数 sigmod，得到向量 $i_t$（$i_t$ 的每一个元素都介于 $0$ 和 $1$ 之间）。\n输入门也有自己的参数矩阵，计作 $W_i$，$W_i$ 也需要从训练数据中学习。\n🆕 New Value 还需要计算新的输入值 new value $\\widetilde{c}_t$，$\\widetilde{c}_t$ 是个向量，计算方法跟遗忘门和输入门都很像。也是把旧状态 $h_{t-1}$，与新输入 $x_t$ 做拼接，再乘到参数矩阵上。\n区别在于激活函数不是 sigmoid，而是双曲正切函数 tanh，所以算出的向量 $\\widetilde{c}_t$ 的元素都介于 (-1, +1)。\n计算 new value $\\widetilde{c}_t$，也需要一个单独的参数矩阵矩阵 $w_c$。\n🚂 更新 传输带 我们已经算出了遗忘门 $f_t$，输入门 $i_t$，以及新的输入值 $\\widetilde{c}_t$，我们还知道传送带旧的值 $c_{t-1}$，现在可以更新传送带 $c$ 了。\n1️⃣ 计算遗忘门 $f_t$ 和传送带旧的值 $c_{t-1}$ 的元素积。\n遗忘门 $f_t$，和传送带 $c_{t-1}$ 是维度相同的向量，算出的乘积也是个向量。遗忘门 $f_t$，可以选择性的遗忘 $c_{t-1}$ 中的一些元素，如果 $f_t$ 中的一个元素是 $0$，那么 $c_{t-1}$ 相应的元素就会被遗忘。\n上一步通过 🚪 遗忘门 选择性删除掉了传送带 $c_{t-1}$ 的一些元素，现在要往传送带上添加新的信息。\n2️⃣ 计算输入门 $i_t$，和新的输入值 $\\widetilde{c}_t$ 的元素积。\n输入门 $i_t$ 和新的值 $\\widetilde{c}_t$ 都是维度相同的向量，他们的乘积也是维度相同的向量，把乘积加到传送带上，这样就完成了对传送带的一轮更新。\n用遗忘门删除了传送带上的一些信息，然后用遗忘门输入加入新的信息，得到了传送带新的值 $c_t$，到现在，已经更新完传送带 $c$ 。\n🚪 Output Gate 最后一步是计算 LSTM 的输出，也就是状态向量 $h_t$。\n$h_t$ 是这么计算的：首先计算输出门 $o_t$，输出门 $o_t$ 跟前面的遗忘门，输入门的计算基本一样。\n把旧的状态 $h_{t-1}$，与新的输入 $x_t$ 做拼接，得到更高维的向量，然后算矩阵 $W_o$ 与这个向量的乘积，得到一个向量，最后使用激活函数 sigmod 得到向量 $o_t$。$o_t$ 的每一个元素都介于 (0, 1)，输出门也有自己的参数向量 $W_o$，$W_o$ 也需要从训练数据中学习。\n现在计算状态向量 $h_t$，对传送带 $c_t$ 的每一个元素求双曲正切tanh，把元素全都压到 (-1, +1) 区间。\n然后，求这两个向量的元素积，这个红色向量是刚刚求出的输出门 $o_t$，这样就得到了状态向量 $h_t$。\n看一下结构图，$h_t$ 他有两份 copys，$h_t$ 的一份 copy 传输到了下一步，另一份 copy 成了 LSTM 的输出。\n到第 t 步为止，一共有 t 个向量 $x$ 被输入了 LSTM，我们可以认为所有这些 $x$ 向量的信息，都积累在了状态 $h_t$ 里面。\n🧮 LSTM 的参数数量 我们来算一下 LSTM 的参数数量，LSTM 有 ❶ 遗忘门；❷ 输入门；❸ 新的输入；❹ 输出门。\n这四个模块都有各自的参数矩阵 $w$，所以一共有 4 个参数矩阵，矩阵的行数是：$shape(h)$，列数是： $shape(h)+shape(x)$\n所以，LSTM 参数的数量是：\n$4 * shape(h) * [ shape(h) + shape(x)]$\n🛠 实现 LSTM Doing\n🎐 总结 总结一下这一节的内容，这节介绍了 LSTM 模型和用 PyTorch 的实现。\nLSTM 和 simple RNN 主要的区别，是用了一条传送带，让过去的信息可以很容易传输到下一时刻，这样就有了更长的记忆。\nLSTM 的表现总是比 simple RNN 要好，所以当我们想使用 RNN 的时候就用 🙋‍♂️ LSTM 模型，而不要用 🙅‍♂️ simple RNN 模型。\nLSTM 有四个组件，分别是：\n🚪 Forget Gate 遗忘门 🚪 Input Gate 输入门 🆕 New Value 新的输入 🚪 Output Gate 输出门 这四个组件各自有一个参数矩阵，所以一共有四个参数矩阵，LSTM 参数的数量是：\n$4 * shape(h) * [ shape(h) + shape(x)]$\n下一节将介绍：\nstacked RNN bi-directional RNN 预训练 ⛓ 参考 🔗 https://github.com/wangshusen/DeepLearning/blob/master/Slides/9_RNN_3.pdf 🔗 https://colah.github.io/posts/2015-08-Understanding-LSTMs/ 🔗 https://www.youtube.com/watch?v=vTouAvxlphc 🔗 https://www.bilibili.com/video/BV1UK4y1d7xa ","date":"2022-03-19T20:36:14+08:00","image":"https://nlp.letout.cn/img/banner.png","permalink":"https://emerywan.github.io/blog/p/nlp-in-action/long-short-term-memory/","title":"Long Shorter Term Memory"},{"content":"本节主要内容为 文本处理 Text Processing 和 词嵌入 Word Embedding。本节和下面两节内容都会使用 IMDb 电影评论的数据，用来搭建机器学习模型分析电影评论。\n🌱 IMDb IMDb 是最有名的电影评论网站，用户可以在 IMDb 上给电影打分，1 分是非常讨厌，10 分是非常喜欢，如果乐意，还可以写一段电影评论。\n如果不给你看分数，只给你看评论，你大概能猜到用户打的分数，但你的猜测可能不太准确。如果换种方式，让你判断电影评论是 正面 positive 的还是 负面 negative 的，你应该会有很高的准确率。有人从 IMDb 上爬了 5 万条电影评论，这些评论都是很极端的，都是强烈的喜欢，或者强烈反感。这个二分类问题对于人来说很简单，人读一下电影评论就能轻易知道这是正面评价还是负面评价，人应该能有 100% 的准确率，这个数据集被分成两半，2 万 5 千条作为训练数据 Train，另外 2 万 5 千条作为测试数据 Test。\n你可以在下面的链接中获取到数据集：https://ai.stanford.edu/~amaas/data/sentiment/\n🔖 文本处理文本处理 在词嵌入 Word Embedding 和搭建机器学习模型之前，首先要进行文本处理，将文本变成序列 Sequence，文本处理很无聊，但我们应该重视它，文本处理的好坏，会直接影响机器学习模型的准确率。\n🚀 Tokenization 文本处理的第一步是 Tokenization，把文本分割成很多 tokens，这里我们把文本分割成很多单词，一个 token 就是一个单词（假如你把文本分割成字符，那么一个 token 就是一个字符），做完 Tokenization，一个很长的字符串就被分割成一个很多单词的列表。\nTokenization 看起来很简单，但是讲究很多。比如：\n🌰 是否应该把大写变成小写？ 通常情况下应该把大写变成小写，大小写单词通常是一个意思；但有时候会混淆单词（Apple -\u0026gt; apple），比如 Apple 是苹果公司，apple 是水果，大小写的 apple 并不是相同的单词。\n🌰 去除停用词。stop word 有些应用会去除 stop word，它是 the、a、of 等最高频的单词，这些词几乎出现在所有的句子里，对这个二分类问题几乎是没有帮助。\n🌰 拼写纠错。 用户发电影评论的时候，大部分情况下并不会仔细检查，所以写的东西难免会有拼写错误，所以做拼写纠错通常是有用的。\n这里只是举了几个例子，实际上做 Tokenization 的时候需要做大量的工作，Tokenization 看似简单，但实际上并不容易。\n🧰 Build Dictionary 第二步是建立一个字典。可以先统计词频，去掉一些低频词，让后让每个单词对应一个正整数，比如让 the -\u0026gt; 1; cat -\u0026gt; 2; sat -\u0026gt; 3。有了这个字典，就可以把每个单词对应到一个整数，这样一来，一句话就可以用正整数的列表表示，这个列表被称为序列 Sequences。\n如果有必要的话，还得进一步做 one-hot encoding，把单词表示成 one-hot 向量。\n在电影评论的例子里，数据是 5 万条电影评论，每条电影评论可以表示成一个字符串。做完 Tokenization 和 Encoding 后，每条电影评论都会转换成一个 Sequences，也就是一个正整数的列表。\n电影评论有长有短，有人只能写几个字的评论，有人能洋洋洒洒写几千字，所以得到的这些 Sequences 的长度也各不相同。比如这两条 Sequences 的长度分别是 52 和 90。\n这就造成了一个问题，训练数据没有对齐，每条 Sequences 都有不同的长度。做机器学习的时候，我们需要把数据存储到矩阵或者张量里，每条序列都得有相同的长度，需要把序列对齐。\n解决方案是这样的：我们可以固定长度为 $w$。假如一个序列长度太长，超过了 $w$ 个词，就砍掉前面的词，只保留最后面 $w$个词（当然保留最前面 $w$ 个词也同样可以）；假如一个序列太短，不到 $w$ 个词，那么就做 zero padding 用 0 来补齐，把长度增加到 $w$。\n这样一来，所有序列的长度都是 $w$，可以存储到一个矩阵里。\n🍀️ 词嵌入 文本处理已经完成了，现在每个词都用一个正整数来表示，下一步是 Word Embedding，把每个词都表示为一个一维向量。\n现在每个单词都用一个数字来表示，该怎么把这些 Categorical 特征表示为数值向量呢？\n显然可以做 one-hot encoding，用一个 one-hot 向量来表示一个单词。 比如 good: index = 2，于是使用标准正交积 $e_2$ 来表示，它的第二个元素是 1，其余元素都是 0，$e_2=[0, 1, 0, 0, \u0026hellip;, 0]$\n假如 vocabulary = v，也就是说字典里一共有 $v$ 个单词，那么就需要维度 dimension = v 的 one-hot 向量，要是字典里有 1 万个单词，那么这些 one-hot 向量都是 1 万维的，这样的向量维度是在太高了。下一节介绍 RNN 的时候你会看到，RNN 的参数数量正比于输入向量的维度，我们肯定不想让输入的向量是 1 万维的，否则一层 RNN 将会有好几十万个参数。所以我们要做 Word Embedding，把这些高维 one-hot 向量映射到低维向量。\n具体做法是吧 one-hot 向量 $e_i$ 乘到参数矩阵 $P^T$ 上，矩阵 $P^T$ 的大小是 $d*v$。其中 $d$ 是词向量的维度，由用户自己决定；$v$ 是 vocabulary，表示字典里单词的数量。\n矩阵的乘法的结果记做向量 $x_i$，$x_i$ 就是一个词向量，维度是 $d*1$，如果 one-hot 向量 $e$ 的第三个元素是 1，那么 $x_i$ 就是 $P^T$ 矩阵的第三列，可以看出，$P^T$ 矩阵每一列都是一个词向量。\n同理，下面这个参数矩阵 $P$ 的每一行都是一个词向量。这个矩阵的行数是 $v$，也就是 vocabulary；每一行对应一个单词，矩阵的列数是 $d$，$d$ 是用户决定的，$d$ 的大小会影响机器学习模型的表现，应该用 交叉验证 Cross Validation 用来选择一个比较好的 $d$。\n字典里的第一个词的是 movie，那么第一行就是 movie 的词向量；字典里的第二个词是 good，那么第二行就是 good 的词向量。\n我们的任务是判断电影评论是正面的还是负面的，这个参数矩阵是从训练数据中学习出来的，所以这些词向量都带有感情色彩，假如这些词向量都是二维的，我们就可以在平面坐标系中标出这些词向量。\nfantastic; good; fun 这些词向量都带有正面情感，所以这三个词的词向量学出来都比较接近；同理，poor; boring; mediocre 这些词带有负面情感，所以学出来的词同样也应该比较接近，但是这些词的词向量应该远离正面色彩的词向量。像 movie; is 这样的中性词，没有感情色彩，它们应该在中间。\n🎐 总结 最后总结一下这一章的内容。\n这一节上半部分，说明了文本处理是什么样的。给我们一条电影评论，首先做 Tokenization，把电影评论分割成很多单词，然后把很多单词编码成数字，这样一整条电影评论就可以很多正整数来表示，我们把这个正整数序列叫做 Sequences，就是神经网络中 Embedding 层的输入。由于电影评论的长短不一，得到的 Sequence 的长短也不一样，没办法存储在一个矩阵里，解决方案是 Alignment 对齐。假设最大长度为 20，如果长度大于20，就只保留最后 20 个单词；如果长度不到 20，就用 0 补齐，把长度增加到 20。这样一来，每个 Sequences 长度都相同。\n","date":"2022-03-11T21:14:45+08:00","image":"https://nlp.letout.cn/img/banner.png","permalink":"https://emerywan.github.io/blog/p/nlp-in-action/text-processing-and-word-embedding/","title":"文本处理与词嵌入"},{"content":"这一节我们来学习循环神经网络Recurrent Neural Networks。本节的内容是 Simple RNN，以及用 Pytorch 编程实现 Simple RNN。\n🌱 简介 现在 RNN 没有以前流行，尤其是在自然语言处理上，RNN 已经有一些过时了，如果训练的数据足够多，RNN 的效果不如 Transformer 模型，但是在小规模的问题上，RNN 还是很有用的。\n🔖 如何建模时序数据？ 机器学习中经常用到文本、语音等 时序数据sequential data（按时间顺序记录的数据列，有长度不固定的特点）。\n首先思考一个问题，怎么对这样的时序数据进行建模？ 在上一小节中，我们将一段文字整体输入到一个逻辑回归 Logistic Regression 模型中，让模型来做二分类，这属于一个 one-to-one 模型，一个输入对应一个输出。\n全连接神经网络和卷积神经网络都属于 one-to-one 模型。\n人脑并不会使用 one-to-one 模型处理时序数据，不会把一整段文字全部输入到大脑，我们在阅读的时候，会从左到右阅读一段文字，不断地在大脑里积累信息，阅读一段话之后，你脑子里就积累了一段文字的大意。\none-to-one 模型要求一个输入对应一个输出，比如：输入一张图片，输出每一类的概率值，one-to-one 的模型比较适合这类图片问题，但是不太适合文本问题。\n对于文本问题，输入和输出的长度并不固定，一段话可长可短，所以输入的长度并不固定；输出的长度也不固定，比如将英语翻译成汉语，一句英语有十个单词，翻译成汉语可能有十个字，可能有八个字，也可能是四个字的成语，输出汉语的字数并不固定，由于输入和输出的长度不固定，one-to-one 模型就不太适合了。\n对于时序数据，更好的是 many-to-one 或者是 many-to-many 模型，RNN 就是这样的模型，输入和输出的长度都不固定。所以 RNN 很适合语音，文本等时序序列数据。\n🍀️ RNN RNN 和跟人的阅读习惯很类似：人每次看一个词，会逐渐在大脑里积累信息；RNN 每看一个词，会用状态向量 $h$ 来积累阅读过的信息。\n首先，我们将输入的每个词用 词嵌入word embedding 变成一个词向量 $x$。\n每次把一个词向量输入 RNN，就会更新状态 $h$ ，把新的输入积累到状态 $h$ 里面。\n在 $h_0$中，包含了第一个词 the 的信息，在 $h_1$ 里面，包含了前两个 the cat 的信息；以此类推，状态 $h_2$ 包含 了前三个词 the cat sat 的信息，最后一个状态 $h_t$ 包含了整句话的信息，可以把 $h_t$ 看做 RNN 从整句话 the cat sat on the mat 抽取的特征向量，在更新状态 $h$ 的时候，需要用到参数矩阵 $A$。\n注意：整个 RNN 只有一个参数矩阵 $A$。无论这条链有多长，参数 $A$ 只有一个，$A$ 随机初始化，然后利用训练数据来学习 $A$。下面首先讲解 Simple RNN Model。\n🚀 Simple RNN 我们具体看看，Simple RNN 简单循环神经网络是怎么把输入的词向量 $x$，结合到状态 $h$ 中的。\n我们将上一个状态记做 $h_t-1$，新输入词向量记做 $x_t$，将这两个向量做拼接 concatenation，得到一个更高维的向量。\n图中这个矩阵 $A$ 是 RNN 的模型参数，这里计算矩阵 $A$ 和这个向量的乘积（拼接后的向量），矩阵和向量的乘积是一个向量，然后使用激活函数 tanh 作用在向量的每一个元素上，最后把激活函数的输出记做新的状态 $h_t$。\n这个激活函数式 双曲正切函数 hyperbolic tangent function，输入是任意实数，输出在 $(-1, +1)$ 之间。由于用了双曲正切激活函数，向量 $h_t$ 的每一个元素都在 $(-1, +1)$ 之间。\n这个神经网络的结构图可以这样理解：新的状态 $h_t$，是旧状态 $h_{t-1}$ 和新的输入 $x_t$ 的函数，神经网络模型的参数是 $A$：新的状态 $h_t$，依赖于向量 $h_{t-1}$, 向量 $x_t$ 以及矩阵 $A$。\n🎨 为什么需要使用 tanh 作为激活函数？ 我们思考这样一个问题：为什么需要使用 tanh 作为激活函数？能否将这个激活函数去掉，去掉之后会发生什么呢？\n首先我们做个简化，假设输入的词向量的元素都是 $0$。如图，这等同于输入的词向量 $x_t$ 都去掉，把矩阵 $A$ 右边一半也去掉。\n$x_0 = x_1 = \u0026hellip; = x_{100} = 0$\n这么一来，第 100 维的特征向量 $h_{100} = Ah_{99} = A^2h_{98} = \u0026hellip; = A^{100}h_0$。\n🌰 假设矩阵 $A$ 最大的特征值略小于 1 比如，最大的特征值等于 0.9。那么会发生什么呢？\n$0.9^{100}$ 非常接近于 0 了，所以矩阵 $A^{100}$ 非常接近于 0，那么新的特征向量 $h_{100}$ 也几乎也是一个全零的向量。\n🌰 假设矩阵 $A$ 最大的特征值略大于 1 比如，最大的特征值等于 1.2。\n$1.2^{100}=82817974.522$，所以矩阵 $A^{100}$ 的元素都超级大，$A^{100}$的每个元素都很大，假如循环的次数更多一些，或者 $A$ 的特征值再大一些，状态向量的值就会爆炸。\n假如没有这个激活函数 tanh，数值计算的时候很有可能会出问题，要么计算出的结果全部等于 0，要么爆炸了全部是 NaN: Not a Number。通过使用这个激活函数，每次更新状态 $h$ 后，都会做一个标准化操作 normalization，让 $h$ 恢复到 $(-1, +1)$ 这个合适的区间里。\n🏝️ Simple RNN 模型参数数量 我们来数一下 Simple RNN 有多少个模型参数。\n如图，先看一下这个拼接后向量，这个向量的维度是 $h_{t-1}$ 的维度加上 $x_t$ 的维度：\n所以 $A$ 一定要有 $shape(h)+shape(x)$ 维度这么多列：\n$A$ 的行数等于 $h$ 的维度：\n所以，最终矩阵 $A$ 的大小等于：\n$parameter(A) = shape(h) * [shape(h) + shape(x)]$\n这个乘积 $parameter(A)$ 就是 simple RNN 的最终的参数数量。\n我们来搭一个简单的网络。最底层是一个词嵌入层 Word Embedding Layer，它可以把词映射为词向量。\n词向量的维度由自己设置（这是一个超参数，我们应该使用交叉验证 cross validation 选择最佳的维度），这里设置 $x$ 的维度是 $32$。\n然后下一层是 Simple RNN Layer，输入的是词向量 $x_i$，输出的是状态 $h_i$。\n$h$ 的维度也是由自己设置，我们设置 $h$ 维度为 $32$。这里 $x$ 和 $h$ 的维度都是 $32$，这只是一个巧合而已，$h$ 和 $x$ 的维度通常不一样。\n前面说过，状态向量 $h$ 会积累输入的信息，比如：$h_0$ 包含第一个单词 I 的信息，$h_1$ 包含前两个词 I love 的信息，最后一个状态 $h_t$ 包含整句话 I love the movice so much 的信息。\n我们可以从 PyTorch 中获取所有的状态 $h={h_1, h_2, \u0026hellip;, h_t}$，也可以只获得最后一个状态向量 $h_t$ 的信息。\n$h_t$ 积累了整句话的信息，所有通常使用 $h_t$ 这一个向量就够了，这里我们只使用 $h_t$，把前面的所有状态 ${h_1, h_2, \u0026hellip;, h_{t-1}}$ 全部都丢掉。\n$h_t$ 相当于从文本中提取的特征向量，把 $h_t$ 输入这个分类器 $sigmoid(v^T h_t)$，分类器就会输出一个 0 或 1 之间的数值，0 代表了负面电影评价，1 代表正面电影评价。\n然后我们设置超参数：\n设置 vocabulary = 1000，意思是词典里有 10000 个词汇； embedding_dim = 32，意思是词向量 $x$ 的维度是 $32$； word_num = 500，意思是每条电影评价有 500 个单词，如果超过 500 个单词，就会被截掉，只保留 500 个，如果不够 500，就用 zero_padding 将句子补成长度为 500； state_dim = 32，意思是状态 $h$ 的维度等于 $32$。 👩‍🚒 PyTorch 实现 接下来开始搭网络，首先我们定义一个类 Model，我们让它继承 nn.Model 父类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import torch import torch.nn as nn class Model(nn.Module): def __init__(self, vocabulary_size, embedding_dim, state_dim): super(Model, self).__init__() self.vocabulary_size = vocabulary_size self.embedding_dim = embedding_dim self.state_dim = state_dim self.Embedding = nn.Embedding(self.vocabulary_size, self.embedding_dim) self.RNN = nn.RNN(self.embedding_dim, self.state_dim) self.fc = nn.Linear(self.state_dim, 1) self.sigmoid = nn.Sigmoid() def forward(self, x): x = x.transpose(0, 1) embedded = self.Embedding(x) output, h_n = self.RNN(embedded) h_n = torch.squeeze(h_n, dim=0) result = self.fc(h_n) return self.sigmoid(result) 接下来，我们在构造函数 __init__ 中定义我们的模型结构，并重载 forword 方法。\n首先是词嵌入层 nn.Embedding()，它是把词映射成向量。\n然后是 simple RNN 层 nn.RNN()，需要指定词向量的维度 embedding_dim 和状态向量 $h$ 的维度 state_dim；\n最后是一个全连接层 nn.Linear()，并且会使用 nn.Sigmoid() 作用于它的结果，输入最后一个状态向量 $h$，输出一个 0、1 之间的数。\nPyTorch 中的 RNN 会有两个返回值：output，h_n。\noutput 是 RNN 所有时刻的状态向量集合（矩阵）； h_n 是 RNN 中最后一个状态向量。 这是模型的一个概要， 词嵌入层 Embedding 的输出是一个 $500*32$ 的矩阵，500 的意思是每个句子有 $500$ 个词，32 的意思是每个词用 $32$ 维的词向量表示。\n1 2 3 4 5 6 7 8 9 10 11 ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== Model -- -- ├─Embedding: 1-1 [500, 1, 32] 320,000 ├─RNN: 1-2 [500, 1, 32] 2,112 ├─Linear: 1-3 [1, 1] 33 ├─Sigmoid: 1-4 [1, 1] -- ========================================================================================== Total params: 322,145 Trainable params: 322,145 Simple RNN 每个状态的输出 $h_t$ 都是一个 32 维的向量。我们看一下 RNN 的参数，他有 2080 个参数，它是这样算出来的：\n$shape(h) * (shape(h)+shape(x)) = 32*(32+32)+32 = 2080$\n这是矩阵 $A$ 的大小，后面的 $32$ 的维度来自 intercept，也叫 bias，偏移量。RNN 会默认使用 intercept，但这个不重要，这里暂时不管它。\n剩下的 32 个参数来自于最后一个状态，因为 PyTorch 中的 RNN 会同时输出所有时刻的状态向量集合和最后一个状态向量。\n🧑‍🔧 运行模型 搭好模型之后，初始化模型，然后用训练数据拟合模型。\n我们指定算法是 optim.Adam，损失函数是 nn.BCELoss()，评价标准是准确率 accuracy。\n然后用训练数据拟合模型，我让算法训练 3 个 epochs，只让算法运行 3 个 epochs，是出现了过拟合，3 个 epochs 之后，validate accuracy 会变差。提前让算法停止运行称为 early stoping 提前终止训练，使 validate accuracy 变差之前就停止。\n最后，用测试数据评价模型的表现，把测试数据作为输入，调用 model.evaluate()，返回 loss 和 accuracy，测试的 accuracy，测试的 accuracy 是 84.36%，比上一节中的 逻辑回归好很多（75%）。\n刚才搭模型的时候，只使用了RNN最后一个状态 $h_t$，把之前的状态都丢掉了，想用 $h_0$ 到 $h_t$ 所有状态也可以，但并没有太大区别。\n我们让 RNN 的第一个输出 output，它是一个矩阵，矩阵每一行就是一个状态向量 $h$。\n如果用所有状态，就要加一个 flatten 层，将状态矩阵变成一个向量，然后把这个向量作为分类器的输入，来判断电影是正面的，还是负面的。只要把网络稍作改动就可以了。\n1 torch.flatten(output, start_dim=1, end_dim=2) 🧰 simple RNN 的缺陷 下面看一下 simple RNN 这种简单的模型有什么缺陷。\n举个栗子 🌰 ，现在有这样一个问题，给定半句话，要求预测下一个单词。\n输入是 clouds are in the，正确的输出应该是 sky，如果在大量文本中预测 RNN，应该是有能力做出这样的预测的。在这个例子里，RNN 只需要看最近的几个词，尤其是 clouds are，并不需要更多的上下文看的更远。\n这个例子是对 simple RNN 十分有利，simple RNN 特别擅长这种 short-term dependence，simple RNN 不擅长的是 long-term dependence。\nRNN 的状态 $h$，和之前所有的输入 $x$ 都有函数依赖关系，照理来说，如果改变输入的单词 $x_1$，所有的状态 $h$ 都会发生变化，但实际上，simple RNN 并没有这种性质，所以很不合理。如果把第 100 个状态向量 $h_{100}$，关于输入 $x_1$ 求导，你会发现导数几乎等于 0。\n$\\frac{\\partial h_{100}}{\\partial x_1} \\approx 0$\n导数几乎等于 0 说明什么呢？说明当我们改变 $x_1$时，$h_{100}$ 几乎不会发生任何变化，也就是说状态 $h_{100}$ 和 100 步之前的输入 $x_1$ 几乎没有关系，这显然不合理，说明状态 $h_{100}$ 几乎把很多步之前的输入都给忘记了，simple RNN 的这种遗忘会给后续操作造成很多问题。\n再举个栗子 🌰 ，这是很长的一段话，一开始是 I grow up in China when I was a child, ... ... 到了很多句话之后，有这样一句，I speak fluent ...。\n下一个词应该是 Chinese，我小时候在中国，所有会说流利的中文，然而 simple RNN 不太可能会做出 Chinese 这个正确的预测，因为 RNN 已经把前文给忘记了。simple RNN 擅长的是 short-term dependence，RNN 看到最近的单词是 speak fluent，所以 RNN 知道下一个单词可能是某种语言，可能是 Chinese、English、French、Japanese 等等，但正确答案是 Chinese，因为上文有 I grow up in china when i was child，simple RNN 就像金鱼一样记忆力只有 7 秒，RNN 根本就不记得上文有这句话，所以 I speak fluent ... 预测单词可能是 English , French 等任何一种语言，未必是 Chinese。\n🎐 总结 最后总结一下这一节的内容：\nRNN 是一种神经网络，但是他的结构不同于全连接网络和卷积网络，RNN 适用于文本，语音等时序序列数据，RNN 按照顺序读取每一个词向量，并且在状态向量 $h$ 中积累看到过得信息，$h_0$ 中包含了 $x_0$ 的信息，$h_1$ 中包含了 $x_0$ 和 $x_1$ 的信息，$h_t$ 中积累了之前所有 $x={x_0, x_1, \u0026hellip;, x_t}$ 的信息。\n有一种错误的看法是 $h_t$ 中只包含了 $x_t$ 的信息，这是不对的，$h_t$ 中包含了之前所有输入的信息，可以认为 $h_t$ 代表了 RNN 从整个序列中抽取的特征向量，所有我们只需要 $h_t$ 就可以判断电影评价是正面的还是负面的。\nsimple RNN 有一个参数矩阵 $A$，它可能还会一个 intercept 参数向量 $b$，上面的介绍中忽略了这个参数向量 $b$，这个参数矩阵 $A$ 的维度是：\n$shape(h) * [shape(h) + shape(x)]$\n参数矩阵 $A$ 一开始随机初始化，然后从训练数据上学习。注意：simple RNN 只有一个参数矩阵，不管这个序列有多长，参数矩阵只有一个，所有模块里的参数都是一样的。\nRNN 有一个缺点，RNN 的记忆比较短，会遗忘很久之前的输入 $x$，如果这个时间序列很长，有好几十步，最终 RNN 就会忘记了之前的输入。下一节将介绍 LSTM，LSTM 的记忆会比 simple RNN 长很多，但是 RNN 也还是会有遗忘的问题。\n","date":"2022-03-08T20:36:14+08:00","image":"https://nlp.letout.cn/img/banner.png","permalink":"https://emerywan.github.io/blog/p/nlp-in-action/simple-rnn/","title":"RNN"},{"content":"🌱 类别特征 机器学习的数据通常有 类别特征 Categorical Features ，我们需要把类别特征 Categorical Features 转化成机器学习模型能理解的数值特征，下面使用一个例子来具体讲解类别特征数据的处理。\n这张表的每一行是一个人的数据，包括：年龄、性别、国籍，我们需要把这些数据变成机器学习模型可以理解的数值特征。\n表格的第一列是年龄，年龄本身就是数值特征，所以可以不用做处理，数值特征的特点是可以比较大小，比如 35 岁的人比 31 岁的年龄大。\n第二列是性别，性别是二元特征，我们可以用一个数来表示性别。用 0 表示女性，用 1 表示男性。这样一来，性别就表示为一个标量：0 / 1。\n第三列是国籍，比如中国，美国，印度。国籍是类别特征，机器学习并不理解国籍，所以我们要把国籍编码成数值向量。世界上约有 197 个国家，我们先用一个 [1 - 197] 的整数表示一个国家。可以建立一个字典，把国籍映射成一个 [1 - 197] 的整数。比如：China:1; US:2; India:3; Japan:4; Germany:5。\n我们要从 1 开始计算，而不能从 0 开始计算。\n做这种映射，国籍就表示成 [1 - 197] 之间的整数。仅仅把国籍表示成 [1 - 197] 的整数还是不行，一个整数只是一种类别，它们之间不能比较大小。US:2; India:3 这个数字并不表示印度大于美国，这些整数只是类别而已，并不是真正的数值特征。\n所以要进一步对国籍做 one-hot encoding ，用 one-hot 向量来表示国籍：\n1 2 China -\u0026gt; 1 -\u0026gt; [1,0,0,0,...,0] US -\u0026gt; 2 -\u0026gt; [0,1,0,0,...,0] 比如，中国对应 1，所以用 197 维的 one-hot 向量 [1,0,0,0...,0] 来表示，其中第一个元素为 1，其余元素都是 0；美国对应 2，这个 197 维的向量 [0,1,0,0...,0] 第二个元素是 1，其余元素都是 0。这样一来，每个国籍就由一个 one-hot 向量表示，一共有 197 个国家，所以每个向量都是 197 维的。\n我们要从 1 开始计算，而不能从 0 开始计算。 因为我们要把 0 保留，用来表示未知或者缺失的国籍。数据库里面经常会有缺失的数据（比如用户没有填写国籍），这样缺失的国籍就用 0 来表示，它的 one-hot 向量就是一个全 0 的向量[0,0,0,0...,0]。\n下面这个例子中，我们用一个 199 维表示一个人的特征。比如这个人 28 岁，女性，国籍是中国。\n其中，一个维度表示年龄，一个维度表示性别，一个 197 维的 one-hot 向量表示国籍。\n这个例子里，这个 36 岁，男性，国籍未知的人的特征是这个 199 维的向量，我们用一个 197 维的全 0 向量表示未知国籍。\n🔖 为什么要用 one-hot 向量表示特征 在处理类别特征的时候，我们使用 one-hot 向量表示国籍，每个国籍都用 197 维的向量表示。为什么要用 one-hot 向量而不用一个数字表示呢？比如用 1 表示中国，2 表示美国，3 表示印度。这样一来，名字就变成了数字，可以做数值计算，而且用一个数字表示的话，可以节省 197 倍的存储空间。当然这是不行的。否则我们就不需要 one-hot encoding 了。\n假设我们使用 1 -\u0026gt; China; 2 -\u0026gt; US; 3 -\u0026gt; India。那么将中国 1 和美国 2 的特征加起来：1+2=3 ，相当于 “中国 + 美国 = 印度”。这样的特征完全不合理。\n使用 one-hot 特征向量更合理。将 China 和 US 的 one-hot 向量加起来，得到 [1,1,0,0,...,0]，第一个和第二个元素都是 1，其余元素都是 0，这个特征向量的解释是：既有中国国籍，又有美国国籍。\n所以做机器学习的时候，不能用一个标量来表示一个类别特征，这种特征做法求和等数值计算是没有意义的。正确的做法是使用 one-hot 向量来表示类别特征。\n🚀 处理文本数据的流程 在自然语言处理的应用中，数据就是文本 document，文本可以分割成很多单词，我们需要把单词表示成数值向量。其中每个单词都是一个类别，如果字典里有一万个单词，那么就有一万的类别，显然单词就是类别特征。我们需要使用处理类别特征的方法，把单词变成数值向量。\n文本处理主要分为三个步骤：\n🔔 把文本分割成单词 🔔 计算每个单词出现的次数 🔔 进行 one-hot 编码 文本处理的第一步是把文本分割成单词。一段话，一篇文章或者一本书可以表示为一个字符串，可以把文本分割成很多单词，这个步骤称为 Tokenization。\n比如说这句话 ... to be or not to be ...， 可以分割成这些单词 [to, be, or, not, to, be]。Tokenization 就是把文本变成单词的列表。\n文本处理的第二步是计算词频，也就是每个单词出现的次数。我们可以用一个哈希表 hash Map 来计算，计算开始之前，哈希表是空的，我们根据以下方式更新哈希表：如果单词 w 不在表里面，说明到目前为止，w 还没有出现在文本里，所以我们要把 w 加入哈希表，并让它的词频等于 1；如果 w 在哈希表里面，说明 w 之前在文本里出现过，只需要把 w 的词频加 1 即可。\n接下来举个例子，我们将挨个处理这个列表里的单词。当处理到单词 to 的时候，首先查一下哈希表，发现哈希表里面有 to，它的词频是 398，说明 to 在文章里已经出现过 398 次了，现在这个单词又出现了一次，于是把表里的词频加 1，变成了 399；当处理到单词 or的时候，在表里找不到，这说明文章里还没有出现过 or 这个单词，第一次出现在文章里，于是我们把 or 插入表里，将词频设置为 1。\n完成统计词频之后，需要把哈希表做一个排序，按照词频递减的顺序进行排列，表的最前面是词频最高的，表最后是词频最低的。然后就把词频换成下标 index，从 1 开始数计数，词频最高的词的 index 是 1。这个例子里，一共有 8 个单词，每个词对应一个 [1, 8] 之间的正整数。这个表称为字典 ，可以把单词映射为一个数字。\n字典里单词的个数称为词汇量 vocabulary。这例子里词汇量等于 8。\n英语里大概有 1 万个常用词，但是统计词频之后，你会发现字典会有几十万甚至上百万个单词。统计词频的目的就是保留常用词，去掉低频词。比如，我们可以保留词频最高的 1 万个单词，删掉其余单词。\n为什么要删掉低频词呢？\n🌰 低频词通常没有意义 很多低频词都是名字实体 name entities，比如我们的名字就是个名字实体，假如我们的名字出现在一个数据集里面，他的频率肯定会很低，在大多数的应用里名字实体没有意义。\n低频词很多都是拼写错误造成的，如果把 prince 的 c 误写成 s，prinse，那么就创造了一个新的单词，这种词的频率也很低，在很多应用里，去掉这种词没有危害。\n🌰 去掉低频词的另一个原因是我们不希望 vocabulary 太大。 下一个步骤做 one-hot encoding 的时候，向量的维度就是字典的大小。字典越大，向量的维度就越高，这会让计算变慢。下一节详细说明词嵌入 Word Embedding 的时候就会看到，字典越大，模型的参数就越会越多，就会容易造成过拟合 overfitting，删掉低频词就会大幅减小 vocabulary。\n文本处理的第三步就是对单词做 one-hot encoding，通过查字典，把单词映射成一个正整数，一个单词的列表就映射成了一个正整数的列表；如果有必要就继续把这些正整数变成 one-hot 向量。这些 one-hot 向量的维度正好等于 vocabulary，在这个例子里面，字典的长度是 8，所以 one-hot 维度就等于 8。\n上面说过，字典里的低频词可能会被删掉，所以有些词在字典里找不到，例如把 be 错误拼写成单词 bi，这个词在字典里找不到，one-hot encoding 时，可以忽略这个词，也可以把它编码成全 0 向量。\n🎐 总结 最后总结一下这一节的内容。\n部分机器学习的数据会具备类别特征 Categorical Features，机器学习模型无法理解，我们需要将其转换成数值特征。类别特征的类别会被映射成一个从 1 开始计算的整数，0 被用来表示缺失或者未知的类别，并且使用 one-hot 向量，能很好的表示类别特征的意义。\n文本处理主要有三个步骤，第一步 tokenization 把文本分割成单词的列表；第二步建立了一个字典vocabulary，把单词映射成一个正整数；第三步进行 one-hot encoding，将分割后的单词列表映射成正整数的列表或变成 one-hot 向量。\n","date":"2022-03-01T02:02:02+08:00","image":"https://nlp.letout.cn/img/banner.png","permalink":"https://emerywan.github.io/blog/p/nlp-in-action/data-processing/","title":"数据处理基础"},{"content":"做自然语言处理写了挺久的 Python ，经常要处理数据。用 Python 中的处理数据真的挺爽的 🫣，我平常都喜欢把各种数据都往 dict（也就是 Java 中的 map） 和 json 上转，用 Python 处理这个各种方便。 最近再写 Java 就经常“手残”，所以总结一下 Java 中处理 Map。\n⭐️ 遍历方式 Java 遍历 Map，要么就是遍历它的 Map.Entry\u0026lt;K,V\u0026gt; \u0026lt;- entrySet()，要么就是遍历它的 key KeySet \u0026lt;- keySet()。主要可以用下面四种方式遍历：\nIterator\nForEach\nLambda\nStream\n除非特殊需要，尽量使用后 3 种遍历方式，使用 Iterator 可能更容易造成一些错误（Effective Java - 58：for-each 优先于 for 循环）。\n在一些 for-each 不能胜任的地方，其实也有很对内置方法能够完成操作，不仅更加直观，而且非常易用。比如删除操作：Collection#removeIf() Java 1.8+。\nForEach EntrySet 1 2 3 4 for (Map.Entry\u0026lt;String, String\u0026gt; entry : map.entrySet()) { String key = entry.getKey(); String value = entry.getValue(); } KeySet 1 2 3 for (String key : map.keySet()) { String value = map.get(key); } Lambda 1 2 3 4 map.forEach((key, value) -\u0026gt; { // key // value }) 1 2 3 4 5 public class HashMap\u0026lt;K,V\u0026gt; extends AbstractMap\u0026lt;K,V\u0026gt; implements Map\u0026lt;K,V\u0026gt;, Cloneable, Serializable { @Override public void forEach(Biconsumer\u0026lt;? super K, ? super V\u0026gt; action) { } } Stream Steram 1 2 3 4 map.entrySet().stream().forEach((entry) -\u0026gt; { String key = entry.getKey(); String value = entry.getValue(); }) parallelStream 1 2 3 4 map.entrySet().parallelStream().forEach((entry) -\u0026gt; { String key = entry.getKey(); String value = entry.getValue(); }) Interator EntrySet 1 2 3 4 5 6 7 Iterator\u0026lt;Map.Entry\u0026lt;String, String\u0026gt;\u0026gt; iterator = map.entrySet().iterator(); while(iterator.hasNext()) { Map.Entry\u0026lt;String, String\u0026gt; entry = iterator.next(); String key = entry.getKey(); String value = entry.getValue(); } 根据 Effective Java - 57，更推荐以下这种写法：\n1 2 3 4 5 for (Iterator\u0026lt;Map.Entry\u0026lt;String, String\u0026gt;\u0026gt; iterator = map.entrySet().iterator(); iterator.hasNext(); ) { Map.Entry\u0026lt;String, String\u0026gt; entry = iterator.next(); String key = entry.getKey(); String value = entry.getValue(); } KeySet 1 2 3 4 for (Iteator\u0026lt;String\u0026gt; iterator = map.keySet().iterator(); iterator.hasNext(); ) { String key = iterator.next(); String value = map.get(key); } 🌟 一些操作 判空 1 2 3 4 5 🙋‍♂️ map.isEmpty(); 🙅‍♂️ boolean b = map.size() == 0; 计数 / merge() 1 2 public V merge(K key, V value, BiFunction\u0026lt;? super V, ? super V, ? extends V\u0026gt; remappingFunction) { } 1 map.merge(key, 1, Integer::sum); removeIf() 🚧 注意：\nMap 本身是没有 removeIf()。\n1 2 3 map.entrySet().removeIf(entry -\u0026gt; {}); map.keySet().removeIf(key -\u0026gt; {}); map.values().removeIf(value -\u0026gt; {}); absent 1 public V putIfAbsent(K key, V value); 1 2 3 4 5 6 7 // 不存在 key 时，按 mappingFunction 添加 value // 存在 key 不改变 public V computeIfAbsent(K key, Function\u0026lt;? super K, ? extends V\u0026gt; mappingFunction) { } public V computeIfPresent(K key, BiFunction\u0026lt;? super K, ? super V, ? extends V\u0026gt; remappingFunction) { } default 1 map.getOrDefault(key, 0); 🔗 参考 https://mp.weixin.qq.com/s/zQBN3UvJDhRTKP6SzcZFKw ","date":"2021-10-28T21:14:45+08:00","image":"https://emerywan.github.io/blog/imgs/11.jpeg","permalink":"https://emerywan.github.io/blog/p/java-map-traversal/","title":"Java Map 操作"},{"content":"现有NER模型缺点：\n基于序列标注的NER模型（The sequence labeling-based NERmodels）\n长实体识别不佳，只关注词级信息 基于分段的NER模型（The segment-based NERmodels）\n处理分段，而非单个词，不能捕获分段中的词级依赖关系 边界检测（boundary detection）和类型预测（type pre-diction）可以相互配合，两个子任务可共享信息，相互加强。\n提出模块化交互网络模型MIN（Modularized Interaction Network）\n利用段级信息和词级依赖关系，结合一种交互机制，支持边界检测和类型预测之间的信息共享。\na recurrentneural network encoder-decoder framework with apointer network is used to detect entity segmentsfor segment information.\n","date":"2021-09-01T21:14:45+08:00","image":"https://emerywan.github.io/blog/imgs/9.jpeg","permalink":"https://emerywan.github.io/blog/p/modularizedinteractionnetworkfornamed-entityrecognition/","title":"Modularized Interaction Network for Named Entity Recognition"},{"content":"借鉴图像领域的目标检测任务，将嵌套任务转化成span的预测，解决嵌套命名实体识别任务。\n具体是采用两阶段：\n第一步 Locate，即定位实体的边界 第二步 Label，即对识别span进行实体类型判断。 ","date":"2021-06-07T21:14:45+08:00","image":"https://emerywan.github.io/blog/imgs/14.jpeg","permalink":"https://emerywan.github.io/blog/p/sequence-to-set/","title":"A Sequence-to-Set Network for Nested Named Entity Recognition"},{"content":"文中将每个 token 用 BERT 和 fastText 进行 embedding，拼接上用 CNN 编码的字符级别向量，送入一个双向 LSTM 编码上下文信息，获得每个词的表达。之后采用两个独立的 FFNN 来得到作为实体开始和结束的位置的表达。\n最后用一个仿射模型得到一个 $l×l×c$ 的 tensor，其中 l 是句子长度，c 是实体类别数量加一（表示无实体）。运用该矩阵，文中对于嵌套和非嵌套采用了两种不同的策略，从而得到了实体的起止位置。\ncode: https://github.com/juntaoy/biaffine-ner\n","date":"2021-06-01T21:14:45+08:00","image":"https://emerywan.github.io/blog/imgs/7.jpeg","permalink":"https://emerywan.github.io/blog/p/namedentityrecognitionasdependencyparsing/","title":"Named Entity Recognition As Dependency Parsing"},{"content":"PyTorch 是什么？ 基于 Python 的科学计算包，服务于以下两种场景：\nNumpy 的替代品，可以使用 GPU 的强大计算力 提供最大的灵活性和高速的深度学习研究平台 Tensors Tensors 与 Numpy 中的 ndarrays 类似，但是在 PyTorch 中 Tensors 可以使用 GPU 进行计算。\n1 2 from __future__ import print_function import torch [ 1 ] 创建一个 5x3 的矩阵，但不初始化：\n1 2 3 4 5 6 7 x = torch.empty(5, 3) print(x) # tensor([[0.0000, 0.0000, 0.0000], # [0.0000, 0.0000, 0.0000], # [0.0000, 0.0000, 0.0000], # [0.0000, 0.0000, 0.0000], # [0.0000, 0.0000, 0.0000]]) [ 2 ] 创建一个随机初始化的矩：\n1 2 3 4 5 6 7 x = torch.rand(5, 3) print(x) # tensor([[0.6972, 0.0231, 0.3087], # [0.2083, 0.6141, 0.6896], # [0.7228, 0.9715, 0.5304], # [0.7727, 0.1621, 0.9777], # [0.6526, 0.6170, 0.2605]]) [ 3 ] 创建一个 0 填充的矩阵，数据类型为 long：\n1 2 3 4 5 6 7 x = torch.zero(5, 3, dtype=torch.long) print(x) # tensor([[0, 0, 0], # [0, 0, 0], # [0, 0, 0], # [0, 0, 0], # [0, 0, 0]]) [ 4 ] 创建一个 tensor 使用现有数据初始化：\n1 2 3 4 5 x = torch.tensor( [5.5, 3] ) print(x) # tensor([5.5000, 3.0000]) [ 5 ] 根据现有的 tensor 创建 tensor。这些方法将重用输入 tensor 的属性（如：dtype，除非设置新的值进行覆盖）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 利用 new_* 方法创建对象 x = x.new_ones(5, 3, dtype=torch.double) print(x) # tensor([[1., 1., 1.], # [1., 1., 1.], # [1., 1., 1.], # [1., 1., 1.], # [1., 1., 1.]], dtype=torch.float64) # 覆盖 dtype # 对象 size 相同，只是 值和类型 发生变化 print(x) # tensor([[ 0.5691, -2.0126, -0.4064], # [-0.0863, 0.4692, -1.1209], # [-1.1177, -0.5764, -0.5363], # [-0.4390, 0.6688, 0.0889], # [ 1.3334, -1.1600, 1.8457]]) [ 6 ] 获取 size：\n1 2 print(x.size()) # torch.Size([5, 3]) Tip:\ntorch.Size 返回 tuple 类型，支持 tuple 类型所有的操作。\n[ 7 ] 操作\n[ 7.1 ] 加法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 y = torch.rand(5, 3) print(x+y) # tensor([[ 0.7808, -1.4388, 0.3151], # [-0.0076, 1.0716, -0.8465], # [-0.8175, 0.3625, -0.2005], # [ 0.2435, 0.8512, 0.7142], # [ 1.4737, -0.8545, 2.4833]]) print(torch.add(x, y)) # tensor([[ 0.7808, -1.4388, 0.3151], # [-0.0076, 1.0716, -0.8465], # [-0.8175, 0.3625, -0.2005], # [ 0.2435, 0.8512, 0.7142], # [ 1.4737, -0.8545, 2.4833]]) 提供输出 tensor 作为参数：\n1 2 3 4 5 6 7 8 result = torch.empty(5, 3) torch.add(x, y, out=result) print(result) # tensor([[ 0.7808, -1.4388, 0.3151], # [-0.0076, 1.0716, -0.8465], # [-0.8175, 0.3625, -0.2005], # [ 0.2435, 0.8512, 0.7142], # [ 1.4737, -0.8545, 2.4833]]) [ 7.2 ] 替换：\n1 2 3 4 5 6 7 8 # add x to y y.add_(x) print(y) # tensor([[ 0.7808, -1.4388, 0.3151], # [-0.0076, 1.0716, -0.8465], # [-0.8175, 0.3625, -0.2005], # [ 0.2435, 0.8512, 0.7142], # [ 1.4737, -0.8545, 2.4833]]) {% note info %} _ 结尾的操作会替换原变量。 如：x_copy_(y)，x.t_() 会改变 x {% endnote %}\n[ 7.3 ] 使用 Numpy 中索引方式，对 tensor 进行操作：\n1 2 print(x[:, 1]) # tensor([-2.0126, 0.4692, -0.5764, 0.6688, -1.1600]) [ 8 ] torch.view 改变 tensor 的维度和大小 （与 Numpy 中 reshape 类似）：\n1 2 3 4 5 6 x = torch.randn(4, 4) y = x.view(16) z = x.view(-1, 8) # -1 从其他维度推断 print(x.size(), y.size(), z.size()) # torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) [ 9 ] 如果只有一个元素的 tensor，使用 item() 获取 Python 数据类型的数值：\n1 2 3 4 5 x = torch.randn(1) print(x) print(x.item()) # tensor([-0.2368]) # -0.23680149018764496 Numpy 转换 Torch Tensor 与 Numpy 数组之间进行转换非常轻松。\n1 2 3 4 5 a = torch.ones(5) b = a.numpy() print(a) # tensor([1., 1., 1., 1., 1.]) print(b) # [1. 1. 1. 1. 1.] Torch Tensor 与 Numpy 数组共享底层内存地址，修改一个会导致另一个的变化。\n1 2 3 4 a.add_(1) print(a) # tensor([2., 2., 2., 2., 2.]) print(b) # [2. 2. 2. 2. 2.] 1 2 3 4 5 6 7 import numpy as np a = np.ones(5) b = torch.from_numpy(a) np.add(a, 1, out=a) print(a) # [2. 2. 2. 2. 2.] print(b) # tensor([2., 2., 2., 2., 2.], dtype=torch.float64) Tip:\n所有的 Tensor 类型默认都是基于 CPU， CharTensor 类型不支持到 Numpy 的装换。\nCUDA 张量 使用 .to() 可以将 Tensor 移动到任何设备中。\n1 2 3 4 5 6 7 if torch.cuda.is_available(): device = torch.device(\u0026#39;cuda\u0026#39;) # CUDA 设备对象 y = torch.ones_like(x, device=device) # 直接从 GPU 创建张量 x = x.to(device) z = x + y print(z) # tensor([0.7632], device=\u0026#39;cuda:0\u0026#39;) print(z.to(\u0026#39;cpu\u0026#39;, torch.double)) # tensor([0.7632], dtype=torch.float64) Autograd 自动求导 autograd 包为 Tensor 上所有的操作提供了自动求导。它是一个运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。\n正向传播 反向传播 神经网络（NN）是在某些输入数据上执行嵌套函数的集合。 这些函数由参数（权重和偏差组成）定义，参数在 PyTorch 中存储在张量中。\n训练 NN 分为两个步骤：\n正向传播：在正向传播中，NN 对正确的输出进行最佳猜测。它通过每个函数运行输入数据以进行猜测。 反向传播：在反向传播中，NN 根据其猜测中的误差调整其参数。它通过从输出向后遍历，收集有关参数（梯度）的误差导数并使用梯度下降来优化参数来实现。 [ 1 ] 我们从 torchvision 加载了经过预训练的 resnet18 模型。创建一个随机数据张量来表示具有 3 个通道的单个图像，高度和宽度为 64，其对应的label初始化为一些随机值。\n1 2 3 4 5 import torch, torchvision model = torchvision.models.resnet18(pretrained=True) data = torch.rand(1, 3, 64, 64) labels = torch.rand(1, 1000) [ 2 ] 接下来，通过模型的每一层运行输入数据进行预测。正向传播。\n1 prediction = model(data) [ 3 ] 使用模型的预测（predication）和相应的标签（labels）来计算误差（loss）。 下一步通过反向传播此误差。我们在 loss tensor 上调用 .backward() 时，开始反向传播。Autograd 会为每个模型参数计算梯度并将其存储在参数 .grad 属性中。\n1 2 loss = (prediction - labels).sum() loss.backword() # backword pass [ 4 ] 接下来，我们加载一个优化器（SDG），学习率为 0.01，动量为 0.9。在 optim 中注册模型的所有参数。\n1 optim = torch.optim.SDG(model.parameters(), lr=1e-2, momentum=0.9) [ 5 ] 最后，调用 .step() 启动梯度下降。优化器通过 .grad 中存储的梯度来调整每个参数。\n1 optim.step() # gradient descent 神经网络的微分 这一小节，我们将看看 autograd 如何收集梯度。 我们在创建 Tensor 时，使用 requires_grad=True 参数，表示将跟踪 Tensor 的所有操作。\n1 2 3 4 5 6 7 import torch a = torch.tensor([2., 3.], require_grad=True) b = torch.tensor([6., 4.], require_grad=True) # 从 tensor a, b 创建另一个 tensor Q Q = 3*a**3 - b**2 假设 tensor a，b 是神经网络的参数，tensor Q 是误差。在 NN 训练中，我们想要获得相对于参数的误差，即各自对应的偏导：\n$$\\frac{\\partial Q}{\\partial a}=9a^2$$\n当我们在 tensor Q 上调用 .backward() 时，Autograd 将计算这些梯度并将其存储在各个张量的 .grad 属性中。\n我们需要在 Q.backword() 中显式传递 gradient 参数（与 Q 形状相同的张量，表示 Q 相对本身的梯度）。\n$$\\frac{\\partial Q}{\\partial b}=-2b$$\n1 2 3 4 5 6 7 8 external_grad = torch.tensor([1., 1.]) Q.backward(gradient=external_grad) # 最后，梯度记录在 a.grad b.grad 中，查看收集的梯度是否正确 print(9*a**2 == a.grad) # tensor([True, True]) print(-2*b == b.grad) # tensor([True, True]) 我们也可以将 Q 聚合为一个标量，然后隐式地向后调用，如：Q.sum().backward()。\n神经网络 上一节，我们了解到 nn 包依赖 autograd 包来定义模型并求导。下面，我们将了解如何定义一个网络。一个 nn.Module 包含个 layer 和一个 forward(input) 方法，该方法返回 output。\n如下，这是一个对手写数字图像进行分类的卷积神经网络：\n神经网络的典型训练过程如下：\n定义包含一些可学习的参数（权重）神经网络模型 在数据集上迭代 通过神经网络处理输入 计算损失（输出结果和正确值的差值大小） 将梯度反向传播回网络的参数 更新网络的参数（梯度下降）：weight = weight - learning_rate * gradient 定义网络 在模型中必须定义 forward()， backword（用来计算梯度）会被 autograd 自动创建。可在 forward() 中使用任何针对 Tensor 的操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import torch import torch.nn as nn import torch.nn.functional as F class Net(nn.Model): def __init__(self): super(Net, self).__init__() # 1 input image channel, 6 output channels, 3x3 square convolution # kernel self.conv_1 = nn.Conv2d(1, 6, 3) self.conv_2 = nn.Conv2d(6, 16, 3) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 6 *6, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max Pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(sefl, x): size = x.size()[1: ] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features net = Net() print(net) # Net( # (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) # (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) # (fc1): Linear(in_features=576, out_features=120, bias=True) # (fc2): Linear(in_features=120, out_features=84, bias=True) # (fc3): Linear(in_features=84, out_features=10, bias=True) # ) parameters() 返回可被学习的参数（权重）列表和值\n1 2 3 4 params = list(net.parameters()) print(len(params)) # 10 print(params[0].size()) # conv1 的 weight torch.Size([6, 1, 3, 3]) 测试随机输入 32x32。注：这个网络（LeNet）的期望的输入大小是 32x32，如果使用 MINIST 数据集来训练这个网络，请把图片大小重新调整到 32x32。\n1 2 3 4 5 input = torch.randn(1, 1, 32, 32) out = net(input) print(out) # tensor([[ 0.1120, 0.0713, 0.1014, -0.0696, -0.1210, 0.0084, -0.0206, 0.1366, # -0.0455, -0.0036]], grad_fn=\u0026lt;AddmmBackward\u0026gt;) 将所有的参数的梯度缓存清零，进行随机梯度的反向传播。\n1 2 net.zero_grad() out.backward(torch.randn(1, 10)) Tip:\ntorch.nn 仅支持小批量输入。整个 torch.nn 包都只支持小批量样本，而不支持单个样本。 如：nn.Conv2d接受一个4维 Tensor，分别维 sSamples * nChannels * Height * Width （样本数* 通道数 * 高 * 宽）。如果你有单个样本，只需要使用 input.unsqueeze(0) 来添加其他的维数。\n至此，我们大致了解了如何构建一个网络，回顾一下到目前为止使用到的类。\ntorch.Tensor： 一个多维数组。 支持使用 backward() 进行自动梯度计算，并保存关于这个向量的梯度 w.r.t.\nnn.Model： 神经网络模块。实现封装参数、移动到 GPU 上运行、导出、加载等。\nnn.Parameter： 一种张量。将其分配为 Model 的属性时，自动注册为参数。\nautograd.Function： 实现一个自动求导操作的前向和反向定义。每个 Tensor 操作都会创建至少一个 Function 节点，该节点连接到创建 Tensor 的函数，并编码其历史记录。\n损失函数 1 2 3 4 5 6 7 8 9 output = net(input) target = torch.randn(10) # 例子：一个假设的结果 target = target.view(1, -1) # 让 target 与 output 的形状相同 criterion = nn.MSELoss() loss = criterion(output, target) print(loss) 反向传播 要实现反向传播误差，只需要 loss.backward()。 但是，需要清除现有的梯度，否则梯度将累积到现有的梯度中。\n1 2 3 4 5 6 7 net.zero_grad() # 将所有的梯度缓冲归零 print(net.conv1.bias.grad) # conv1.bias.grad 反向传播前 tensor([0., 0., 0., 0., 0., 0.]) loss.backward() print(net.conv1.bias.grad) # 反向传播后 tensor([0.0111, -0.0064, 0.0053, -0.0047, 0.0026, -0.0153]) 更新权重 在使用 PyTorch 时，可以使用 torch.optim 中提供的方法进行梯度下降。如：SDG，Nesterov-SDG，Adam，RMSprop 等。\n1 2 3 4 5 6 7 8 9 10 11 import torch.optim as optim # 创建一个 optimizer optimizer = optim.SDG(net.parameters(), lr=0.01) # 在训练中循环 optimizer.zero_grad() # 将梯度缓冲区清零 output = net(input) loss = criterion(output, target) loass.backword() optimizer.step() # 更新 训练分类器 数据从哪里来？ 通常，需要处理图像、文本、音频或视频数据时，可以使用将数据加载到 NumPy 数组中的标准 Python 包，再将该数值转换为 torch.*Tensor。\n处理图像，可以使用 Pillow，OpenCV 处理音频，可以使用 SciPy，librosa 处理文本，可基于 Python 或 Cython 的原始加载，或 NLTK 和 SpaCy 对于图像任务，其中包含了一个 torchvision 的包，含有常见的数据集（Imagenet，CIFAR10，MNIST等）的数据加载器，以及用于图像的数据转换器（torchvision.datasets 和 torch.utils.data.DataLoader）。\n在本示例中，将使用 CIFAR10 数据集。其中包含 10 分类的图像：“飞机”，“汽车”，“鸟”，“猫”，“鹿”，“狗”，“青蛙”，“马”，“船”，“卡车”。图像的尺寸为 3 * 32 * 32，即尺寸为 32 * 32 像素的 3 通道彩色图像。\n接下来，作为演示，将按顺序执行以下步骤训练图像分类器：\n使用 torchvision 加载并标准化 CIFAR10 训练和测试数据集 定义 CNN 定义损失函数 根据训练数据训练网络 在测试数据上测试网络 加载并标准化 CIFAR10 1 2 3 import torch import torchvision import torchvision.transforms as transforms torchvision 的输出是 [0, 1] 的 PILImage 图像，我们要把它转换为归一化范围为 [-1, 1] 的张量。\n1 2 3 4 5 6 7 8 9 10 11 transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] ) trainset = torchvision.datasets.CIFAR10(root=\u0026#39;./data\u0026#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testset = trochvision.datasets.CIFAR10(root=\u0026#39;./data\u0026#39;, train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, barch_size=4, shuffle=False, num_workers=2) classes = (\u0026#39;plane\u0026#39;, \u0026#39;car\u0026#39;, \u0026#39;bird\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;deer\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;frog\u0026#39;, \u0026#39;horse\u0026#39;, \u0026#39;ship\u0026#39;, \u0026#39;truck\u0026#39;) 定义 CNN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() # in_channels, out_channels, kernel_size # 输入的为 3 通道图像，提取 6 个特征，得到 6 个 feature map，卷积核为一个 5*5 的矩阵 self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) # 卷积层输出了 16 个 feature map，每个 feature map 是 6*6 的二维数据 self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net(s) 定义损失函数和优化器 这里我们使用交叉熵作为损失函数，使用带动量的随机梯度下降。\n1 2 3 4 import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SDG(net.parameters(), lr=0.001, momentum=0.9) 训练网络 接下来，只需要在迭代数据，将数据输入网络中并优化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 for epoch in range(2): running_loss = 0.0 for i, data in enumerate(trainloader, 0): inputs, labels = data # 获取输入 optimizer.zero_grad() # 将梯度缓冲区清零 outputs = net(inputs) # 正向传播 loss = criterion(outputs, lables) loss.backward() # 反向传播 optimizer.step() # 优化 running_loss += loss.item() if i % 2000 == 1999: # 每 2000 批次打印一次 print(\u0026#39;[]\u0026#39; % (epoch+1, i+1, running_loss / 2000)) running_loss = 0.0 在测试集上测试数据 在上面的训练中，我们训练了 2 次，接下来，我们要检测网络是否从数据集中学习到了有用的东西。通过预测神经网络输出的类别标签与实际情况标签对比进行检测。\n1 2 3 4 5 6 7 8 9 10 11 12 corrent = 0 total = 0 with torch.no_grad(): for data in testloader: images, lobels = data outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) corrent += (predicted == labels).sum().item() print(\u0026#39;Accuracy of the network on the 10000 test images: %d %%\u0026#39; % (100 *corrent / total)) # Accuracy of the network on the 10000 test images: 9% 在训练两次的网络中，随机选择的正确率为 10%。网络似乎学到了一些东西。\n那这个网络，识别哪一类好，哪一类不好呢？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class_corrent = list(0. for i in range(10)) class_total = list(0. for i in range(10)) with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i].item() class_total[label] += 1 for i in range(10): print(\u0026#39;Accuracy of %5s : %2d %%\u0026#39; % (classes[i], 100 * class_correct[i] / class_total[i])) # Accuracy of plane : 99 % # Accuracy of car : 0 % # Accuracy of bird : 0 % # Accuracy of cat : 0 % # Accuracy of deer : 0 % # Accuracy of dog : 0 % # Accuracy of frog : 0 % # Accuracy of horse : 0 % # Accuracy of ship : 0 % # Accuracy of truck : 0 % 使用 GPU 与将 tensor 移到 GPU 上一样，神经网络也可以移动到 GPU 上。 如果可以使用 CUDA，将设备定义为第一个 cuda 设备：\n1 2 3 4 device = torch.device(\u0026#39;cuda:0\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) print(device) # cuda:0 复制 nn 和 tensor 到 GPU 上。\n1 2 3 model = net.to(device) inputs, labels = data[0].to(device), data[1].to(device) Tip:\n使用 .to(device) 并没有复制 nn / tensor 到 GPU 上，而是返回了一个 copy。需要赋值到一个新的变量后在 GPU 上使用这个 nn / tensor。\n参考 https://pytorch.apachecn.org/#/docs/1.7/02 ","date":"2021-04-01T21:14:45+08:00","image":"https://emerywan.github.io/blog/imgs/10.jpeg","permalink":"https://emerywan.github.io/blog/p/pytorch-handbook/","title":"PyTorch 火速上手"},{"content":"React Hooks 简介 Hook 含义 组件类和函数组件 useState 使用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import React, { useState } from \u0026#39;react\u0026#39;; const Example() { // 声明了一个 count 的 state 变量，并初始化为 0 // setCount 设置 state 值 count 的方法 const [count, setCount] = useState(0); // 每次点击按钮，将 count 的值 +1 return ( \u0026lt;\u0026gt; \u0026lt;p\u0026gt;You clicked {count} times\u0026lt;/p\u0026gt; \u0026lt;button onClick={() =\u0026gt; setCount(count + 1)}\u0026gt; Click me \u0026lt;/button\u0026gt; \u0026lt;/\u0026gt; ) } export default Example; 语法 1 2 // 声明一个 state 变量，并同时初始化 const [state, setState] = useState(initialState) useState 返回一个包含两个元素的数组 state 变量，指向状态当前值 setState 更新 state 值的方法 initialState 状态初始值可以是数字，字符串，数组，对象等 与类中使用 setState 异同 相同点 在一次渲染周期中调用多次 setState，数据只改变一次 不同点 类组件中 setState 为合并 函数组件中 setState 为替换 useEffect useEffect 用来执行副作用。可以将 useEffect Hook 看做 componentDidMount, componentDidUpdate, componentWillMount 生命周期函数的组合。\n常用在：\n服务器请求 访问元素 dom 元素 本地持久化缓存 绑定/解绑事件 添加订阅 设置定时器 记录日志 等 useEffect 接收一个函数，该函数会在组件渲染完毕后才执行，该函数有要求：要么返回一个能清除副作用的函数，要么就不返回任何内容。\n不需要清除的 effect 在 React 更新 DOM 之后，我们想运行一些额外的代码。比如发送网络请求，手动变更 DOM，记录日志，这些都是常见的无需清除的操作。 因为我们在执行完这些操作之后，就可以忽略他们了。\n以下为使用 class 和 Hook 都是怎么实现这些副作用的对比。\n使用类组件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // 在 React 更新 DOM 操作后，立即更新 document 的 title 属性 class Example extends React.Component { constructor(props) { super(props); this.state = { count: 0 }; } // 组件加载时需要更新 componentDidMount() { document.title = `You clicked ${this.state.count} times`; } // 组件更新时需要更新 componentDidUpdate() { document.title = `You clicked ${this.state.count} times`; } // 在 render() 不能有任何副作用。应该在 react 更新 DOM 后再执行副作用。 render() { return ( \u0026lt;div\u0026gt; \u0026lt;p\u0026gt;You clicked {this.state.count} times.\u0026lt;/p\u0026gt; \u0026lt;button onClick={() =\u0026gt; this.setState({ count: this.state.count + 1})}\u0026gt; click me. \u0026lt;/button\u0026gt; \u0026lt;div/\u0026gt; ) } } 使用 Hook 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import React, {useState, useEffect} from \u0026#39;react\u0026#39;; export default function Example() { const [count, setCount] = useState(0); useEffect(() =\u0026gt; { // 经过渲染或更新， DOM 更新完毕之后，会运行该 effect document.title = `You click ${count} times`; }); return ( \u0026lt;div\u0026gt; \u0026lt;p\u0026gt;You clicked {this.state.count} times.\u0026lt;/p\u0026gt; \u0026lt;button onClick={() =\u0026gt; this.setState({ count: this.state.count + 1})}\u0026gt; click me. \u0026lt;/button\u0026gt; \u0026lt;div/\u0026gt; ); } 第一次渲染之后和每次更新之后都会执行 useEffect。且 React 保证每次运行 effect 的同时，DOM 已经更新完毕。 useEffect 放在组件内部让我们可以在 effect 中直接访问 state / props。我们不需要特殊的 API 来读取。 与 componentDidMount 或 componentDidUpdate 不同，使用 useEffect 调度的 effect 不会阻塞浏览器更新屏幕，这让你的应用看起来响应更快。大多数情况下，effect 不需要同步地执行。在个别情况下（例如测量布局），有单独的 useLayoutEffect Hook 供你使用，其 API 与 useEffect 相同。 需要清除的 effect 有一些副作用是需要清除的，如订阅外部数据源。这种情况下，清除工作是非常重要的，可以防止引起内存泄露！\n使用类组件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class FriendStatus extends React.Component { constructor(props) { super(props); this.state = { isOnline: null }; this.handleStatusChange = this.handleStatusChange.bind(this); } componentDidMount() { ChatAPI.subscribeToFriendStatus( this.props.friend.id, this.handleStatusChange ); } componentDidUpdate () { // ... } componentWillUnmount() { // 在组件卸载时，关闭订阅的数据源 ChatAPI.unsubscribeFromFriendStatus( this.props.friend.id, this.handleStatusChange ); } handleStatusChange(status) { this.setState({ isOnline: status.isOnline }); } render() { if (this.state.isOnline === null) { return \u0026#39;Loading...\u0026#39;; } return this.state.isOnline ? \u0026#39;Online\u0026#39; : \u0026#39;Offline\u0026#39;; } } 使用函数组件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import React, {useState, useEffect} from \u0026#39;react\u0026#39;; export default function FriendStatus(props) { const [isOnline, setIsOnline] = useState(null); useEffect(() =\u0026gt; { function handleStatusChange(status) { setIsOnline(status.isOnline); } ChatAPI.subscribeToFriendStatus(props.friend.id, handleStatusChange); // effect 可选清除副作用的函数，返回函数不一定需要命名 return function cleanup() { ChatAPI.unsubscribeFromFriendStatus(props.friend.id, handleStatusChange); }; }); if (isOnline === null) { return \u0026#39;Loading...\u0026#39;; } return isOnline ? \u0026#39;Online\u0026#39; : \u0026#39;Offline\u0026#39;; } React 会在组件卸载的时候执行清除操作。 多个 Effect 实现关注点分离 使用 Hook 的其中一个目的是要解决： class 生命周期函数中经常包含不相关逻辑，相关逻辑又分离在几个不同的方法中。\n如下为上文中计数器和好友状态指示器逻辑组合在一起的组件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class FriendStatusWithCounter extends React.Component { constructor(props) { super(props); this.state = { count: 0, isOnline: null }; this.handleStautsChange = this.handleStatusChange.bind(this); } // 相应的逻辑被分配在三个不同的生命周期函数中 componentDidMount() { // counter document.title = `You clicked ${this.state.count} times`; // friend status ChatAPI.subscribeToFriendStatus( this.props.fried.id, this.handleStatusChange ) } componentDidUpdate() { document.title = `You clicked ${this.state.count} times`; } componentWillUnmount() { ChatAPI.unsubscribeFromFriendStatus( this.props.friend.id, this.handleStatusChange ); } handleStatusChange(status) { this.setState({ isOnline: status.isOnline }); } // ... } 若使用 Hook，和使用多个 state 的 Hook 一样，通过使用多个 effect，将不相关的逻辑分离到不同的 effect 中。\nHook 允许按照代码的用途进行分离，不同于生命周期函数。将按照 effect 声明的顺序依次调用组件中的每一个 effect。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 export default const FriendStatusWithCounter = () =\u0026gt; { const [count, setCount] = useState(0); useEffect(() =\u0026gt; { document.title = `You clicked ${this.state.count} times`; }); const [isOneline, useIsOneline] = useState(null); useEffect(() =\u0026gt; { function handleStatusChange(status) { setIsOneline(status.isOneline); } ChatAPI.subscribeToFriendStatus(props.friend.id, handleStatusChange); return () =\u0026gt; { ChatAPI.unsubscribeFromFriendStatus(props.friend.id, handleStatusChange); }; }); // ... } 跳过 Effect 进行性能优化 在类组件中，我们可以通过 componentDidUpdate 中 prevProps 和 prevState 的 props / state 的变化比较，判断是否需要执行某些副作用。\n1 2 3 4 5 componentDidUpdate(prevProps, prevState) { if (prevState.count !== this.state.count) { document.title = `You clicked ${this.state.count} times`; } } 在 Hook 中，可以通过对 useEffect 传递数组作为第二可选参数，通知 React 跳过对 effect 的调用。\n1 2 3 4 5 useEffect(() =\u0026gt; { document.title = `You clicked ${count} times`; }, [count]); // 表示 count 更改时会进行更新 // {... , [5]} 当 count === 5 时，跳过 effect 若只想运行一次 effect（仅在组件挂载和卸载时执行），可传递一个空数组 [] 作为第二参数。 传入一个空数组（[]），effect 内部的 props 和 state 会一直拥有其初始值。 使用该优化，请确保数组中包含了所有外部作用域中会随时间变化且在 effect 中使用的变量，否则代码会用到先前渲染中的旧变量。 每次更新的时候都要运行 Effect 的原因 自定义 Hook 自定义 Hook 可以将组件逻辑提取到可重用的函数中。\n目前为止，在 React 中有两种流行的方式共享组件之间的状态逻辑：\nrender props 高阶组件 使用 Hook 可以在不增加组件的情况下解决相同的问题。 创建自定义 Hook 当我们想在函数间共享逻辑时，我们可以把它提取到另外一个函数中。由于组件和 Hook 都是函数，所以也同样使用于这种方式。\n自定义 Hook 是一个函数，其名称以 \u0026ldquo;use\u0026rdquo; 开头，函数内部可以调用其他的 Hook。\n以下是根据上文中 FriendStatus 实例，所提取的自定义 Hook。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import { useState, useEffect } from \u0026#39;react\u0026#39;; // 名称一定以 use 开头 function useFriendStatus(friendID) { const [isOnline, setIsOnline] = useState(null); useEffect(() =\u0026gt; { function handleStatusChange(status) { setIsOnline(status.isOneline); } ChatAPI.subscribeToFriendStatus(friendID, handleStatusChange); return () =\u0026gt; { ChatAPI.unsubcribeFromFriendStatus(friendID, handleStatusChange); }; }); return isOnline; } export default useFriendStatus; 在这个自定义 Hook 中，没有包含任何新内容（逻辑与组件中的完全一致）\n使用自定义 Hook 与组件一致，请确保只在自定义 Hook 的顶层无条件地调用其他 Hook。\n1 2 3 4 5 6 7 8 function FriendStatus(props) { const isOnline = useFriendStatus(props.friend.id); if (isOnline === null) { return \u0026#34;Loading...\u0026#34;; } return isOnline ? \u0026#39;Online\u0026#39; : \u0026#39;Offline\u0026#39;; } 这段代码与之前的工作方式完全一样，我们只是将函数中需要共享的逻辑提取到单独的函数中。\n自定义 Hook 是一种自然遵循 Hook 设计的约定，不是 React 的特性。 自定义 Hook 必须以 \u0026ldquo;use\u0026rdquo; 开头 不同组件中使用相同的 Hook 不会共享 state。 自定义 Hook 是重用状态逻辑的机制（例如设置为订阅并存储当前值），每次使用自定义 Hook 时，其中的所有 state 和副作用完全隔离。 在多个 Hook 之间传递信息 由于 Hook 本身就是函数，因此我们可以在它们之间传递信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // 这是一个聊天消息接收者的选择器，它会显示当前选定的好友是否在线 function ChatRecipientPicker() { // 当前选择的好友 ID 保存在 recipientID 状态变量中 const [recipientID, setRecipientID] = useState(1); // 当更新 recipientID 状态变量时，useFriendStatus Hook 会取消订阅之前选中的好友，并订阅新选中的好友状态 const isRecipientOnline = useFriendStatus(recipientID); return ( \u0026lt;\u0026gt; \u0026lt;Circle color={isRecipientOnline ? \u0026#39;green\u0026#39; : \u0026#39;red\u0026#39;} /\u0026gt; \u0026lt;select // \u0026lt;select\u0026gt; 中选择其他好友时更新 recipientID value={recipientID} onChange={e =\u0026gt; setRecipientID(Number(e.target.value))} \u0026gt; { friendList.map(friend =\u0026gt; ( \u0026lt;option key={friend.id} value={friend.id}\u0026gt; {friend.name} \u0026lt;/option\u0026gt; )) } \u0026lt;/select\u0026gt; \u0026lt;/\u0026gt; ) } 我们将当前选择的好友 ID 保存在 recipientID 状态变量中，并会在用户从 \u0026lt;select\u0026gt; 中选择其他好友时更新这个 state。\n由于 useState 会提供 recipientID 状态变量的最新值，可以将它作为参数传递给自定义的 useFriendStatus Hook。当我们选择不同的好友并更新 recipientID 状态变量时，useFriendStatus Hook 会取消订阅之前选中的好友，并订阅新选中的好友状态。\nHook 规则 额外的 Hook 参考 https://juejin.im/post/6844903814349193229 https://juejin.im/post/6844903985338400782 https://www.ruanyifeng.com/blog/2019/09/react-hooks.html https://zh-hans.reactjs.org/docs/hooks-intro.html ","date":"2021-01-28T21:14:45+08:00","image":"https://emerywan.github.io/blog/imgs/9.jpeg","permalink":"https://emerywan.github.io/blog/p/react-hooks/","title":"React Hooks"},{"content":"Ubuntu 耳机没有声音 笔记本使用 Ubuntu 插有线耳机后，并没有反应，该外放还是外放。\n感觉是适配问题，windows 下没有问题。找了一圈办法，只能通过软件手动切换了，效果还不错。\n1 sudo apt install pavucontrol 可以在这里切换输出设备：\n切换输入设备：\n","date":"2021-01-02T02:02:02+08:00","image":"https://emerywan.github.io/blog/imgs/5.jpeg","permalink":"https://emerywan.github.io/blog/p/ubuntu-sound/","title":"Ubuntu 耳机没有声音"},{"content":"数据类型 数组 numpy.array np.zeros 创建指定大小的数组，数组元素以 0 填充。\nnumpy.zeros(shape, dtype=None, order = \u0026lsquo;C\u0026rsquo;) shape 数组形状 dtype 数据类型 order \u0026lsquo;C\u0026rsquo; 用于 C 的行数组，或者 \u0026lsquo;F\u0026rsquo; 用于 FORTRAN 的列数组 1 2 3 4 5 6 7 8 9 10 11 12 13 np.zero(5) # [0. 0. 0. 0. 0.] np.zero(5, dtype=int) # [0 0 0 0 0] np.zero((3, 5)) # 三行五列 np.zero(shape=(3, 5)) # [ # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # ] np.ones 创建指定大小的数组，数组元素以 1 填充。\nnumpy.ones(shape, dtype=None, order=\u0026lsquo;C\u0026rsquo;) np.full 创建指定大小和数据的数组\nnumpy.full(shape, fill_value, dtype=None, order=\u0026lsquo;C\u0026rsquo;) 1 2 # 创建一个 三行五列 值为 666.0 的矩阵 np.full((3, 5), 666.0) np.arange 在数组中，可以使用 range 创建指定范围的数组（不能传递浮点数）。\n1 2 3 4 # [0, 20) 步长为 2 [i for i in range(0, 20, 2)] # [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] 在 numpy 中，可以使用 arange 创建数组范围并返回 ndarray 对象。\nnumpy.arange(start, stop, step, dtype) start 起始值，默认为 0 stop 终止值，不包含 step 步长，默认为 1 dtype 数据类型 1 2 3 4 5 np.arange(0, 20, 2) # 可以传递浮点数 np.arange(0, 1, 0.2) # [0., 0.2, 0.4, 0.6, 0.8] np.linspace 创建一个一维数组，构成一个等差数列。\nnumpy.linspace(start, stop, num=50, endpoint=True, retstep=False,dtype=None, axis=0) start 起始值 stop 终止值（默认包含 endpoint=True） endpoint 当为 True 时，会包含 stop 的值 retstep 当为 True 时，生成的数组中会显示间距 dtype 数据类型 1 2 3 # 生成 [0, 20] 间距为 2 的 10 个数 np.linspace(0, 20, 11) # [0., 2., 4., 6., 8., 10., 12., 14., 16., 18., 20.] np.random numpy.random.randint(low, high=None, size=None, dtype=None) 生成指定的整数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 在 [0, 10) 间生成一个随机数 np.random.randint(0, 10) # 在 [0, 10) 间生成一个大小为 10 的一维数组 np.random.randint(0, 10, size=10) # [7, 7, 7, 7, 7, 4, 5, 6, 4, 7] # 在 [0, 10) 间生成一个 3行6列 的矩阵 np.random.randint(0, 10, size=(3, 5)) # 指定随机种子，使生成的数据在测试时保持一致 np.random.seed(666) numpy.random.random(size=None) 生成 [0.0, 1.0) 间的浮点数。\nnumpy.random.normal(loc=0.0, scale=1.0, size=None) 生成一个符合正态分布的浮点数。\nloc 均值 scale 方差 size 大小 1 2 3 4 5 # 生成 均值为 10 方差为 100 的浮点数 np.random.normal(10, 100) # 生成 均值为 0 方差为 1 的 3行5列的矩阵 np.random.normal(0, 1, (3, 5)) numpy.array 基本操作 假设有如下一个一维的数组 x 和 一个三行五列的矩阵 X：\n1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np np.random.seed(0) x = np.arange(10) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] X = np.arange(15).reshape(3, 5) # [ # [ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14] # ] Reshape 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # 将 x 转置为 2行5列的 矩阵 A = x.reshape(2, 5) # [ # [0, 1, 2, 3, 4], # [5, 6, 7, 8, 9] # ] # 将向量装置成矩阵 B = x.reshape(1, 10) # [ # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # ] # -1：让 numpy 自己决定维度 # 10列 C = x.reshape(-1, 10) # [ # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # ] # 10行 D = x.reshape(10, -1) # [ # [0], # [1], # [2], # [3], # [4], # [5], # [6], # [7], # [8], # [9], # ] # 2行 E = x.reshape(2, -1) # [ # [0, 1, 2, 3, 4], # [5, 6, 7, 8, 9] # ] # 数据不能整除的情况下，不能 reshape F = x.reshape(3, -1) 基本属性 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # ndarray.ndim 秩，即轴的数量或维度的数量 x.ndim # 1 X.ndim # 2 # ndarray.shape 数组维度，对于矩阵为n行m列 x.shape # (10, ) X.shape # (3, 5) # ndarray.size 数组元素的总个数，对于矩阵为 n*m 个 x.size # 10 X.size # 15 数据访问 一维 numpy.array 可以和数组一样进行访问。\n多维 numpy.array 在访问时，推荐传入多个参数。\n1 2 3 # 一维 x[0] # 0 ","date":"2020-12-01T21:14:45+08:00","image":"https://emerywan.github.io/blog/imgs/3.jpeg","permalink":"https://emerywan.github.io/blog/p/numpy/","title":"NumPy"},{"content":"机器学习 基本术语 如图所示表格，是鸢尾花（lris）相关信息的数据，其中：\n数据整体称为数据集（data set） 每一行数据称为一个样本（sample） 每一列（除表格最后一列）表达样本的一个特征（feature） 最后一列，成为标记（label） 如图所示信息，其中 萼片长度、宽度，花瓣的长度、宽度称为 特征；每一行特征的值称为特征向量（数学上通常会将其表示为列向量）。\n若将该图中的数据表示为矩阵的方式，结果如图：\n选择两种鸢尾花的特征为例，将其表示为如图的二维空间（多维特征将其表示为多维空间）。\n我们将这样的空间称为 特征空间（feature space）。\n分类任务本质就是在特征空间切分，如图所示，在特征空间中，将鸢尾花根据两种特征分为了两类。\n机器学习的基本任务 机器学习的基本任务基本有两类，分别为：\n分类 回归 分类任务 一些算法只支持完成二分类的任务 多分类的任务可以转换成二分类的任务 一些算法天然的支持多分类问题 二分类 二分类问题的常见实例：\n判断邮件是垃圾邮件；不是垃圾邮件 判断发放给客户信用卡有风险；无风险 判断病患良性肿瘤；恶性肿瘤 判断某支股票涨；跌 多分类 多分类问题的常见实例：\n数字识别 图像识别 判断发放给客户信用卡的风险评级 围棋游戏等 自动驾驶识别 多标签分类 如图所示，为多标签分类问题的常见实例：\n回归任务 回归问题的结果与分类问题的结果不同，回归问题的结果是一个连续数字的值，而非一个类别。\n常见的回归问题有：\n房屋价格 市场分析 学术成绩 股票价格 在一些情况下，回归任务可以简化成分类任务。\n机器学习算法分类 监督学习 监督学习 supervised learning 主要处理的是分类问题和回归问题。\n监督学习的含义是给机器的训练数据中拥有“标记”或者“答案”。根据这些数据进行模型的训练。\n如根据图片判断猫狗（图像已经拥有了标定信息） 如更具手写字体识别数字（给出结果标记） 银行已经积累了一定的客户信息和他们信用卡的信用情况\n医院已经积累了一定的病人信息和他们最终确诊是否患病的情况。\n常见的监督学习算法有：\nk近邻 线性回归和多项式回归 逻辑回归 SVM 决策树和随机森林 非监督学习 非监督学习 unsupervised learning 给机器的训练数据没有任何“标记”或者“答案”。\n对没有“标记”的数据进行分类 \u0026ndash; 聚类分析。\n意义 对数据进行降维处理 特征提取：行用卡的信用评级和人的胖瘦无关？ 特征压缩：PCA 降维处理的意义：方便可视化\n我们无法理解四维以上空间，所以可以将其降维到三维或者二维空间，方便理解。\n异常检测 半监督学习 semi-supervised learning\n一部分数据有“标记”或者答案，另一部分数据没有\n常见的场景：各种原因产生的标记缺失。\n我们通常都先使用无监督学习手段对数据做处理，之后使用监督学习手段做模型的训练和预测。\n增强学习 根据周围环境的情况，采取行动，根据采取行动的结果，学习行动方式。\n无人驾驶 机器人 在增强学习中，监督学习和半监督学习是基础。\n机器学习的其他分类 在线学习和批量学习 在线学习 Online Learning\n优点：及时反映新的环境变化\n问题：新的数据可能带来不好的变化，错误的数据可能带来错误的结果\n解决方案：需要加强对数据的监控 也适用于数据量巨大，完全无法批量学习的环境\n批量学习（离线学习） Batch Learning / Offline Learning\n优点：简单\n问题：要考虑如如何适应环境变化\n解决方案：定时重新批量学习 缺点：每次重新批量学习，运行量巨大。在某些环境变化非常快的情况下，甚至是不可能的。\n参数学习和非参数学习 参数学习 Parametric Learning\n如图所示，为房屋面积与价格的关系曲线。\n特点：一旦学习到了参数，就不需要原有的数据集。\n非参数学习 Nonparametric Learning\n不对模型进行过多假设\n非参数不等于没有参数，而是对整个不进行建模，不学习一些参数\n","date":"2020-09-28T14:02:02+08:00","permalink":"https://emerywan.github.io/blog/p/mechine-learning-basics/","title":"机器学习基础"},{"content":"Spring Cloud Hystrix 在微服务架构中，我们将系统拆分成了一个个的服务单元，各单元应用间通过服务注册与订阅的方式互相依赖。由于每个单元都在不同的进程中运行，如果某个服务不可用，可能导致级联故障，造成整个系统不可用的情况（雪崩效应）。为了解决这样的问题，产生了断路器等一系列的服务保护机制。\n简介 Spring Cloud Hystrix，它是一个基于 Netflix 的开源框架，具有如下功能：\n服务降级 依赖隔离 服务熔断 监控（Hystrix Dashboard） 实现一个 Hystrix Server 添加依赖 1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-hystrix\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 添加注解 1 2 3 4 5 6 7 8 @EnableCircuitBreaker @EnableDiscoveryClient @SpringBootApplication public class HystrixApplication { public static void main(String[] args) { SpringApplication.run(HystrixApplication.class, args); } } 配置文件 1 2 3 4 5 6 7 8 9 10 11 spring: application: name: hystrix-server server: port: 8080 eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ Hystrix 应用 服务降级 假设现在有一个接口 /user/{id} 获取用户信息。\n1 2 3 public ResultVo\u0026lt;UserInfo\u0026gt; getUserInfo(@PathVariable String id) { return userService.getUserInfo(id); } 在 UserService 中添加调用方法的服务降级。\n1 2 3 4 5 6 7 8 9 10 11 12 @HystrixCommand(fallbackMethod = \u0026#34;userFallback\u0026#34;) public ResultVo\u0026lt;UserInfo\u0026gt; getUserInfo(String id) { // 正常的服务调用和业务 此处以 restTemplate 为例 // ... return restTemplate.getForObject(url + \u0026#34;/user/{1}\u0026#34;, ResultVo.class, id); } public ResultVo\u0026lt;UserInfo\u0026gt; userFallback() { // ... // 处理服务降级需要返回的内容 } 服务降级 OpenFiegn 配置文件 1 2 3 feign: hystrix: enabled: true OpenFeign Client 端实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @FeignClient( name = \u0026#34;user\u0026#34;, // 远程服务名 fallback = UserClientFallback.class // 指定 当服务降级时，采用的方法 ) public interface UserClient { @GetMapping(\u0026#34;/user/{id}\u0026#34;) ResultVo\u0026lt;UserInfo\u0026gt; getUserInfo(@PathVariable String id); } --- // 实现 UserClient 接口 @Component public class UserClientFallback implements UserClient { @Override ResultVo\u0026lt;UserInfo\u0026gt; getUserInfo(@PathVariable String id) { // ... // 实现降级内容 } } 依赖隔离 SpringCloud Hystrix 的 依赖隔离 类似于docker的“舱壁模式”。 docker通过”舱壁模式”实现进程隔离，使得容器之间互不影响。 而Hystrix使用该模式实现：“线程池隔离”，会为每一个HystrixCommand创建一个独立线程池，这样就算某个在Hystrix包装下的依赖服务出现延迟过高情况，也只是对该依赖服务的调用产生影响，并不会拖慢其他服务。\n使用 @HystrixCommand 来将某个函数包装成了 Hystrix 命令时，Hystrix框架自动地为这个函数实现了依赖隔离。所以依赖隔离，服务降级在使用时候都是一体化实现的，这样就可以实现服务容错保护。在编程模型上就会非常方便。\n服务熔断 ","date":"2020-07-29T21:14:45+08:00","permalink":"https://emerywan.github.io/blog/p/spring-cloud-hystrix/","title":"Spring Cloud Hystrix 服务容错"},{"content":"Spring Cloud Zuul 在微服务架构中，后端服务往往不直接开放给调用端，而是通过一个API网关根据请求的url，路由到相应的服务。 当添加API网关后，在第三方调用端和服务提供方之间就创建了一面墙，这面墙直接与调用方通信进行权限控制，后将请求均衡分发给后台服务端。 Spring Cloud Zuul 是一个基于JVM路由和服务端的负载均衡器，提供动态路由，监控，弹性，安全等的边缘服务。\n简介 Zuul 的主要功能是路由转发和过滤器（Filter）。不同类型的 Filter 用于处理请求，可以实现以下功能：\n权限控制和安全性：可以识别认证需要的信息和拒绝不满足条件的请求 监控：监控请求信息 动态路由：根据需要动态地路由请求到后台的不同服务集群 压力测试：逐渐增大到集群的流量，以便进行性能评估 负载均衡：为每种类型的请求分配容量并丢弃超过限额的请求 限流 黑白名单过滤 静态资源处理：直接在zuul处理静态资源的响应而不需要转发这些请求到内部集群中 基础使用 创建一个 api-gateway 添加依赖 1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-zuul\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 添加注解 1 2 3 4 5 6 7 8 @EnableZuulProxy @EnableDiscoveryClient @SpringBootApplication public class ApiGatewayApplication { public static void main(String[] args) { SpringApplication.run(ApiGatewayApplication.class, args); } } 配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 spring: application: name: api-gateway server: port: 9000 # 自定义路由规则 zuul: routes: user: path: /user/** serviceId: user 测试 通过以上配置文件配置，即可通过 api-gateway 服务去请求 user 服务。\n假设 user 服务的端口为 8080，其中包含一个 api 为 /info。 通过访问 http://localhost:9000/user/info，即可访问该 api。（若原接口包含路由前缀 /user，需要使用 /user/user/info 访问）\n常用功能 统一前缀 1 2 zuul: prefix: /proxy Header 过滤及重定向添加 Host 1 2 3 4 5 6 7 8 9 zuul: # 默认为该配置，会过滤 Cookie Set-Cookie Authorization 信息 # 设置为空即不会过滤 sensitive-headers: Cookie,Set-Cookie,Authorization --- zuul: add-host-header: true # 重定向会添加 host 请求头 查看路由信息 添加依赖 1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置文件 1 2 3 4 5 6 7 8 server: port: 9000 management: endpoints: web: exposure: include: \u0026#39;routes\u0026#39; 访问接口 访问 http://localhost:9000/actuator/routes 获取信息 访问 http://localhost:9000/actuator/routes/details 获取详细信息 Zuul 应用 Zuul Filter Filter是Zuul的核心，用来实现对外服务的控制。Filter有4个生命周期。\npre 在请求被路由到目标服务前执行。 比如权限校验、打印日志等功能。 routing 在请求被路由到目标服务时执行。 用于构建发送给微服务的请求，并使用 Apache HttpClient 或 Netfilx Ribbon 请求微服务。 post 这种过滤器在路由到微服务以后执行。 为响应添加标准的HTTP Header、收集统计信息和指标、将响应从微服务发送给客户端等。 error 其他阶段发生错误时执行该过滤器。 自定义 Filter 实现自定义 Filter，需继承 com.netflix.zuul.ZuulFilter，并覆盖继承的方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @Component public class MyFilter extends ZuulFilter { @Override String filterType() { // 定义filter的类型，pre、route、post、error return null; } @Override int filterOrder() { // 定义filter的顺序，数字越小表示顺序越高，越先执行 return 0; } @Override boolean shouldFilter() { // 是否需要执行该filter，true表示执行，false表示不执行 return false; } @Override Object run() { // filter需要执行的具体操作 return null; } } Zuul 限流 限流在前置过滤器（pre）前使用，在请求被转发前调用，且优先级最高。\n令牌桶限流示例。令牌桶算法能够在限制数据的平均传输速率的同时还允许某种程度的突发传输。\n令牌桶算法会以一个恒定的速度往桶里放入令牌，而如果请求需要被处理，则需要先从桶里获取一个令牌，当桶里没有令牌可取时，则拒绝服务。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import org.springframework.cloud.netflix.zuul.filters.support.FilterConstants; @Component public class RateLimiterFilter extends ZuulFilter { // 直接使用 guava 中的 RateLimiter 实现 private static final RateLimiter RATE_LIMITER = RateLimiter.create(100); @Override public String filterType() { return FilterConstants.PRE_TYPE; } @Override public int filterOrder() { // 限流是最高优先级，所以比最高优先级 -3 还要小 return FilterConstants.SERVLET_DETECTION_FILTER_ORDER - 1; } @Override public boolean shouldFilter() { return true; } @Override public Object run() throws ZuulException { // 如果没有拿到令牌 if (!RATE_LIMITER.tryAcquire()) { throw new RuntimeException(); } return null; } } Zuul 鉴权 在请求服务前，判断是否有权限访问。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @Component public class TokenFilter extends ZuulFilter { @Override public String filterType() { return FilterConstants.PRE_TYPE; } @Override public int filterOrder() { // 越小越靠前，放在 PRE_DECORATION_FILTER_ORDER 之前 return PRE_DECORATION_FILTER_ORDER - 1; } @Override public boolean shouldFilter() { return true; } // 需要定义的逻辑 @Override public Object run() throws ZuulException { RequestContext requestContext = RequestContext.getCurrentContext(); HttpServletRequest request = requestContext.getRequest(); // 需要根据实际情况，从 header / cookie 中获取信息 String token = ...; // 根据实际情况进行校验 if (...) { // 首先设置 zuul requestContext.setSendZuulResponse(false); // 设置返回信息 requestContext.setResponseStatusCode(HttpStatus.UNAUTHORIZED.value()); } return null; } } Zuul 跨域 在浏览器中的 ajax 请求是有同源策略的，如果违反了同源策略，就会有跨域问题。在 Zuul 中添加 CorsFilter 过滤器，是跨域问题的一种解决方案。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Configuration public class CorsConfig { @Bean public CorsFilter corsFilter() { final UrlBaseCorsConfigurationSource source = new UrlBaseCorsConfigrationSOurce(); final CorsConfiguration config = new CorsConfiguration(); config.setAllowCredentials(true); config.setAllowedOrigins(Collections.singletonList(\u0026#34;*\u0026#34;)); config.setAllowedHeaders(Collections.singletonList(\u0026#34;*\u0026#34;))； config.setAllowedMethods(Collections.singletonList(\u0026#34;*\u0026#34;)); config.setMaxAge(300L); source.registerCorsConfiguration(\u0026#34;/**\u0026#34;, config); return new CorsFilter(source); } } ","date":"2020-07-28T15:14:45+08:00","permalink":"https://emerywan.github.io/blog/p/spring-cloud-zuul/","title":"Spring Cloud Zuul 服务网关"},{"content":"Spring Cloud Config Spring Cloud Config 是一个解决分布式系统的配置管理方案。\nServer 提供配置文件的存储，以接口的形式提供配置文件的内容 Client 通过接口获取数据，并依据此数据初始化应用 Config Server 新建一个 git 仓库 在 git 服务器上创建一个仓库，用来存放配置文件。\n并在仓库中添加相应的配置文件。\nuser-dev.yml user-test.yml user-prod.yml 具体实现 添加依赖 1 2 3 4 5 6 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-config-server\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 添加注解 1 2 3 4 5 6 7 8 @EnableConfigServer @EnableDiscoveryClient @SpringBootApplication public class ConfigApplication { public static void main(String[] args) { SpringApplication.run(ConfigApplication.class, args); } } 配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 spring: application: name: config cloud: config: server: git: uri: http://github.com/xxx/confg # 配置文件仓库地址 username: ... password: ... searchpath: config-repo # git仓库地址下的相对地址，可配置多个 server: port: 8000 eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 通过接口查看配置 仓库中的配置文件会被转换成web接口，启动应用后，可在浏览器中查看配置文件（若配置文件的格式有错误，将无法访问）。\n如访问 http://localhost:8000/user/dev 即可返回 user-dev.yml 的配置信息。\n/{application}/{profile}[/{label}] /{application}-{profile}.yml /{label}/{application}-{profile}.yml profile：配置环境，label：仓库分支。\nConfig Client 在项目中创建 bootstrap.yml 在项目中，bootstrap.yml 会优先于 application.yml 加载。\napplication.yml 应用场景 主要用于 Spring Boot 项目的自动化配置。\nbootstrap.yml 应用场景 从额外的资源加载配置信息（如使用 Spring Cloud Config 时） 一些固定不能被覆盖的属性（具有高优先级，一般不会被本地配置或application中同名配置覆盖） 一些 加密/解密 的场景 具体实现 添加依赖 1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-config\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 spring: application: name: user # 该应用获取之前配置好的 user-dev.yml cloud: config: url: http://localhost:8000/ profile: dev label: master --- spring: cloud: config: discovery: enable: true # 启用服务发现 (Eureka) service-id: config # spring cloud config server 应用名称 profile: dev label: master 启动服务 启动服务时，即会先去 git 仓库获取配置信息。\n配置信息自动更新 当 git 仓库中的配置信息更新后，使用配置的客户端并不会自动更新配置。所以我们需要一些机制去触发配置的更新。\nactuator 添加依赖 1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 添加注解，打开更新机制 通过在需要加载更新配置的类上添加 @RefreshScope，当客户端通过触发 POST 方式的 /refresh 时，会自动将新的配置更新到相应的字段中。\n1 2 3 4 5 6 7 8 9 10 @RefreshScope // 该类中配置相关会自动刷新 @RestController public class ActuatorController { @Value(\u0026#34;${env}\u0026#34;) private String env; @RequestMapping(\u0026#34;/env\u0026#34;) public String env { return this.env; } } 配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # server 端添加 management: endpoints: web: exposure: include: \u0026#34;*\u0026#34; --- # client 端添加 management: endpoints: web: exposure: include: refresh 测试自动刷新 当 git 仓库中配置文件更新后，通过发送 POST 请求到 /refresh 后，客户端会自动获取最新配置。\n1 curl -v -X POST http://localhost:8080/actuator/refresh Spring Cloud Bus （推荐） 通过 spring cloud bus，通过 POST 请求 /bus-refresh，实现自动获取最新配置。\n至此两种消息代理：\nRabbitMQ Kafka WebHook WebHook 是当某个事件发生时，通过发送 http post 请求的方式来通知信息接收方。\n通过创建 WebHook 即可自动触发 POST 请求，让客户端动态刷新配置。\n","date":"2020-07-27T12:14:45+08:00","permalink":"https://emerywan.github.io/blog/p/spring-cloud-config/","title":"Spring Cloud Config 统一配置中心"},{"content":"Spring Cloud 服务通信 同步通信：\ndobbo 通过 RPC 远程调用。 spring cloud 通过 REST 接口调用。 异步通信：\n通过消息对列，如：RabbitMQ，Kafka，ActiveM 等。 本文主要介绍 Spring Cloud 使用 RestTemplate / OpenFeign 进行 REST 接口调用。\nRestTemplate 通过 RestTemplate 进行调用 1 2 3 4 5 6 7 8 9 public ProductInfo getProductMsg(String id) { RestTemplate restTemplate = new RestTemplate(); ProductInfo response = restTemplate.getForObject( \u0026#34;http://example.com/product/info\u0026#34;, // 远程调用地址 ProductInfo.class, // response 类型 id // 需要传递的参数 ); return response; } 利用 LoadBalancerClient 获取信息 1 2 3 4 5 6 7 8 9 10 11 12 @Autowired private LoadBalancerClient loadBalancerClient; public ProductInfo getProductMsg(String id) { RestTemplate restTemplate = new RestTemplate(); ServiceInstance serviceInstance = loadBalancerClient.choose(\u0026#34;PRODUCT\u0026#34;); // 利用 loadBalancerClient 通过应用名（spring.application.name）获取信息 String url = String.format(\u0026#34;http://%s:%s\u0026#34;, serviceInstance.getHost(), serviceInstance.getPort()) + \u0026#34;/product/info\u0026#34;; ProductInfo response = restTemplate.getForObject(url, ProductInfo.class, id); return response; } 利用 LoadBalance 在 RestTemplate 中直接使用应用名称 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class RestTemplateConfig { @Bean @LoadBalance public RestTemplate restTemplate() { return new RestTemplate(); } } --- @Autowired private RestTemplate restTemplate; public ProductInfo getProductMsg(String id) { // 利用 @LoadBalance 可以在 RestTemplate 中使用应用名称 ProductInfo response = restTemplate.getForObject(\u0026#34;http://PRODUCT/product/msg\u0026#34;, ProductInfo.class, id); return response; } OpenFeign （推荐） 引入依赖 1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-openfeign\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 添加启动注解 1 2 3 4 5 6 7 8 9 10 import org.springframework.cloud.openfeign.EnableFeignClients; @EnableFeignClients @SpringBootApplication @EnableDiscoveryClient public class FeignApplication { public static void main(String[] args) { SpringApplication.run(FeignApplication.class, args); } } 具体实现 现在有两个服务，分别为 Prodcut 和 Order 。 需求： Order 服务中，客户进行了下单操作后，调用 Product（Feign） 的进行减库存操作。\nProduct 服务中，定义远程调用端。 1 2 3 4 5 6 7 8 9 10 11 12 public class DecreaseStockInput { // ... } --- @FeignClient(name = \u0026#34;product\u0026#34;) // name: 远程服务名(Spring.application.name) public interface ProductClient { @RequestMapping(value = \u0026#34;/product/decrease_stock\u0026#34;) void decreaseStock(@RequestBody List\u0026lt;DecreaseStockInput\u0026gt; decreaseStockInputList); } Order 服务中，对 Product Client 进行调用。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Service public class OrderServiceImpl implements OrderService { // 注入的为 Product 中的 ProductClient // 通过依赖的方式 @Autowired private ProductClient productClient; public OrderDTO create(OrderDTO orderDTO) { // ... // 调用 Product 服务中的 api 进行减库存操作 productClient.decreaseStock(decreaseStockInputList); // ... } } ","date":"2020-07-26T21:14:45+08:00","permalink":"https://emerywan.github.io/blog/p/spring-cloud-openfeign/","title":"Spring Cloud 服务通信"},{"content":"Eureka 找到啦！ 服务注册与发现\n简介 Spring Cloud 封装了 Netflix 公司开发的 Eureka 模块来实现服务注册和发现。采用了 C-S 的设计架构。用来简化与服务器的交互、作为轮询负载均衡器，并提供服务的故障切换支持。 由两个组件组成。 Ereka Server。 注册中心 Ereka Client。 服务注册 Eureka 采用了客户端发现的方式，在服务运行时，通过(轮训、hash等负载均衡机制等方式)注册中心找到需要服务（即 A 通过 注册中心 找 B，需要谁找谁）。\nEureka Server 注册中心记录着所有应用的信息和状态(如：应用名，所在服务器，是否正常工作)。\n实现一个注册中心 1. 引入依赖 1 2 3 4 5 6 7 8 9 10 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-eureka-server\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 2. 添加启动注解 1 2 3 4 5 6 7 @EnableEurekaServer @SpringBootApplication public class EurekaServerApplication { public static void main(String[] args) { SpringApplication.run(EurekaServerApplication.class, args); } } 3. 配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 spring: application: name: eureka-server Server: port: 8761 eureka: client: register-with-eureka: false # 是否将自己注册到Eureka Server，作为 Server 端不需要 fetch-registry: false # 是否从Eureka Server获取注册信息，作为 Server 端不需要 service-url: # 接收的是一个 Map 结构 defaultZone: http://localhost:8761/eureka/ 4. 启动程序 启动后，访问 http://localhost:8761/，即可看到 Spring Eureka 界面。\n实现 Eureka 集群 在一个分布式系统中，服务注册中心是最重要的基础部分，理应随时处于可以提供服务的状态。为了维持其可用性，通常会采用集群的方案。Eureka通过互相注册的方式来实现高可用的部署\n双节点注册 创建两台服务器，端口分别为 8761 和 8762。\n将 8761 的服务器配置指向 8761，将 8762 的服务器指向 8761。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 spring: application: name: eureka-server Server: port: 8761 eureka: client: register-with-eureka: false fetch-registry: false service-url: # 将 service-url 指向 8762 defaultZone: http://localhost:8762/eureka/ --- spring: application: name: eureka-server Server: port: 8762 eureka: client: register-with-eureka: false fetch-registry: false service-url: # 将 service-url 指向 8761 defaultZone: http://localhost:8761/eureka/ 启动程序后，通过 http://localhost:8761/ 和 http://localhost:8762/ 都可访问 Eureka 界面。并且可以看到另一个节点信息节点的信息。\n多节点注册 在生产中我们需要三台或者大于三台的注册中心来保证服务的稳定性，配置的原理其实都一样：将注册中心分别指向其它的注册中心。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 --- spring: application: name: eureka-server server: port: 8761 eureka: client: register-with-eureka: false fetch-registry: false serviceUrl: defaultZone: http://localhost:8762/eureka/,http://localhost:8763/eureka/ --- spring: application: name: eureka-server server: port: 8762 eureka: client: register-with-eureka: false fetch-registry: false serviceUrl: defaultZone: http://localhost:8761/eureka/,http://localhost:8763/eureka/ --- spring: application: name: eureka-server server: port: 8763 eureka: client: register-with-eureka: false fetch-registry: false serviceUrl: defaultZone: http://localhost:8761/eureka/,http://localhost:8762/eureka/ Eureka Client 实现一个服务注册 1. 依赖配置 1 2 3 4 5 6 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-eureka\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 2. 添加启动注解 1 2 3 4 5 6 7 8 9 import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer; @EnableDiscoveryClient @SpringBootApplication public class ClientApplication { public static void main(String[] args) { SpringApplication.run(ClientApplication.class, args); } } 3. 配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 spring: application: name: eureka-client server: port: 8080 eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ # instance: # 自定义链接 # hostname: example.com 4. 启动程序 启动程序后，进入 Eureka 页面 http://localhost:8761/eureka/，即可看到注册的服务 eureka-client。\n总结 分布式系统中，服务注册中心是最重要的基础部分 @EnableEurekaServer @EnableEurekaClient 具有 心跳检测，健康检查，负载均衡等功能 为保证高可用，建议集群部署 ","date":"2020-07-25T13:14:45+08:00","permalink":"https://emerywan.github.io/blog/p/spring-cloud-eureka/","title":"Spring Cloud Eureka 服务注册与发现"},{"content":"警告：该教程仅为个人记录，该笔记本安装涉及解锁BIOS，存在一定风险，如您使用该教程对计算机进行更改，所造成的的任何后果我概不负责。\n写在最前 开学后太忙，并且电脑我换了块更大的硬盘，这台电脑我不打算再安装黑苹果了，本教程可能会不再更新，最新上传的配置文件中，添加了对 type-C 4K 30Hz 的支持，4K 60Hz 显示器会黑屏，无法输出信号。\n如果想要使用 Clover 安装 10.15 或以下系统的话，可以依旧按照此教程（请将 Clover 和 kext 选择到适应版本）。\n如果打算安装新系统使用的话，建议使用 Opencore 安装。推荐参考 RazerBlade15-Base-Model-Hackintosh_macOS_Monterey，该教程写的非常详细，感谢他的付出。\n建议不要使用 东芝（铠侠）的固态硬盘，我的硬盘为东芝 tr200，在 macOS 中莫名卡顿，我的东芝U盘在 macOS 中也莫名卡顿，其他系统没有什么问题。（玄学问题？）\n祝你玩得愉快！\nGithub ➡️\n因为疫情原因春节一直宅在家，学校假期也延长了，找到了同款笔记本的教程，所以入坑安装黑苹果，最近把步骤整理了一下。\n安装过程我主要参考了 这篇 和 这篇 教程，感谢他们的辛苦付出。部分内容为他们所写教程的汉化，详细或精简，我的水平有限，刚接触黑苹果，建议同时参考他们的教程。\n硬件介绍 结果介绍 解锁BIOS 安装前准备 系统安装 DSDT，SSDT制作 网卡 一些优化 参考 更新 [1] 硬件介绍 型号 最终情况 CPU i7-8750H 可用 GPU Nvdia 1060 Max-Q 除 10.13 High Sierra 安装 WebDriver 外不可用 硬盘 更换了 金士顿 A2000 可用 网卡 9560NGW WIFI 目前无解，蓝牙可用 显示器 1080P 可用 摄像头 可用 扬声器 可用 耳机 无法检测到麦克风 麦克风 不可用，已识别，但在设置中未看见输入电平 触控板 手势可用（反应稍慢） HDMI 接口 直通显卡，除安装 High Sierra 外不可用 Mini DP 接口 直通显卡，除安装 High Sierra 外不可用 雷电3 被识别成 USB3.1，可外接拓展坞外接显示器，我的电脑中需要删除SSDT-12-OptTabl.aml [2] 安装结果 一些小问题 我也是刚刚接触黑苹果，很多问题我也无法解决，有谁了解的话希望能帮助一下，感谢。\n麦克风无法使用，系统能找到但无法使用，耳机麦克风无法找到。想要使用的话只能通过蓝牙耳机了。\n耳机麦克无法识别。\n输出设备默认识别到了扬声器和耳机（即使未插入耳机），无法自动切换，需手动切换。\n更新 今天本来想根据 这篇文章 尝试自己定制一下 AppleALC ，当我把有效节点和路径弄完之后，准备下载 AppleALC-DEBUG 编译的时候，没想到最新版本已经添加了这个笔记本的 layout-id:23。\n请按照如图修改，保存后重启。我的电脑耳机麦克风无法识别（我在 Ubuntu 下也无法找到耳机麦克风的有效节点信息）\n添加 type-c 输出 4k，只能支持到最高 30Hz，输出 60Hz 会直接黑屏。可以安装一个 RDM 进行管理。\n[3] 解锁BIOS 解锁BIOS，存在一定风险，如您使用该教程对计算机进行更改，所造成的的任何后果我概不负责！！！\n雷蛇国内官网没有提供驱动和BIOS的下载，如有需要，需要访问美国官网。点我。\n该笔记本 DVMT 预分配默认为 32MB，不足以启动 MacOS，在 BIOS 中该设置项默认隐藏，所以要提取本机 BIOS 并且进行解锁，将 DVMT 预分配默认设置为 64MB（1080P），分辨率更高请分配更大空间。\n建议在 windows 下操作。\n[3-1] 提取本机 BIOS 注意备份好。\n打开 AFUWINGUI.exe，点击 Save 按钮，导出本机当前 BIOS 。\n[3-2] 修改 BIOS 打开 AMIBCP.exe ，点击 File -\u0026gt; open 打开导出的 BIOS。 如图，在左侧选择 / -\u0026gt; Setup -\u0026gt; Chipset，将左侧的 System Agent Configuration 的 Access 由 Default 修改为 USER 修改完后点击 File -\u0026gt; Save as。重命名为新的 BIOS。 [3-3] 刷入新 BIOS ！！！ 注意，该过程虽然简单，但有一定风险，造成的任何结果与本人无关。\n重新打开 AFUWINGUI.exe，点击 Open 打开刚刚修改后的 BIOS。\n尽可能的退出其他程序，尽量保持后台干净，再点击 Flash 刷入新的 BIOS。\n重启 [4] 安装前准备 [4-1] 准备macOS Catalina 安装盘 推荐使用黑果小兵制作的镜像，使用 TransMac 制作（软件在文件夹中已提供）。这里是10.15.3的镜像。\n如果您要安装更新的系统，请升级 CLOVER，和 kexts/ 到对应兼容或更新的版本，可将制作好的安装盘中 EFI/CLOVER 的文件进行同名替换。（未来的新版本可能不可预知的问题，请酌情升级）。\n[4-2] 启动盘制作 请参考，或自行搜索，网上教程很多。点我。\n[5] 系统安装 [5-1] BIOS 设置 Advanced\nThunderbolt(TM) Configuration Security Level 设置成 No Security Chipset\nSystem Agent (SA) Configuration Graphics Configuration DVMT Pre-Allocated 设置成 64 DVMT Total Gfx Mem 设置成 MAX Security\nSecure Boot 设置成 Disabled Boot\nFast Boot 设置成 Disabled\nCSM Configuration\nCSM Support 设置成 Disabled [5-2] 安装过程 系统安装过程大致相同，选择U盘启动后进入安装。安装过程会重启几次。\n可自行搜索，参考其他人的步骤。\n[5-3] 安装时可能出现的问题 显示程序副本已损坏\n断网 打开终端 修改时间为系统发布对应的时间。 如修改为 2019年。输入 date 000000002019。\n[6] DSDT，SSDT制作 通过修补DSDT，SSDT驱动触控板，音频，电池状态，亮度控制等。\n[6-1] 准备修补 [6-1-1]\n开机在 Clover 引导界面中按 F4，所需文件会加载到 EFI/Clover/ACPI/origin 中。通过 Clover Configurator 挂载启动的 EFI（通过U盘启动就挂载U盘）。\n[6-1-2]\n将 origin 文件夹复制到桌面，同时将 iasl 软件复制到文件夹中。\n[6-1-3]\n打开终端\n1 2 3 cd ~/Desktop mkdir patched ./origin/iasl -da -dl DSDT.aml [6-1-4]\n打开 origin，使用 MaciASL 打开生成的 DSDT.dsl 文件。点击 Compile，确保没有错误。（默认应该没有 error，但有很多 warning，warning 不必关系，若有 error 请将 error 处代码注释或删除）\n[6-1-5]\n确保没有 errors 后，点击 Patch。\n[6-2] 修复电池 [1] 在弹窗的左侧点击 _RehabMan Laptop/[bat]Razer Blade (2014) ，等待右侧进行匹配后点击 Apply。\n如果网络不好的话可能无法加载（github），请切换到合适的网络，或访问 这里，或复制以下代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 #Maintained by: RehabMan for: Laptop Patches #battery_Razer-Blade-2014.txt # created by sidelia 2016-01-17 # changes for Razer Blade Stealth (Kaby Lake) by BlenderSleuth (minor fixes by RehabMan) # works for: # Razer Blade (2014) # Razer Blade Stealth (Kaby Lake), per BlenderSleuth # Razer Blade (14\u0026#34;, late 2016) # Razer Blade Pro (2017) # Razer Blade 15 (2018), per JomanJi/blodtanner into method label B1B2 remove_entry; into definitionblock code_regex . insert begin Method (B1B2, 2, NotSerialized) { Return(Or(Arg0, ShiftLeft(Arg1, 8))) }\\n end; into device label EC0 code_regex BIF1,\\s+16, replace_matched begin IF10,8,IF11,8, end; into device label EC0 code_regex BIF2,\\s+16, replace_matched begin IF20,8,IF21,8, end; into device label EC0 code_regex BIF3,\\s+16, replace_matched begin IF30,8,IF31,8, end; into device label EC0 code_regex BIF4,\\s+16, replace_matched begin IF40,8,IF41,8, end; into device label EC0 code_regex BST0,\\s+16, replace_matched begin ST00,8,ST01,8, end; into device label EC0 code_regex BST1,\\s+16, replace_matched begin ST10,8,ST11,8, end; into device label EC0 code_regex BST2,\\s+16, replace_matched begin ST20,8,ST21,8, end; into device label EC0 code_regex BST3,\\s+16, replace_matched begin ST30,8,ST31,8, end; into method label _BIF code_regex \\^\\^EC0\\.BIF1, replaceall_matched begin B1B2(^^EC0.IF10,^^EC0.IF11), end; into method label _BIF code_regex \\^\\^EC0\\.BIF2, replaceall_matched begin B1B2(^^EC0.IF20,^^EC0.IF21), end; into method label _BIF code_regex \\^\\^EC0\\.BIF3, replaceall_matched begin B1B2(^^EC0.IF30,^^EC0.IF31), end; into method label _BIF code_regex \\^\\^EC0\\.BIF4, replaceall_matched begin B1B2(^^EC0.IF40,^^EC0.IF41), end; into method label _BST code_regex \\^\\^EC0\\.BST0, replaceall_matched begin B1B2(^^EC0.ST00,^^EC0.ST01), end; into method label _BST code_regex \\^\\^EC0\\.BST1, replaceall_matched begin B1B2(^^EC0.ST10,^^EC0.ST11), end; into method label _BST code_regex \\^\\^EC0\\.BST2, replaceall_matched begin B1B2(^^EC0.ST20,^^EC0.ST21), end; into method label _BST code_regex \\^\\^EC0\\.BST3, replaceall_matched begin B1B2(^^EC0.ST30,^^EC0.ST31), end; # added for Razer Blade 15 (2018), per JomanJi into device label EC0 code_regex BIF0,\\s+16, replace_matched begin IF00,8,IF01,8, end; into method label _BIF code_regex \\(\\^\\^EC0.BIF0, replaceall_matched begin (B1B2(\\^\\^EC0.IF00,\\^\\^EC0.IF01), end; # utility methods to read/write buffers from/to EC into method label RE1B parent_label EC0 remove_entry; into method label RECB parent_label EC0 remove_entry; into device label EC0 insert begin Method (RE1B, 1, NotSerialized)\\n {\\n OperationRegion(ERAM, EmbeddedControl, Arg0, 1)\\n Field(ERAM, ByteAcc, NoLock, Preserve) { BYTE, 8 }\\n Return(BYTE)\\n }\\n Method (RECB, 2, Serialized)\\n // Arg0 - offset in bytes from zero-based EC\\n // Arg1 - size of buffer in bits\\n {\\n ShiftRight(Add(Arg1,7), 3, Arg1)\\n Name(TEMP, Buffer(Arg1) { })\\n Add(Arg0, Arg1, Arg1)\\n Store(0, Local0)\\n While (LLess(Arg0, Arg1))\\n {\\n Store(RE1B(Arg0), Index(TEMP, Local0))\\n Increment(Arg0)\\n Increment(Local0)\\n }\\n Return(TEMP)\\n }\\n end; # buffer fields into device label EC0 code_regex (ECCM,)\\s+(256) replace_matched begin ECCX,%2,//%1%2 end; into method label _BIF code_regex \\(\\^\\^EC0.ECCM, replaceall_matched begin (^^EC0.RECB(0x60,256), end; [2] 点击 Compile，确保没有错误。（默认情况下没有，不同版本BIOS可能情况不同）。\n[6-4] 修复重启保存背光亮度 [6-4-1]\n在左侧菜单栏向下滑动，找到 [gfx0] Disable/Enable on _WAK/_PTS (DSDT)，点击都单击 Apply。\n网络不好可点击 这里 或复制以下代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #Maintained by: RehabMan for: Laptop Patches #graphics_PTS_WAK-disable.txt # # The purpose of this patch is to add code to to _WAK # that disables Radeon/nvidia on wake and add code # to _PTS that enables it on _PTS. # # The path of _OFF may have to be customized to match your SSDTs # The patch attempts to identify the correct _REG by using # the ACPI PNP identifier for the EC. # # Use this patch if you experience trouble shutting down # or restarting your laptop when disabling nvida/radeon. # into method label _PTS code_regex ([\\s\\S]*) replace_matched begin External(\\\\_SB.PCI0.PEG0.PEGP._ON, MethodObj)\\n If (CondRefOf(\\\\_SB.PCI0.PEG0.PEGP._ON)) { \\\\_SB.PCI0.PEG0.PEGP._ON() }\\n %1 end; into method label _WAK code_regex (Return\\s+\\(.*) replace_matched begin External(\\\\_SB.PCI0.PEG0.PEGP._OFF, MethodObj)\\n If (CondRefOf(\\\\_SB.PCI0.PEG0.PEGP._OFF)) { \\\\_SB.PCI0.PEG0.PEGP._OFF() }\\n %1 end; [6-4-2] 按 command + F 搜索 Device (ALSD)，找到如图代码，将其替换为以下代码。\n1 2 3 4 5 6 7 8 9 10 Device (_SB.ALS0) { Name (_HID, \u0026#34;ACPI0008\u0026#34;) // _HID: Hardware ID Name (_CID, \u0026#34;smc-als\u0026#34;) // _STA: Status Name (_ALI, 300) // _ALI: Ambient Light Illuminance Name (_ALR, Package () // _ALR: Ambient Light Response { Package () { 100, 300 }, }) } [6-3] 修复触控板 灵刃 15 的标准版和精英版使用的触控板不同，请根据自己的电脑进行选择修复方案。\n[6-3-1] 标准版 [6-3-1-1]\n继续搜索 SSCN。找到 Scope 为 _SB.PCI0.I2C0 下的 SSCN 方法。复制 SSCN 与 FMCN（在 SSCN 下方）这 两个方法。并将这两个方法如图重命名（也可选择删除）。\n重命名为：\n[6-3-1-3]\n搜索 TPD0。将之前剪切的两个方法放到 _INI 方法后。\n[6-3-1-4]\n向下找到如下代码。\n将其更改为如图。\n[6-3-2] 精英版 [6-3-2-1]\n在 Patch 页面中粘贴以下代码代码，点击 Apply。\n[6-3-2-2]\n点击 Compile 进行编译确定无 error（默认没有）。\n1 2 3 4 5 6 7 into method label _STA parent_label GPI0 replace_content begin Return (0x0F) end; into_all method label _CRS parent_label TPD0 replace_content begin ConcatenateResTemplate (SBFB, SBFI) end; [6-4] 保存修改好的 DSDT.aml 点击 File -\u0026gt; save as 。\nFile Format 选择 ACPI Machine Language Binary。命名为 DSDT.aml。存入桌面中的 parched 文件夹中。\n[6-5] 屏蔽 Nvdia 显卡 如果你选择安装 High Sierra 安装 WebDriver 使用 Nvidia 显卡的话，不用该补丁。 [点击这里查看支持驱动的 High Sierra ](https : //www.tonymacx86.com/nvidia-drivers/)\n在的笔记本上使用该补丁会导致 type-c 转视频接口无信号，无法拓展显示器，若出现相同情况请删除该补丁。 [6-5-1]\n再次进入 origin 文件夹中，在终端输入\n1 ./origin/iasl -da -dl SSDT-12-OptTabl.aml [6-5-2]\n根据上方修补电池状态，触控板的方式类似，使用 MaciASL 打开 SSDT-12-OptTabl.dsl\n[6-5-3]\n按 command + F 搜索以下代码\n1 Method (_OFF, 0, Serialized) // _OFF: Power Off [6-5-4]\n在该代码上方，粘贴以下代码\n1 Method (_INI) {_OFF() } // added to call _OFF [6-5-5]\n点击 patch，将以下代码粘贴到弹窗中，点击 apply。\n1 2 3 4 5 6 into method label _INI parent_label \\_SB.PCI0.GFX0 insert begin //added to turn nvidia/radeon off\\n External(\\_SB.PCI0.PEG0.PEGP._OFF, MethodObj)\\n \\n end; [6-5-6]\n点击编译，出现一个错误。\n[6-5-7]\n搜索一下代码，并将其删除，再次编译。\n1 External (_SB_.PCI0.PEG0.TGPC, IntObj) // (from opcode) [6-5-8]\n点击 File -\u0026gt; Save As。将最终的 /aml 文件保存。\n[6-6] 制作 SSDT-USBX.aml 如果想制作自己的 SSDT-USBX.aml。请参考 点我。\n使用 USBMap。点我\n[6-7] 复制提供的的 .aml 文件 将文件夹中的 SSDT-PNLF.aml，SSDT-UIAC-ALL.aml，SSDT-USBX.aml，SSDT-XOSI.aml 同上面修补的两个文件一同放入 patched 文件夹中，最后 patched 文件夹中应该有如下6个文件。\n如果 type-c 转视频接口无信号，请删除SSDT-12-OptTabl.aml\n[7] 网卡 [7-1] 更换博通网卡 在网上找过拆机图，网卡附近的位置还是挺多的，我的选择是拆机的 BCM94360cs2 + 转接卡，可直接免驱使用。\n相比使用原装的网卡位置稍有点高，压在一根的排线上，但是不影响，如选择同款网卡，请注意绝缘，建议上螺丝的时候不要拧太紧，不松动即可。装上之后的效果如图。\n**小提示: **拧螺丝前最好把易碎贴给清理干净。这个贴纸分量太足，卡在螺丝孔中导致一直滑丝。\n**使用效果: **2.4G WIFI 和 蓝牙有干扰，尤其是 2.4G WIFI 使用带宽高的时候，蓝牙几乎不能用。其他使用场景基本良好。\n想折腾的话可选择 DW1820A，可参考 这里。\n博通 BCM94352Z ，现在价钱被炒的很高，目前将近 300，有钱随意。\n[7-2] 使用自带网卡 Intel 蓝牙默认免驱，WiFi 目前无解。\n蓝牙从 windows 重启进入 macOS 可使用（网卡未断电所以上传了驱动）。\n将 该驱动 放入 EFI/CLOVER/kexts，可以实现冷启动驱动自带网卡蓝牙。\n**使用效果: **蓝牙键盘，蓝牙音箱没有问题，蓝牙鼠标貌似不能用。\n[7-3] 使用USB网卡 usb 网卡驱动安装。点我。\nCOMFAST CF-WU815N 150M 单频 COMFAST CF-811AC 650M 双频 COMFAST CF-812AC 1300M 双频 更多其他型号自行搜索 [8] 一些优化 [8-1] HIDPI 开启 HIDPI 后可能会导致开机第二阶段 Logo 变大，因为分辨率是仿冒的，不影响使用。\n使用终端执行：\n1 sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/xzhih/one-key-hidpi/master/hidpi.sh)\u0026#34; 选择 \u0026ldquo;开启 HIDPI\u0026rdquo;\n显示的 ICON 选择 Macbook Pro（在设置界面显示的样式）\n选择分辨率配置 1080P 显示器（根据自身情况选择）\n更多详细情况可参考这篇文章。点我。\n[8-2] 打开 TRIM 如果使用 SSD，一定要打开 TRIM，防止系统多次擦写，确保硬盘寿命。\n1 sudo trimforce enable 完成后系统会进行一次重启。\n[8-3] 禁用睡眠 在终端运行以下命令，并在 设置 -\u0026gt; 节能 中关闭相应设置。\n1 2 3 4 5 sudo pmset -a hibernatemode 0 sudo rm /var/vm/sleepimage sudo mkdir /var/vm/sleepimage [8-4] “洗白”序列号 网络上已经有很多教程，自行搜一下。\n参考 https://github.com/stonevil/Razer_Blade_Advanced_early_2019_Hackintosh https://www.tonymacx86.com/threads/guide-razer-blade-15-2018-detailed-install-guide-high-sierra-10-13-6-17g2208-17g5019.264017/ https://blog.daliansky.net/ ","date":"2020-02-27T21:14:45+08:00","permalink":"https://emerywan.github.io/blog/p/razer-blade-base-hackintosh/","title":"雷蛇 灵刃 15 标准版 2018 黑苹果"}]