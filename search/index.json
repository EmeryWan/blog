[{"content":"🌱 介绍 许多平台都提供了静态页面托管的服务，如 Github Pages，Vercel，Netlify等。但在国内由于一些“原因”，这些国外的服务在国内的访问并不稳定。\n国内的托管平台如 Gitee 限制很多，不可以自定义域名而且之前出现了防盗链问题，作为托管博客不太推荐。\n腾讯云推出的云开发 CloudBase 也有静态页面托管服务，虽然是付费，但是在按量付费的情况下资费不是很高，在博客访问量不是很高的情况下十分适合。\n目前在腾讯云中暂时还没有 Hugo 的模板。目前有两种方式可以达到自动部署的功能：\n  🌰 使用 Github Actions 通过 Tencent CloudBase Github Action 自动部署到 CloudBase。\n  🌰 使用 Github Actions 推送到 Web 应用托管（webify） 的简易静态页面模板。\n  🏖 使用 CloudBase 使用 CloudBse 时，使用按量计费环境会有一些免费用量。\n☁️ 腾讯云 创建环境 在 云开发 CloudBase 新建一个应用，选择 空模板，根据自身需求填写信息。\n创建成功后，获得 环境ID。\n获取 API 密钥 为部署新建一个密钥对。在 访问管理 -\u0026gt; 用户列表 -\u0026gt; 新建用户 -\u0026gt; 自定义创建 -\u0026gt; 可访问资源并接收消息。🔗 传送门\n根据自己的需要，新建用户名后选择 编程访问，点击下一步。\n在自定义策略中勾选：\n QcloudAccessForTCBRole：授予云开发（TCB）对云资源的访问权限； QcloudAccessForTCBRoleInAccessCloudBaseRun：供云开发（TCB）服务角色（TCB_QcsRole）进行关联，用于 TCB 访问其他云服务资源。包含私有网络 VPC、云服务器 CVM 相关操作权限。  点一下一步后，新建用户成功。可以获得 SecretId 和 SecretKey。\n⚙️ Github Actions 设置 Github Secrets 在项目的 Settings -\u0026gt; Secrets -\u0026gt; Actions 中添加上述得到的 ENV_ID，SECRET_ID，SECRET_KEY（名称可以自定义）。\n添加 workflows 可以在仓库的 Actions 中 new workflow，或者在项目中 .github/workflows 添加。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  name:Tencent CloudBaseon:push:branches:- mainjobs:hugo-publish:name:publish content to static websiteruns-on:ubuntu-lateststeps:- name:Checkoutuses:actions/checkout@v3with:submodules:truefetch-depth:0- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:latestextended:true- name:Buildrun:hugo --minify --gc# 使用云开发 Github Action 部署- name:Deploy to Tencent CloudBaseuses:TencentCloudBase/cloudbase-action@v2with:secretId:${{ secrets.QCLOUD_SECRET_ID }}secretKey:${{ secrets.QCLOUD_SECRET_KEY }}envId:${{ secrets.QCLOUD_ENV_ID }}   🚧 提示：\n这里使用了 TencentCloudBase/cloudbase-action@v2，需要在项目根目录添加 cloudbaserc.json。\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  { \u0026#34;envId\u0026#34;: \u0026#34;{{env.ENV_ID}}\u0026#34;, // 这里需要更改为你的 环境ID，或者在 .env 文件中配置  \u0026#34;$schema\u0026#34;: \u0026#34;https://framework-1258016615.tcloudbaseapp.com/schema/latest.json\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;framework\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hugo-blog\u0026#34;, \u0026#34;plugins\u0026#34;: { \u0026#34;client\u0026#34;: { \u0026#34;use\u0026#34;: \u0026#34;@cloudbase/framework-plugin-website\u0026#34;, \u0026#34;inputs\u0026#34;: { \u0026#34;outputPath\u0026#34;: \u0026#34;public\u0026#34;, \u0026#34;ignore\u0026#34;: [ \u0026#34;.git\u0026#34;, \u0026#34;.github\u0026#34;, \u0026#34;cloudbaserc.js\u0026#34; ] } } } } }    🚧 提示：\n这里也可以选择  Tencent CloudBase Github Action V1。\nV2 比 V1 有更多功能，比如生成静态文件，但目前没有 Hugo 模板，都需要在 Github Actions 生成今天文件，所以目前来说，没什么区别，甚至 V1 更简洁一点。\n 1 2 3 4 5 6 7 8  # 将上述 jobs 内替换为- name:Deploy to Tencent CloudBaseuses:TencentCloudBase/cloudbase-action@v1.1.1with:secretId:${{ secrets.QCLOUD_SECRET_ID }}secretKey:${{ secrets.QCLOUD_SECRET_KEY }}envId:${{ secrets.QCLOUD_ENV_ID }}staticSrcPath:public  🏝 使用 webify 使用 Web 应用托管（webify）主要是利用 Github Actions 生成 静态页面到另一个分支，再托管这个分支的内容。\n☁️ 创建服务 在 Web 应用托管 -\u0026gt; 新建应用 新建一个简易静态页面模板。根据需求填写信息。选择纯静态页面。\n在 应用列表 -\u0026gt; 应用设置 中配置仓库信息，并根据自身情况选择部署分支。（没有生成静态页面分支，可完成后面操作后再进行此步骤）。\n⚙️ Github Actions 设置 Github Token 在用户的 Settings -\u0026gt; Developer settings -\u0026gt; Personal access tokens -\u0026gt; Generate new token 获取一个 Repo Token。🔗 传送门\n在项目的 Settings -\u0026gt; Secrets -\u0026gt; Actions 中添加上述得到的 Token。\n添加 workflows 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  name:Tencent CloudBaseon:push:branches:- mainjobs:hugo-publish:name:publish content to static websiteruns-on:ubuntu-lateststeps:- name:Checkoutuses:actions/checkout@v3with:submodules:truefetch-depth:0- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:latestextended:true- name:Buildrun:hugo --minify --gc- name:Deploy to Branchuses:peaceiris/actions-gh-pages@v3with:github_token:${{ secrets.ACCESS_TOKEN }}keep_files:falsepublish_branch:gh-pages # 更改为你想要生成的分支publich_dir:./publiccommit_message:${{ github.event.head_commit.message }}  ⛓ 参考  🔗 https://blog.wangjunfeng.com/post/hugo-cloudbase/ 🔗 https://cloud.tencent.com/document/product/1210/43389 🔗 https://github.com/TencentCloudBase/cloudbase-action 🔗 https://github.com/TencentCloudBase/cloudbase-action/blob/3354b442713265aa9d7c5bf03b0b8cb0173f546f/README.md  ","date":"2022-05-01T21:14:45+08:00","permalink":"https://emerywan.github.io/blog/p/hugo-in-cloudbase/","title":"使用 Github Actions 在腾讯云 CloudBase 部署 Hugo"},{"content":"这一节，将介绍 LSTM (Long Shorter Term Memory)，以及用 pytorch 实现 LSTM 。\nLSTM 是一种 RNN 模型，是对 simple RNN 的改进，LSTM 可以避免梯度消失的问题，可以有更长的记忆。LSTM 的论文在 1997 年发表。\n Hochreiter and Schmidhuber.\tLong short-term\tmemory. Neural computation, 1997.\n 🔖 LSTM LSTM 也是一种循环神经网络，原理跟 simple RNN 差不多，每当读取一个新的输入 $x$，就会更新状态 $h$。\nLSTM 的结构比 simple RNN 要复杂很多，simple RNN 只有一个参数矩阵， LSTM 有四个参数矩阵。接下来我们具体来看看 LSTM 的内部结构。\n🚠 传送带 LSTM 最重要的设计是这个传送带 Conveyor belt，即为向量 $C$。过去的信息通过传送带，直接送到下一个时刻，不会发生太大的变化。LSTM 就是靠传送带来避免梯度消失的问题。\nLSTM 中有很多个门 gate，可以有选择的让信息通过。\n🚪 Forgate Gate 首先介绍 forget gate 遗忘门。遗忘门由 ☘️ sigmoid 函数，和 🍀 元素积 element wise multiplication 两部分组成。\n🌼 输入 sigmoid 的是一个向量 $a$，sigmoid 作用到向量 $a$ 的每一个元素上，把每一个元素都压到 0 和 1 之间。\n举个例子，假如向量 $a$ 是：[1, 3, 0, -2]，那么，sigmoid 函数将分别作用在这四个元素上。然后分别输出：[0.73, 0.95, 0.5, 0.12] 。\n输入的向量 $a$，与输出的向量 $f$ 应该有相同的维度，这个例子里，向量 $a$ 是四维的，向量 $f$ 也会是四维的。\n🌸 算出 $f$ 向量之后，计算传送带向量 $c$ 与遗忘门向量 $f$ 的元素积。元素积 element wise multiplication 是这样算的：\n$c$ 和 $f$ 都是四维的向量，将它们的每一个元素分别相乘。所以元素积的结果也是个四维的向量。\n这个遗忘门 $f$，有选择的让传送带 $c$ 的值通过：\n  🌰 假如 $f$ 的一个元素是 $0$，那么 $c$ 对应的元素不能通过，对应的输出是 $0$；\n  🌰 假如 $f$ 的一个元素是 $1$，那么 $c$ 对应的元素就全部通过，对应的输出是 $c$ 本身。\n  遗忘门 $f$ 具体是这么算出来的：首先看这张结构图，$f_t$ 是上一个状态 $h_{t-1}$，与当前输入 $x$ 的函数。\n把状态 $h_{t-1}$ 与输入 $x_t$ 做拼接 concatnation，得到更高维度的向量。然后计算矩阵 $w_f$ 与这个向量的乘积，得到一个向量，再用 sigmoid 函数，得到向量 $f_t$，$f_t$ 的每一个元素都介于 0 和 1 之间，遗忘门有一个参数矩阵 $w_f$，需要通过 反向传播 从训练数据里学习。\n🚪 Input Gate 刚才讲了遗忘门，现在来看一看 input gate 输入门。在这张结构图里，输入门 $i_t$，依赖于旧的状态向量 $h_{t-1}$，和新的输入 $x_t$。\n输入门 $i_t$ 的计算类似于遗忘门，把旧的状态 $h_{t-1}$，与新的输入 $x_t$ 做拼接 concatnation，得到更高维的向量。\n然后计算矩阵 $w_i$ 与这个向量的乘积得到一个向量，最后使用激活函数 sigmod，得到向量 $i_t$（$i_t$ 的每一个元素都介于0和1之间）。\n输入门也有自己的参数矩阵，计作 $w_i$，$w_i$ 也需要从训练数据中学习。\n🆕 新的输入 还需要计算新的输入值 new value $\\widetilde{C}_t$，$\\widetilde{C}t$ 是个向量，计算方法跟遗忘门和输入门都很像。也是把旧状态 $h{t-1}$，与新输入 $x_t$ 做拼接，再乘到参数矩阵上。\n区别在于激活函数不是 sigmoid，而是双曲正切函数 tanh，所以算出的向量 $\\widetilde{C}_t$ 的元素都介于 (-1, +1)。\n计算 new value $\\widetilde{C}_t$，也需要一个单独的参数矩阵矩阵 $w_c$。\n我们已经算出了遗忘门 $f_t$，输入门 $i_t$，以及新的输入值 $\\widetilde{C}t$，我们还知道传送带旧的值 $c{t-1}$，现在可以更新传送带 $c$ 了。\n计算遗忘门 $f_t$ 和传送带旧的值 $c_{t-1}$ 的元素积。\n遗忘门 $f_t$，和传送带 $c_{t-1}$ 是维度相同的向量，算出的乘积也是个向量。遗忘门 $f_t$，可以选择性的遗忘 $c_{t-1}$ 中的一些元素，如果 $f_t$ 中的一个元素是 0，那么 $c_{t-1}$ 相应的元素就会被遗忘。\n上一步通过 遗忘门🚪 选择性删除掉了传送带 $c_{t-1}$ 的一些元素，现在要往传送带上添加新的信息。\n计算输入门 $i_t$，和新的输入值 $\\widetilde{C}_t$ 的元素积。\n输入门 $i_t$ 和新的值 $\\widetilde{C}_t$ 都是维度相同的向量，他们的乘积也是维度相同的向量，把乘积加到传送带上，这样就完成了对传送带的一轮更新。\n用遗忘门删除了传送带上的一些信息，然后用遗忘门输入加入新的信息，得到了传送带新的值 $C_t$，到现在，已经更新完传送带 $C$ 了。\n🚪 Output Gate 最后一步是计算 LSTM 的输出，也就是状态向量 $h_t$。\n$h_t$ 是这么计算的，首先计算输出门 $o_t$，输出门 $o_t$ 跟前面的遗忘门，输入门的计算基本一样。\n把旧的状态 $h_{t-1}$，与新的输入 $x_t$ 做拼接，得到更高维的向量，然后算矩阵 $w_o$ 与这个向量的乘积，得到一个向量，最后使用激活函数 sigmod 得到向量 $o_t$。$o_t$ 的每一个元素都介于 (0, 1)，输出门也有自己的参数向量 $w_o$，$w_o$ 也需要从训练数据中学习。\n现在计算状态向量 $h_t$，对传送带 $C_t$ 的每一个元素求双曲正切tanh，把元素全都压到 (-1, +1) 区间。\n然后，求这两个向量的元素积，这个红色向量是刚刚求出的输出门 $o_t$，这样就得到了状态向量 $h_t$。\n看一下结构图，$h_t$ 他有两份 copys，$h_t$ 的一份 copy 传输到了下一步，另一份 copy 成了 LSTM 的输出。\n到 $d_t$ 为止，一共有 7 个向量 $x$ 输入了 LSTM，我们可以认为所有这些 $x$ 向量的信息，都积累在了状态 $h_t$ 里面。\n🧮 LSTM 的参数数量 我们来算一下 LSTM 的参数数量，LSTM 有 ❶ 遗忘门；❷ 输入门；❸ 新的输入；❹ 输出门。\n这四个模块都有各自的参数矩阵 $w$，所以一共有 4 个参数矩阵，矩阵的行数是：$shape(h)$，列数是： $shape(h)+shape(x)$\n所以，LSTM 参数的数量是：\n$4 * shape(h) * [ shape(h) + shape(x)]$\n🛠 实现 LSTM Doing\n🎐 总结 总结一下这一节的内容，这节介绍了 LSTM 模型和用 PyTorch 的实现。\nLSTM 和 simple RNN 主要的区别，是用了一条传送带，让过去的信息可以很容易传输到下一时刻，这样就有了更长的记忆。\nLSTM 的表现总是比 simple RNN 要好，所以当我们想使用 RNN 的时候就用 🙋‍♂️ LSTM 模型，而不要用 🙅‍♂️ simple RNN 模型。\nLSTM 有四个组件，分别是：\n 🚪 Forget Gate 遗忘门 🚪 Input Gate 输入门 🆕 New Value 新的输入 🚪 Output Gate 输出门  这四个组件各自有一个参数矩阵，所以一共有四个参数矩阵，LSTM 参数的数量是：\n$4 * shape(h) * [ shape(h) + shape(x)]$\n下一节将介绍：\n stacked RNN bi-directional RNN 预训练  参考  https://colah.github.io/posts/2015-08-Understanding-LSTMs/  ","date":"2022-03-19T20:36:14+08:00","image":"https://nlp.letout.cn/img/nlp/banner.png","permalink":"https://emerywan.github.io/blog/p/nlp-in-action/long-short-term-memory/","title":"Long Shorter Term Memory"},{"content":"本节主要内容为 文本处理 Text Processing 和 词嵌入 Word Embedding。本节和下面两节内容都会使用 IMDb 电影评论的数据，用来搭建机器学习模型分析电影评论。\n🌱 IMDb IMDb 是最有名的电影评论网站，用户可以在 IMDb 上给电影打分，1 分是非常讨厌，10 分是非常喜欢，如果乐意，还可以写一段电影评论。\n如果不给你看分数，只给你看评论，你大概能猜到用户打的分数，但你的猜测可能不太准确。如果换种方式，让你判断电影评论是 正面 positive 的还是 负面 negative 的，你应该会有很高的准确率。有人从 IMDb 上爬了 5 万条电影评论，这些评论都是很极端的，都是强烈的喜欢，或者强烈反感。这个二分类问题对于人来说很简单，人读一下电影评论就能轻易知道这是正面评价还是负面评价，人应该能有 100% 的准确率，这个数据集被分成两半，2 万 5 千条作为训练数据 Train，另外 2 万 5 千条作为测试数据 Test。\n你可以在下面的链接中获取到数据集：https://ai.stanford.edu/~amaas/data/sentiment/\n🔖 文本处理文本处理 在词嵌入 Word Embedding 和搭建机器学习模型之前，首先要进行文本处理，将文本变成序列 Sequence，文本处理很无聊，但我们应该重视它，文本处理的好坏，会直接影响机器学习模型的准确率。\n🚀 Tokenization 文本处理的第一步是 Tokenization，把文本分割成很多 tokens，这里我们把文本分割成很多单词，一个 token 就是一个单词（假如你把文本分割成字符，那么一个 token 就是一个字符），做完 Tokenization，一个很长的字符串就被分割成一个很多单词的列表。Tokenization 看起来很简单，但是讲究很多。比如：\n 🌰 是否应该把大写变成小写？  通常情况下应该把大写变成小写，大小写单词通常是一个意思；但有时候会混淆单词（Apple -\u0026gt; apple），比如 Apple 是苹果公司，apple 是水果，大小写的 apple 并不是相同的单词。\n 🌰 去除停用词。stop word  有些应用会去除 stop word，它是 the、a、of 等最高频的单词，这些词几乎出现在所有的句子里，对这个二分类问题几乎是没有帮助。\n 🌰 拼写纠错。  用户发电影评论的时候，大部分情况下并不会仔细检查，所以写的东西难免会有拼写错误，所以做拼写纠错通常是有用的。\n这里只是举了几个例子，实际上做 Tokenization 的时候需要做大量的工作，Tokenization 看似简单，但实际上并不容易。\n🧰 Build Dictionary 第二步是建立一个字典。可以先统计词频，去掉一些低频词，让后让每个单词对应一个正整数，比如让 the -\u0026gt; 1; cat -\u0026gt; 2; sat -\u0026gt; 3。有了这个字典，就可以把每个单词对应到一个整数，这样一来，一句话就可以用正整数的列表表示，这个列表被称为序列 Sequences。如果有必要的话，还得进一步做 one-hot encoding，把单词表示成 one-hot 向量。\n在电影评论的例子里，数据是 5 万条电影评论，每条电影评论可以表示成一个字符串。做完 Tokenization 和 Encoding 后，每条电影评论都会转换成一个 Sequences，也就是一个正整数的列表。电影评论有长有短，有人只能写几个字的评论，有人能洋洋洒洒写几千字，所以得到的这些 Sequences 的长度也各不相同。比如这两条 Sequences 的长度分别是 52 和 90。这就造成了一个问题，训练数据没有对齐，每条 Sequences 都有不同的长度。做机器学习的时候，我们需要把数据存储到矩阵或者张量里，每条序列都得有相同的长度，需要把序列对齐。解决方案是这样的：我们可以固定长度为 $w$。假如一个序列长度太长，超过了 $w$ 个词，就砍掉前面的词，只保留最后面 $w$个词（当然保留最前面 $w$ 个词也同样可以）；假如一个序列太短，不到 $w$ 个词，那么就做 zero padding 用 0 来补齐，把长度增加到 $w$。这样一来，所有序列的长度都是 $w$，可以存储到一个矩阵里。\n🍀️ 词嵌入 文本处理已经完成了，现在每个词都用一个正整数来表示，下一步是 Word Embedding，把每个词都表示为一个一维向量。\n现在每个单词都用一个数字来表示，该怎么把这些 Categorical 特征表示为数值向量呢？显然可以做 one-hot encoding，用一个 one-hot 向量来表示一个单词。比如 good: index = 2，于是使用标准正交积 $e_2$ 来表示，它的第二个元素是 1，其余元素都是 0，$e_2=[0, 1, 0, 0, \u0026hellip;, 0]$\n假如 vocabulary = v，也就是说字典里一共有 $v$ 个单词，那么就需要维度 dimension = v 的 one-hot 向量，要是字典里有 1 万个单词，那么这些 one-hot 向量都是 1 万维的，这样的向量维度是在太高了。下一节介绍 RNN 的时候你会看到，RNN 的参数数量正比于输入向量的维度，我们肯定不想让输入的向量是 1 万维的，否则一层 RNN 将会有好几十万个参数。所以我们要做 Word Embedding，把这些高维 one-hot 向量映射到低维向量。\n具体做法是吧 one-hot 向量 $e_i$ 乘到参数矩阵 $P^T$ 上，矩阵 $P^T$ 的大小是 $d*v$。其中 $d$ 是词向量的维度，由用户自己决定；$v$ 是 vocabulary，表示字典里单词的数量。\n矩阵的乘法的结果记做向量 $x_i$，$x_i$ 就是一个词向量，维度是 $d*1$，如果 one-hot 向量 $e$ 的第三个元素是 1，那么 $x_i$ 就是 $P^T$ 矩阵的第三列，可以看出，$P^T$ 矩阵每一列都是一个词向量。\n同理，下面这个参数矩阵 $P$ 的每一行都是一个词向量。这个矩阵的行数是 $v$，也就是 vocabulary；每一行对应一个单词，矩阵的列数是 $d$，$d$ 是用户决定的，$d$ 的大小会影响机器学习模型的表现，应该用 交叉验证 Cross Validation 用来选择一个比较好的 $d$。\n字典里的第一个词的是 movie，那么第一行就是 movie 的词向量；字典里的第二个词是 good，那么第二行就是 good 的词向量。\n我们的任务是判断电影评论是正面的还是负面的，这个参数矩阵是从训练数据中学习出来的，所以这些词向量都带有感情色彩，假如这些词向量都是二维的，我们就可以在平面坐标系中标出这些词向量。\nfantastic; good; fun 这些词向量都带有正面情感，所以这三个词的词向量学出来都比较接近；同理，poor; boring; mediocre 这些词带有负面情感，所以学出来的词同样也应该比较接近，但是这些词的词向量应该远离正面色彩的词向量。像 movie; is 这样的中性词，没有感情色彩，它们应该在中间。\n🎐 总结 最后总结一下这一章的内容。\n这一节上半部分，说明了文本处理是什么样的。给我们一条电影评论，首先做 Tokenization，把电影评论分割成很多单词，然后把很多单词编码成数字，这样一整条电影评论就可以很多正整数来表示，我们把这个正整数序列叫做 Sequences，就是神经网络中 Embedding 层的输入。由于电影评论的长短不一，得到的 Sequence 的长短也不一样，没办法存储在一个矩阵里，解决方案是 Alignment 对齐。假设最大长度为 20，如果长度大于20，就只保留最后 20 个单词；如果长度不到 20，就用 0 补齐，把长度增加到 20。这样一来，每个 Sequences 长度都相同。\n","date":"2022-03-11T21:14:45+08:00","image":"https://nlp.letout.cn/img/nlp/banner.png","permalink":"https://emerywan.github.io/blog/p/nlp-in-action/text-processing-and-word-embedding/","title":"文本处理与词嵌入"},{"content":"这一节我们来学习循环神经网络Recurrent Neural Networks。本节的内容是 Simple RNN，以及用 Pytorch 编程实现 Simple RNN。\n🌱 简介 现在 RNN 没有以前流行，尤其是在自然语言处理上，RNN 已经有一些过时了，如果训练的数据足够多，RNN 的效果不如 Transformer 模型，但是在小规模的问题上，RNN 还是很有用的。\n🔖 如何建模时序数据？ 机器学习中经常用到文本、语音等 时序数据sequential data（按时间顺序记录的数据列，有长度不固定的特点）。\n首先思考一个问题，怎么对这样的时序数据进行建模？ 在上一小节中，我们将一段文字整体输入到一个逻辑回归 Logistic Regression 模型中，让模型来做二分类，这属于一个 one-to-one 模型，一个输入对应一个输出。\n全连接神经网络和卷积神经网络都属于 one-to-one 模型。\n人脑并不会使用 one-to-one 模型处理时序数据，不会把一整段文字全部输入到大脑，我们在阅读的时候，会从左到右阅读一段文字，不断地在大脑里积累信息，阅读一段话之后，你脑子里就积累了一段文字的大意。\none-to-one 模型要求一个输入对应一个输出，比如：输入一张图片，输出每一类的概率值，one-to-one 的模型比较适合这类图片问题，但是不太适合文本问题。\n对于文本问题，输入和输出的长度并不固定，一段话可长可短，所以输入的长度并不固定；输出的长度也不固定，比如将英语翻译成汉语，一句英语有十个单词，翻译成汉语可能有十个字，可能有八个字，也可能是四个字的成语，输出汉语的字数并不固定，由于输入和输出的长度不固定，one-to-one 模型就不太适合了。\n对于时序数据，更好的是 many-to-one 或者是 many-to-many 模型，RNN 就是这样的模型，输入和输出的长度都不固定。所以 RNN 很适合语音，文本等时序序列数据。\n🍀️ RNN RNN 和跟人的阅读习惯很类似：人每次看一个词，会逐渐在大脑里积累信息；RNN 每看一个词，会用状态向量 $h$ 来积累阅读过的信息。\n首先，我们将输入的每个词用 词嵌入word embedding 变成一个词向量 $x$。\n每次把一个词向量输入 RNN，就会更新状态 $h$ ，把新的输入积累到状态 $h$ 里面。\n在 $h_0$中，包含了第一个词 the 的信息，在 $h_1$ 里面，包含了前两个 the cat 的信息；以此类推，状态 $h_2$ 包含 了前三个词 the cat sat 的信息，最后一个状态 $h_t$ 包含了整句话的信息，可以把 $h_t$ 看做 RNN 从整句话 the cat sat on the mat 抽取的特征向量，在更新状态 $h$ 的时候，需要用到参数矩阵 $A$。\n注意：整个 RNN 只有一个参数矩阵 $A$。无论这条链有多长，参数 $A$ 只有一个，$A$ 随机初始化，然后利用训练数据来学习 $A$。下面首先讲解 Simple RNN Model。\n🚀 Simple RNN 我们具体看看，Simple RNN 简单循环神经网络是怎么把输入的词向量 $x$，结合到状态 $h$ 中的。\n我们将上一个状态记做 $h_t-1$，新输入词向量记做 $x_t$，将这两个向量做拼接 concatenation，得到一个更高维的向量。\n图中这个矩阵 $A$ 是 RNN 的模型参数，这里计算矩阵 $A$ 和这个向量的乘积（拼接后的向量），矩阵和向量的乘积是一个向量，然后使用激活函数 tanh 作用在向量的每一个元素上，最后把激活函数的输出记做新的状态 $h_t$。\n这个激活函数式 双曲正切函数 hyperbolic tangent function，输入是任意实数，输出在 $(-1, +1)$ 之间。由于用了双曲正切激活函数，向量 $h_t$ 的每一个元素都在 $(-1, +1)$ 之间。\n这个神经网络的结构图可以这样理解：新的状态 $h_t$，是旧状态 $h_{t-1}$ 和新的输入 $x_t$ 的函数，神经网络模型的参数是 $A$：新的状态 $h_t$，依赖于向量 $h_{t-1}$, 向量 $x_t$ 以及矩阵 $A$。\n🎨 为什么需要使用 tanh 作为激活函数？ 我们思考这样一个问题：为什么需要使用 tanh 作为激活函数？能否将这个激活函数去掉，去掉之后会发生什么呢？\n首先我们做个简化，假设输入的词向量的元素都是 $0$。如图，这等同于输入的词向量 $x_t$ 都去掉，把矩阵 $A$ 右边一半也去掉。\n$x_0 = x_1 = \u0026hellip; = x_{100} = 0$\n这么一来，第 100 维的特征向量 $h_{100} = Ah_{99} = A^2h_{98} = \u0026hellip; = A^{100}h_0$。\n 🌰 假设矩阵 $A$ 最大的特征值略小于 1  比如，最大的特征值等于 0.9。那么会发生什么呢？\n$0.9^{100}$ 非常接近于 0 了，所以矩阵 $A^{100}$ 非常接近于 0，那么新的特征向量 $h_{100}$ 也几乎也是一个全零的向量。\n 🌰 假设矩阵 $A$ 最大的特征值略大于 1  比如，最大的特征值等于 1.2。\n$1.2^{100}=82817974.522$，所以矩阵 $A^{100}$ 的元素都超级大，$A^{100}$的每个元素都很大，假如循环的次数更多一些，或者 $A$ 的特征值再大一些，状态向量的值就会爆炸。\n假如没有这个激活函数 tanh，数值计算的时候很有可能会出问题，要么计算出的结果全部等于 0，要么爆炸了全部是 NaN: Not a Number。通过使用这个激活函数，每次更新状态 $h$ 后，都会做一个标准化操作 normalization，让 $h$ 恢复到 $(-1, +1)$ 这个合适的区间里。\n🏝️ Simple RNN 模型参数数量 我们来数一下 Simple RNN 有多少个模型参数。\n如图，先看一下这个拼接后向量，这个向量的维度是 $h_{t-1}$ 的维度加上 $x_t$ 的维度：\n所以 $A$ 一定要有 $shape(h)+shape(x)$ 维度这么多列：\n$A$ 的行数等于 $h$ 的维度：\n所以，最终矩阵 $A$ 的大小等于：\n$parameter(A) = shape(h) * [shape(h) + shape(x)]$\n这个乘积 $parameter(A)$ 就是 simple RNN 的最终的参数数量。\n📖 (TODO) Simple RNN 的电影评论分析 Doing\n🧰 simple RNN 的缺陷 下面看一下 simple RNN 这种简单的模型有什么缺陷。\n举个栗子 🌰 ，现在有这样一个问题，给定半句话，要求预测下一个单词。\n输入是 clouds are in the，正确的输出应该是 sky，如果在大量文本中预测 RNN，应该是有能力做出这样的预测的。在这个例子里，RNN 只需要看最近的几个词，尤其是 clouds are，并不需要更多的上下文看的更远。\n这个例子是对 simple RNN 十分有利，simple RNN 特别擅长这种 short-term dependence，simple RNN 不擅长的是 long-term dependence。\nRNN 的状态 $h$，和之前所有的输入 $x$ 都有函数依赖关系，照理来说，如果改变输入的单词 $x_1$，所有的状态 $h$ 都会发生变化，但实际上，simple RNN 并没有这种性质，所以很不合理。如果把第 100 个状态向量 $h_{100}$，关于输入 $x_1$ 求导，你会发现导数几乎等于 0。\n$\\frac{\\partial h_{100}}{\\partial x_1} \\approx 0$\n导数几乎等于 0 说明什么呢？说明当我们改变 $x_1$时，$h_{100}$ 几乎不会发生任何变化，也就是说状态 $h_{100}$ 和 100 步之前的输入 $x_1$ 几乎没有关系，这显然不合理，说明状态 $h_{100}$ 几乎把很多步之前的输入都给忘记了，simple RNN 的这种遗忘会给后续操作造成很多问题。\n再举个栗子 🌰 ，这是很长的一段话，一开始是 I grow up in China when I was a child, ... ... 到了很多句话之后，有这样一句，I speak fluent ...。\n下一个词应该是 Chinese，我小时候在中国，所有会说流利的中文，然而 simple RNN 不太可能会做出 Chinese 这个正确的预测，因为 RNN 已经把前文给忘记了。simple RNN 擅长的是 short-term dependence，RNN 看到最近的单词是 speak fluent，所以 RNN 知道下一个单词可能是某种语言，可能是 Chinese、English、French、Japanese 等等，但正确答案是 Chinese，因为上文有 I grow up in china when i was child，simple RNN 就像金鱼一样记忆力只有 7 秒，RNN 根本就不记得上文有这句话，所以 I speak fluent ... 预测单词可能是 English , French 等任何一种语言，未必是 Chinese。\n🎐 总结 最后总结一下这一节的内容：\nRNN 是一种神经网络，但是他的结构不同于全连接网络和卷积网络，RNN 适用于文本，语音等时序序列数据，RNN 按照顺序读取每一个词向量，并且在状态向量 $h$ 中积累看到过得信息，$h_0$ 中包含了 $x_0$ 的信息，$h_1$ 中包含了 $x_0$ 和 $x_1$ 的信息，$h_t$ 中积累了之前所有 $x={x_0, x_1, \u0026hellip;, x_t}$ 的信息。\n有一种错误的看法是 $h_t$ 中只包含了 $x_t$ 的信息，这是不对的，$h_t$ 中包含了之前所有输入的信息，可以认为 $h_t$ 代表了 RNN 从整个序列中抽取的特征向量，所有我们只需要 $h_t$ 就可以判断电影评价是正面的还是负面的。\nsimple RNN 有一个参数矩阵 $A$，它可能还会一个 intercept 参数向量 $b$，上面的介绍中忽略了这个参数向量 $b$，这个参数矩阵 $A$ 的维度是：\n$shape(h) * [shape(h) + shape(x)]$\n参数矩阵 $A$ 一开始随机初始化，然后从训练数据上学习。注意：simple RNN 只有一个参数矩阵，不管这个序列有多长，参数矩阵只有一个，所有模块里的参数都是一样的。\nRNN 有一个缺点，RNN 的记忆比较短，会遗忘很久之前的输入 $x$，如果这个时间序列很长，有好几十步，最终 RNN 就会忘记了之前的输入。下一节将介绍 LSTM，LSTM 的记忆会比 simple RNN 长很多，但是 RNN 也还是会有遗忘的问题。\n","date":"2022-03-08T20:36:14+08:00","image":"https://nlp.letout.cn/img/nlp/banner.png","permalink":"https://emerywan.github.io/blog/p/nlp-in-action/simple-rnn/","title":"RNN"},{"content":"🌱 类别特征 机器学习的数据通常有 类别特征 Categorical Features ，我们需要把类别特征 Categorical Features 转化成机器学习模型能理解的数值特征，下面使用一个例子来具体讲解类别特征数据的处理。\n这张表的每一行是一个人的数据，包括：年龄、性别、国籍，我们需要把这些数据变成机器学习模型可以理解的数值特征。\n表格的第一列是年龄，年龄本身就是数值特征，所以可以不用做处理，数值特征的特点是可以比较大小，比如 35 岁的人比 31 岁的年龄大。\n第二列是性别，性别是二元特征，我们可以用一个数来表示性别。用 0 表示女性，用 1 表示男性。这样一来，性别就表示为一个标量：0 / 1。\n第三列是国籍，比如中国，美国，印度。国籍是类别特征，机器学习并不理解国籍，所以我们要把国籍编码成数值向量。世界上约有 197 个国家，我们先用一个 [1 - 197] 的整数表示一个国家。可以建立一个字典，把国籍映射成一个 [1 - 197] 的整数。比如：China:1; US:2; India:3; Japan:4; Germany:5。\n我们要从 1 开始计算，而不能从 0 开始计算。\n做这种映射，国籍就表示成 [1 - 197] 之间的整数。仅仅把国籍表示成 [1 - 197] 的整数还是不行，一个整数只是一种类别，它们之间不能比较大小。US:2; India:3 这个数字并不表示印度大于美国，这些整数只是类别而已，并不是真正的数值特征。\n所以要进一步对国籍做 one-hot encoding ，用 one-hot 向量来表示国籍：\n1 2  China -\u0026gt; 1 -\u0026gt; [1,0,0,0,...,0] US -\u0026gt; 2 -\u0026gt; [0,1,0,0,...,0]   比如，中国对应 1，所以用 197 维的 one-hot 向量 [1,0,0,0...,0] 来表示，其中第一个元素为 1，其余元素都是 0；美国对应 2，这个 197 维的向量 [0,1,0,0...,0] 第二个元素是 1，其余元素都是 0。这样一来，每个国籍就由一个 one-hot 向量表示，一共有 197 个国家，所以每个向量都是 197 维的。\n我们要从 1 开始计算，而不能从 0 开始计算。因为我们要把 0 保留，用来表示未知或者缺失的国籍。数据库里面经常会有缺失的数据（比如用户没有填写国籍），这样缺失的国籍就用 0 来表示，它的 one-hot 向量就是一个全 0 的向量[0,0,0,0...,0]。\n下面这个例子中，我们用一个 199 维表示一个人的特征。比如这个人 28 岁，女性，国籍是中国。\n其中，一个维度表示年龄，一个维度表示性别，一个 197 维的 one-hot 向量表示国籍。\n这个例子里，这个 36 岁，男性，国籍未知的人的特征是这个 199 维的向量，我们用一个 197 维的全 0 向量表示未知国籍。\n🔖 为什么要用 one-hot 向量表示特征 在处理类别特征的时候，我们使用 one-hot 向量表示国籍，每个国籍都用 197 维的向量表示。为什么要用 one-hot 向量而不用一个数字表示呢？比如用 1 表示中国，2 表示美国，3 表示印度。这样一来，名字就变成了数字，可以做数值计算，而且用一个数字表示的话，可以节省 197 倍的存储空间。当然这是不行的。否则我们就不需要 one-hot encoding 了。\n假设我们使用 1 -\u0026gt; China; 2 -\u0026gt; US; 3 -\u0026gt; India。那么将中国 1 和美国 2 的特征加起来：1+2=3 ，相当于 “中国 + 美国 = 印度”。这样的特征完全不合理。\n使用 one-hot 特征向量更合理。将 China 和 US 的 one-hot 向量加起来，得到 [1,1,0,0,...,0]，第一个和第二个元素都是 1，其余元素都是 0，这个特征向量的解释是：既有中国国籍，又有美国国籍。\n所以做机器学习的时候，不能用一个标量来表示一个类别特征，这种特征做法求和等数值计算是没有意义的。正确的做法是使用 one-hot 向量来表示类别特征。\n🚀 处理文本数据的流程 在自然语言处理的应用中，数据就是文本 document，文本可以分割成很多单词，我们需要把单词表示成数值向量。其中每个单词都是一个类别，如果字典里有一万个单词，那么就有一万的类别，显然单词就是类别特征。我们需要使用处理类别特征的方法，把单词变成数值向量。\n文本处理主要分为三个步骤：\n 🔔 把文本分割成单词 🔔 计算每个单词出现的次数 🔔 进行 one-hot 编码  文本处理的第一步是把文本分割成单词。一段话，一篇文章或者一本书可以表示为一个字符串，可以把文本分割成很多单词，这个步骤称为 Tokenization。\n比如说这句话 ... to be or not to be ...， 可以分割成这些单词 [to, be, or, not, to, be]。Tokenization 就是把文本变成单词的列表。\n文本处理的第二步是计算词频，也就是每个单词出现的次数。我们可以用一个哈希表 hash Map 来计算，计算开始之前，哈希表是空的，我们根据以下方式更新哈希表：如果单词 w 不在表里面，说明到目前为止，w 还没有出现在文本里，所以我们要把 w 加入哈希表，并让它的词频等于 1；如果 w 在哈希表里面，说明 w 之前在文本里出现过，只需要把 w 的词频加 1 即可。\n接下来举个例子，我们将挨个处理这个列表里的单词。当处理到单词 to 的时候，首先查一下哈希表，发现哈希表里面有 to，它的词频是 398，说明 to 在文章里已经出现过 398 次了，现在这个单词又出现了一次，于是把表里的词频加 1，变成了 399；当处理到单词 or的时候，在表里找不到，这说明文章里还没有出现过 or 这个单词，第一次出现在文章里，于是我们把 or 插入表里，将词频设置为 1。\n完成统计词频之后，需要把哈希表做一个排序，按照词频递减的顺序进行排列，表的最前面是词频最高的，表最后是词频最低的。然后就把词频换成下标 index，从 1 开始数计数，词频最高的词的 index 是 1。这个例子里，一共有 8 个单词，每个词对应一个 [1, 8] 之间的正整数。这个表称为字典 ，可以把单词映射为一个数字。\n字典里单词的个数称为词汇量 vocabulary。这例子里词汇量等于 8。\n英语里大概有 1 万个常用词，但是统计词频之后，你会发现字典会有几十万甚至上百万个单词。统计词频的目的就是保留常用词，去掉低频词。比如，我们可以保留词频最高的 1 万个单词，删掉其余单词。\n为什么要删掉低频词呢？\n 🌰 低频词通常没有意义  很多低频词都是名字实体 name entities，比如我们的名字就是个名字实体，假如我们的名字出现在一个数据集里面，他的频率肯定会很低，在大多数的应用里名字实体没有意义。\n低频词很多都是拼写错误造成的，如果把 prince 的 c 误写成 s，prinse，那么就创造了一个新的单词，这种词的频率也很低，在很多应用里，去掉这种词没有危害。\n 🌰 去掉低频词的另一个原因是我们不希望 vocabulary 太大。  下一个步骤做 one-hot encoding 的时候，向量的维度就是字典的大小。字典越大，向量的维度就越高，这会让计算变慢。下一节详细说明词嵌入 Word Embedding 的时候就会看到，字典越大，模型的参数就越会越多，就会容易造成过拟合 overfitting，删掉低频词就会大幅减小 vocabulary。\n文本处理的第三步就是对单词做 one-hot encoding，通过查字典，把单词映射成一个正整数，一个单词的列表就映射成了一个正整数的列表；如果有必要就继续把这些正整数变成 one-hot 向量。这些 one-hot 向量的维度正好等于 vocabulary，在这个例子里面，字典的长度是 8，所以 one-hot 维度就等于 8。\n上面说过，字典里的低频词可能会被删掉，所以有些词在字典里找不到，例如把 be 错误拼写成单词 bi，这个词在字典里找不到，one-hot encoding 时，可以忽略这个词，也可以把它编码成全 0 向量。\n🎐 总结 最后总结一下这一节的内容。\n部分机器学习的数据会具备类别特征 Categorical Features，机器学习模型无法理解，我们需要将其转换成数值特征。类别特征的类别会被映射成一个从 1 开始计算的整数，0 被用来表示缺失或者未知的类别，并且使用 one-hot 向量，能很好的表示类别特征的意义。\n文本处理主要有三个步骤，第一步 tokenization 把文本分割成单词的列表；第二步建立了一个字典vocabulary，把单词映射成一个正整数；第三步进行 one-hot encoding，将分割后的单词列表映射成正整数的列表或变成 one-hot 向量。\n","date":"2022-03-01T02:02:02+08:00","image":"https://nlp.letout.cn/img/nlp/banner.png","permalink":"https://emerywan.github.io/blog/p/nlp-in-action/data-processing/","title":"数据处理基础"}]